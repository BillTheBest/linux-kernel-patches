diff -urN linux-3.1.4/arch/x86/boot/compressed/Makefile linux-3.1.4-scc/arch/x86/boot/compressed/Makefile
--- linux-3.1.4/arch/x86/boot/compressed/Makefile	2011-11-28 23:48:14.000000000 +0100
+++ linux-3.1.4-scc/arch/x86/boot/compressed/Makefile	2011-12-20 15:27:07.565882437 +0100
@@ -23,9 +23,15 @@
 
 hostprogs-y	:= mkpiggy
 
+ifeq ($(CONFIG_KERNEL_NONE),y)
+OBJCOPYFLAGS_vmlinux :=
+$(obj)/vmlinux: vmlinux FORCE
+	$(call if_changed,objcopy)
+else
 $(obj)/vmlinux: $(obj)/vmlinux.lds $(obj)/head_$(BITS).o $(obj)/misc.o $(obj)/string.o $(obj)/cmdline.o $(obj)/early_serial_console.o $(obj)/piggy.o FORCE
 	$(call if_changed,ld)
 	@:
+endif
 
 OBJCOPYFLAGS_vmlinux.bin :=  -R .comment -S
 $(obj)/vmlinux.bin: vmlinux FORCE
diff -urN linux-3.1.4/arch/x86/boot/header.S linux-3.1.4-scc/arch/x86/boot/header.S
--- linux-3.1.4/arch/x86/boot/header.S	2011-11-28 23:48:14.000000000 +0100
+++ linux-3.1.4-scc/arch/x86/boot/header.S	2011-12-20 15:27:07.565882437 +0100
@@ -217,8 +217,14 @@
 
 hardware_subarch_data:	.quad 0
 
+#ifdef CONFIG_KERNEL_NONE
+# We do not have a compressed payload when using an uncompressed kernel.
+payload_offset:		.long 0
+payload_length:		.long 0
+#else
 payload_offset:		.long ZO_input_data
 payload_length:		.long ZO_z_input_len
+#endif
 
 setup_data:		.quad 0			# 64-bit physical pointer to
 						# single linked list of
diff -urN linux-3.1.4/arch/x86/boot/pm.c linux-3.1.4-scc/arch/x86/boot/pm.c
--- linux-3.1.4/arch/x86/boot/pm.c	2011-11-28 23:48:14.000000000 +0100
+++ linux-3.1.4-scc/arch/x86/boot/pm.c	2011-12-20 15:27:07.565882437 +0100
@@ -103,6 +103,9 @@
  */
 void go_to_protected_mode(void)
 {
+#ifdef CONFIG_X86_SCC
+	asm("cli");
+#else
 	/* Hook before leaving real mode, also disables interrupts */
 	realmode_switch_hook();
 
@@ -117,6 +120,7 @@
 
 	/* Mask all interrupts in the PIC */
 	mask_all_interrupts();
+#endif
 
 	/* Actual transition to protected mode... */
 	setup_idt();
diff -urN linux-3.1.4/arch/x86/boot/tty.c linux-3.1.4-scc/arch/x86/boot/tty.c
--- linux-3.1.4/arch/x86/boot/tty.c	2011-11-28 23:48:14.000000000 +0100
+++ linux-3.1.4-scc/arch/x86/boot/tty.c	2011-12-20 15:27:07.565882437 +0100
@@ -54,10 +54,16 @@
 	if (ch == '\n')
 		putchar('\r');	/* \n -> \r\n */
 
+#ifdef CONFIG_X86_SCC
+#ifdef CONFIG_SCC_BOOT_DEBUG
+	outb(ch & 0xFF, 0x80);
+#endif
+#else
 	bios_putchar(ch);
 
 	if (early_serial_base != 0)
 		serial_putchar(ch);
+#endif
 }
 
 void __attribute__((section(".inittext"))) puts(const char *str)
diff -urN linux-3.1.4/arch/x86/include/asm/bootparam.h linux-3.1.4-scc/arch/x86/include/asm/bootparam.h
--- linux-3.1.4/arch/x86/include/asm/bootparam.h	2011-11-28 23:48:14.000000000 +0100
+++ linux-3.1.4-scc/arch/x86/include/asm/bootparam.h	2011-12-20 15:27:07.565882437 +0100
@@ -126,6 +126,7 @@
 	X86_SUBARCH_XEN,
 	X86_SUBARCH_MRST,
 	X86_SUBARCH_CE4100,
+	X86_SUBARCH_SCC,
 	X86_NR_SUBARCHS,
 };
 
diff -urN linux-3.1.4/arch/x86/include/asm/cacheflush.h linux-3.1.4-scc/arch/x86/include/asm/cacheflush.h
--- linux-3.1.4/arch/x86/include/asm/cacheflush.h	2011-11-28 23:48:14.000000000 +0100
+++ linux-3.1.4-scc/arch/x86/include/asm/cacheflush.h	2011-12-20 15:27:07.565882437 +0100
@@ -97,6 +97,10 @@
 int set_memory_uc(unsigned long addr, int numpages);
 int set_memory_wc(unsigned long addr, int numpages);
 int set_memory_wb(unsigned long addr, int numpages);
+#ifdef CONFIG_X86_SCC
+int set_memory_mpbt(unsigned long addr, int numpages);
+int set_memory_wt_mpbt(unsigned long addr, int numpages);
+#endif
 int set_memory_x(unsigned long addr, int numpages);
 int set_memory_nx(unsigned long addr, int numpages);
 int set_memory_ro(unsigned long addr, int numpages);
@@ -133,6 +137,10 @@
  */
 
 int set_pages_uc(struct page *page, int numpages);
+#ifdef CONFIG_X86_SCC
+int set_pages_mpbt(struct page *page, int numpages);
+int set_pages_wt_mpbt(struct page *page, int numpages);
+#endif
 int set_pages_wb(struct page *page, int numpages);
 int set_pages_x(struct page *page, int numpages);
 int set_pages_nx(struct page *page, int numpages);
diff -urN linux-3.1.4/arch/x86/include/asm/io.h linux-3.1.4-scc/arch/x86/include/asm/io.h
--- linux-3.1.4/arch/x86/include/asm/io.h	2011-11-28 23:48:14.000000000 +0100
+++ linux-3.1.4-scc/arch/x86/include/asm/io.h	2011-12-20 15:27:07.565882437 +0100
@@ -175,6 +175,7 @@
 extern void __iomem *ioremap_cache(resource_size_t offset, unsigned long size);
 extern void __iomem *ioremap_prot(resource_size_t offset, unsigned long size,
 				unsigned long prot_val);
+extern void __iomem *ioremap_mpbt(resource_size_t offset, unsigned long size);
 
 /*
  * The default ioremap() behavior is non-cached:
@@ -325,6 +326,8 @@
 extern void early_ioremap_reset(void);
 extern void __iomem *early_ioremap(resource_size_t phys_addr,
 				   unsigned long size);
+extern void __iomem *early_ioremap_nocache(resource_size_t phys_addr,
+					   unsigned long size);
 extern void __iomem *early_memremap(resource_size_t phys_addr,
 				    unsigned long size);
 extern void early_iounmap(void __iomem *addr, unsigned long size);
diff -urN linux-3.1.4/arch/x86/include/asm/lapic.h linux-3.1.4-scc/arch/x86/include/asm/lapic.h
--- linux-3.1.4/arch/x86/include/asm/lapic.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-3.1.4-scc/arch/x86/include/asm/lapic.h	2011-12-20 15:27:07.565882437 +0100
@@ -0,0 +1,21 @@
+#ifndef __ASM_LAPIC_H
+#define __ASM_LAPIC_H
+
+#include <asm/apic.h>
+
+static __inline void set_lapic_mask(unsigned long reg, unsigned int irq)
+{
+  unsigned long v;
+  v = apic_read(reg);
+  apic_write(reg, v | APIC_LVT_MASKED);
+}
+
+// unset_lapic_mask enables interrupt
+static __inline void unset_lapic_mask(unsigned long reg, unsigned int irq)
+{
+  unsigned long v;
+  v = apic_read(reg);
+  apic_write(reg, v & ~APIC_LVT_MASKED);
+}
+
+#endif /* __ASM_LAPIC_H */
diff -urN linux-3.1.4/arch/x86/include/asm/pgtable_types.h linux-3.1.4-scc/arch/x86/include/asm/pgtable_types.h
--- linux-3.1.4/arch/x86/include/asm/pgtable_types.h	2011-11-28 23:48:14.000000000 +0100
+++ linux-3.1.4-scc/arch/x86/include/asm/pgtable_types.h	2011-12-20 15:27:07.565882437 +0100
@@ -15,6 +15,9 @@
 #define _PAGE_BIT_DIRTY		6	/* was written to (raised by CPU) */
 #define _PAGE_BIT_PSE		7	/* 4 MB (or 2MB) page */
 #define _PAGE_BIT_PAT		7	/* on 4KB pages */
+#ifdef CONFIG_X86_SCC
+#define _PAGE_BIT_PMB		7	/* on 4KB pages if CR4 has (!PSE && MBE) */
+#endif
 #define _PAGE_BIT_GLOBAL	8	/* Global TLB entry PPro+ */
 #define _PAGE_BIT_UNUSED1	9	/* available for programmer */
 #define _PAGE_BIT_IOMAP		10	/* flag used to indicate IO mapping */
@@ -39,6 +42,9 @@
 #define _PAGE_ACCESSED	(_AT(pteval_t, 1) << _PAGE_BIT_ACCESSED)
 #define _PAGE_DIRTY	(_AT(pteval_t, 1) << _PAGE_BIT_DIRTY)
 #define _PAGE_PSE	(_AT(pteval_t, 1) << _PAGE_BIT_PSE)
+#ifdef CONFIG_X86_SCC
+#define _PAGE_PMB	(_AT(pteval_t, 1) << _PAGE_BIT_PMB)
+#endif
 #define _PAGE_GLOBAL	(_AT(pteval_t, 1) << _PAGE_BIT_GLOBAL)
 #define _PAGE_UNUSED1	(_AT(pteval_t, 1) << _PAGE_BIT_UNUSED1)
 #define _PAGE_IOMAP	(_AT(pteval_t, 1) << _PAGE_BIT_IOMAP)
diff -urN linux-3.1.4/arch/x86/include/asm/processor-flags.h linux-3.1.4-scc/arch/x86/include/asm/processor-flags.h
--- linux-3.1.4/arch/x86/include/asm/processor-flags.h	2011-11-28 23:48:14.000000000 +0100
+++ linux-3.1.4-scc/arch/x86/include/asm/processor-flags.h	2011-12-20 15:27:07.565882437 +0100
@@ -58,6 +58,9 @@
 #define X86_CR4_PCE	0x00000100 /* enable performance counters at ipl 3 */
 #define X86_CR4_OSFXSR	0x00000200 /* enable fast FPU save and restore */
 #define X86_CR4_OSXMMEXCPT 0x00000400 /* enable unmasked SSE exceptions */
+#ifdef CONFIG_X86_SCC
+#define X86_CR4_MPE	0x00000800 /* SCC: enable MPBT caching */
+#endif
 #define X86_CR4_VMXE	0x00002000 /* enable VMX virtualization */
 #define X86_CR4_RDWRGSFS 0x00010000 /* enable RDWRGSFS support */
 #define X86_CR4_OSXSAVE 0x00040000 /* enable xsave and xrestore */
diff -urN linux-3.1.4/arch/x86/include/asm/serial.h linux-3.1.4-scc/arch/x86/include/asm/serial.h
--- linux-3.1.4/arch/x86/include/asm/serial.h	2011-11-28 23:48:14.000000000 +0100
+++ linux-3.1.4-scc/arch/x86/include/asm/serial.h	2011-12-20 15:27:07.565882437 +0100
@@ -19,11 +19,21 @@
 #define STD_COM4_FLAGS ASYNC_BOOT_AUTOCONF
 #endif
 
+#ifdef CONFIG_X86_SCC
+#define SERIAL_PORT_DFNS			\
+	/* UART CLK   PORT IRQ     FLAGS        */			\
+	{ 0, BASE_BAUD, 0x3F8, 0, STD_COM_FLAGS },	/* ttyS0 */	\
+	{ 0, BASE_BAUD, 0x2F8, 0, STD_COM_FLAGS },	/* ttyS1 */	\
+	{ 0, BASE_BAUD, 0x3E8, 0, STD_COM_FLAGS },	/* ttyS2 */	\
+	{ 0, BASE_BAUD, 0x2E8, 0, STD_COM4_FLAGS },	/* ttyS3 */
+#else
+
 #define SERIAL_PORT_DFNS			\
 	/* UART CLK   PORT IRQ     FLAGS        */			\
 	{ 0, BASE_BAUD, 0x3F8, 4, STD_COM_FLAGS },	/* ttyS0 */	\
 	{ 0, BASE_BAUD, 0x2F8, 3, STD_COM_FLAGS },	/* ttyS1 */	\
 	{ 0, BASE_BAUD, 0x3E8, 4, STD_COM_FLAGS },	/* ttyS2 */	\
 	{ 0, BASE_BAUD, 0x2E8, 3, STD_COM4_FLAGS },	/* ttyS3 */
+#endif
 
 #endif /* _ASM_X86_SERIAL_H */
diff -urN linux-3.1.4/arch/x86/include/asm/setup.h linux-3.1.4-scc/arch/x86/include/asm/setup.h
--- linux-3.1.4/arch/x86/include/asm/setup.h	2011-11-28 23:48:14.000000000 +0100
+++ linux-3.1.4-scc/arch/x86/include/asm/setup.h	2011-12-20 15:27:07.565882437 +0100
@@ -53,6 +53,12 @@
 static inline void x86_mrst_early_setup(void) { }
 #endif
 
+#ifdef CONFIG_X86_SCC
+extern void x86_scc_early_setup(void);
+#else
+static inline void x86_scc_early_setup(void) { }
+#endif
+
 #ifdef CONFIG_X86_INTEL_CE
 extern void x86_ce4100_early_setup(void);
 #else
diff -urN linux-3.1.4/arch/x86/Kconfig linux-3.1.4-scc/arch/x86/Kconfig
--- linux-3.1.4/arch/x86/Kconfig	2011-11-28 23:48:14.000000000 +0100
+++ linux-3.1.4-scc/arch/x86/Kconfig	2011-12-20 15:27:07.565882437 +0100
@@ -53,6 +53,7 @@
 	select HAVE_KERNEL_LZMA
 	select HAVE_KERNEL_XZ
 	select HAVE_KERNEL_LZO
+	select HAVE_KERNEL_NONE
 	select HAVE_HW_BREAKPOINT
 	select HAVE_MIXED_BREAKPOINTS_REGS
 	select PERF_EVENTS
@@ -422,6 +423,106 @@
 
 endif
 
+config X86_SCC
+	bool "Single-Chip Cloud Computer (SCC)"
+	 depends on X86_32
+	 depends on X86_EXTENDED_PLATFORM
+	---help---
+	  This enables certain tweaks for running the kernel on SCC cores.
+
+config SCC_BUSCLOCK
+	int "Frequency of SCC FSB clock in Hz"
+	default 533000000
+	depends on X86_SCC
+	range 1000000 2000000000
+	---help---
+	  SCC core clock frequency used to calculate configuration for LAPIC
+	  timer setup.
+
+config SCC_QUERY_FREQUENCY_FROM_FPGA
+	bool "Get real SCC FSB clock from FPGA"
+	default y
+	depends on X86_SCC
+	---help---
+	  Read the real SCC FSB clock frequency from the FPGA's global
+	  configuration register bank (GRB) during boot. If this option is
+	  enabled and a valid frequency is read, it overwrites the statically-
+	  configured value. If this option is disabled or the value read from
+	  the FPGA is obviously wrong (e.g., a FASTCLOCK of zero), the
+	  configured value is used instead.
+
+config X86_SCC_LAPIC_TRACK_CPU_FREQ
+	bool "LAPIC timer: Track CPU frequency changes"
+	default y
+	depends on X86_SCC
+	depends on X86_LOCAL_APIC
+	depends on CPU_FREQ
+	---help---
+	  Make the LAPIC timer track changes to the CPU core frequency via the
+	  sccfreq driver.
+
+	  If this option is enabled, the clock divisor used by the LAPIC timer
+	  is adjusted automatically whenever the core frequency is changed, thus
+	  allowing the frequency of the LAPIC timer interrupts to stay the same.
+	  This setting allows the "jiffies" clock source to report correct
+	  (real) time.
+
+	  If this option is disabled, the clock divisor is set once during boot,
+	  based on the detected or statically-configured bus frequency.
+	  This setting results in the "jiffies" clock source reporting time
+	  relative to the ratio of the current and boot frequency.
+
+config X86_SCC_TSC_TRACK_CPU_FREQ
+	bool "TSC: Track CPU frequency changes"
+	default y
+	depends on X86_SCC
+	depends on CPU_FREQ
+	---help---
+	  Make the TSC clock source track changes to the CPU core frequency via
+	  the sccfreq driver.
+
+	  If this option is enabled, the frequency of the TSC clock source is
+	  adjusted automatically whenever the core frequency is changed.
+	  This setting allows the "tsc" clock source to report correct (real)
+	  time.
+
+	  If this option is disabled, the frequency of the TSC clock source is
+	  set once during boot, based on the detected or statically-configured
+	  bus frequency. This results in the "tsc" clock source reporting time
+	  relative to the ratio of the current and boot frequency.
+
+config SCCSYS
+	bool "SCC System Driver"
+	depends on X86_SCC
+	default y
+	---help---
+	  SCC system driver. This driver provides access to the
+	  architecture-specific configuration register banks (CRB) of the tile,
+	  and the global configuration register bank (GRB) of the FPGA, and is
+	  also responsible for initializing message-passing buffers and the
+	  MPBT (message-passing buffer type) caching type.
+	  This driver is a prerequisite for using other SCC-specific drivers
+	  like SCCMEM (the SCC memory driver providing devices like /dev/rckncm)
+	  or SCCMB, SCCPC or SCCEMAC (the network drivers for on-die and off-die
+	  communication).
+
+config SCCSYS_FPGA_CLOCK
+	bool "Use global timestamp counter as clock source"
+	default y
+	depends on SCCSYS
+	---help---
+	  Register a clocksource backed by the global timestamp counter in the
+	  FPGA. The counter runs at a frequency of 125MHz and is independent of
+	  changes to the core and mesh frequencies, thus providing a reliable
+	  walltime to the core.
+
+config SCCSYS_RECURSIVE_PID_LOCK
+	bool "Recursive PID locks"
+	default y
+	depends on SCCSYS
+	---help---
+	  Allow acquiring the core-specific test&set registers recursively.
+
 config X86_RDC321X
 	bool "RDC R-321x SoC"
 	depends on X86_32
diff -urN linux-3.1.4/arch/x86/kernel/apic/apic.c linux-3.1.4-scc/arch/x86/kernel/apic/apic.c
--- linux-3.1.4/arch/x86/kernel/apic/apic.c	2011-11-28 23:48:14.000000000 +0100
+++ linux-3.1.4-scc/arch/x86/kernel/apic/apic.c	2011-12-20 15:27:07.565882437 +0100
@@ -54,6 +54,12 @@
 #include <asm/tsc.h>
 #include <asm/hypervisor.h>
 
+#ifdef CONFIG_X86_SCC
+#include <linux/sccsys.h>	/* scc_get_boot_busclock() */
+#include <linux/cpufreq.h>
+#include <linux/percpu.h>
+#endif
+
 unsigned int num_processors;
 
 unsigned disabled_cpus __cpuinitdata;
@@ -436,6 +442,16 @@
 	return 0;
 }
 
+#ifdef CONFIG_X86_SCC_LAPIC_TRACK_CPU_FREQ
+static DEFINE_PER_CPU(enum clock_event_mode, last_APIC_timer_mode);
+static inline void __save_APIC_timer_setup(enum clock_event_mode mode)
+{
+	percpu_write(last_APIC_timer_mode, mode);
+}
+#else // CONFIG_X86_SCC_LAPIC_TRACK_CPU_FREQ
+static inline void __save_APIC_timer_setup(enum clock_event_mode) {}
+#endif // !CONFIG_X86_SCC_LAPIC_TRACK_CPU_FREQ
+
 /*
  * Setup the lapic timer in periodic or oneshot mode
  */
@@ -451,6 +467,8 @@
 
 	local_irq_save(flags);
 
+	__save_APIC_timer_setup(mode);
+
 	switch (mode) {
 	case CLOCK_EVT_MODE_PERIODIC:
 	case CLOCK_EVT_MODE_ONESHOT:
@@ -516,6 +534,7 @@
 	memcpy(levt, &lapic_clockevent, sizeof(*levt));
 	levt->cpumask = cpumask_of(smp_processor_id());
 
+	__save_APIC_timer_setup(CLOCK_EVT_MODE_UNUSED);
 	clockevents_register_device(levt);
 }
 
@@ -632,6 +651,7 @@
 
 static int __init calibrate_APIC_clock(void)
 {
+#ifndef CONFIG_X86_SCC
 	struct clock_event_device *levt = &__get_cpu_var(lapic_events);
 	void (*real_handler)(struct clock_event_device *dev);
 	unsigned long deltaj;
@@ -748,10 +768,111 @@
 		pr_warning("APIC timer disabled due to verification failure\n");
 			return -1;
 	}
+#else // CONFIG_X86_SCC
+	long delta;
 
+	/*
+	 * No calibration possible since it is based on another clock we don't
+	 * have. On the SCC, the LAPIC timer *is* the primary clock source, so
+	 * we need to fake a calibration result by referring to the configured
+	 * bus clock.
+         */
+
+	calibration_result = scc_get_boot_busclock() / HZ;
+	printk("SCC: Setting APIC timer based on configured Bus clock of %u.%04u MHz.\n",
+		calibration_result/(1000000/HZ), calibration_result%(1000000/HZ));
+
+	/* Calculate the scaled math multiplication factor. In the regular path,
+	 * this is done by first getting the delta value based on the hardware
+	 * reference clock, then calculating calibration_result. As we specify
+	 * calibration_result directly, we go the other way around and calculate
+	 * delta now, as it is used to set the lapic_clockevent parameters. */
+	delta = calibration_result * LAPIC_CAL_LOOPS / APIC_DIVISOR;
+
+	/* We have a delta value, so this code can simply be copied from the
+	 * above calibration routine. */
+	lapic_clockevent.mult = div_sc(delta, TICK_NSEC * LAPIC_CAL_LOOPS,
+				       lapic_clockevent.shift);
+	lapic_clockevent.max_delta_ns =
+		clockevent_delta2ns(0x7FFFFFFF, &lapic_clockevent);
+	lapic_clockevent.min_delta_ns =
+		clockevent_delta2ns(0xF, &lapic_clockevent);
+#endif // CONFIG_X86_SCC
 	return 0;
 }
 
+#if defined(CONFIG_CPU_FREQ) && defined(CONFIG_X86_SCC_LAPIC_TRACK_CPU_FREQ)
+/* Update the specified APIC timer device to use a new frequency base.
+ * The routine must be invoked on the CPU the specified APIC timer device
+ * belongs to. */
+static inline void __updatefreq_APIC_timer(struct clock_event_device* evt)
+{
+	if (evt->features & CLOCK_EVT_FEAT_DUMMY) {
+		return;
+	}
+
+	/* Configure the APIC timer with the new calibration value. We simply
+	 * re-apply the last timer mode, which also sets the new clock divider.
+	 * The initial counter value in APIC_TMICT does not need to be updated,
+	 * as the counter frequency itself does not change. */
+	evt->set_mode(percpu_read(last_APIC_timer_mode), evt);
+}
+
+static unsigned int  ref_freq;
+static unsigned long calibration_ref;
+
+static int lapic_cpufreq_notifier(struct notifier_block *nb, unsigned long val,
+				void *data)
+{
+	struct cpufreq_freqs *freq = data;
+	unsigned int new_calibration;
+
+	if (cpu_has(&cpu_data(freq->cpu), X86_FEATURE_CONSTANT_TSC))
+		return 0;
+
+	if (lapic_clockevent.features & CLOCK_EVT_FEAT_DUMMY)
+		return 0;
+
+	if (!ref_freq) {
+		ref_freq = freq->old;
+		calibration_ref = calibration_result;
+	}
+	if ((val == CPUFREQ_PRECHANGE  && freq->old < freq->new) ||
+			(val == CPUFREQ_POSTCHANGE && freq->old > freq->new) ||
+			(val == CPUFREQ_RESUMECHANGE)) {
+		new_calibration = cpufreq_scale(calibration_ref, ref_freq, freq->new);
+	} else {
+		new_calibration = calibration_result;
+	}
+
+	if (new_calibration != calibration_result) {
+		struct clock_event_device *levt = &__get_cpu_var(lapic_events);
+
+		calibration_result = new_calibration;
+		__updatefreq_APIC_timer(levt);
+	}
+
+	return 0;
+}
+
+static struct notifier_block lapic_cpufreq_notifier_block = {
+	.notifier_call  = lapic_cpufreq_notifier
+};
+
+static int __init cpufreq_lapic(void)
+{
+	if (!cpu_has_tsc)
+		return 0;
+	if (boot_cpu_has(X86_FEATURE_CONSTANT_TSC))
+		return 0;
+	cpufreq_register_notifier(&lapic_cpufreq_notifier_block,
+				CPUFREQ_TRANSITION_NOTIFIER);
+	return 0;
+}
+
+core_initcall(cpufreq_lapic);
+#endif // CONFIG_CPU_FREQ && CONFIG_X86_SCC_LAPIC_TRACK_CPU_FREQ
+
 /*
  * Setup the boot APIC
  *
@@ -1559,9 +1680,17 @@
 	mp_lapic_addr = APIC_DEFAULT_PHYS_BASE;
 
 	/* The BIOS may have set up the APIC at some other address */
-	rdmsr(MSR_IA32_APICBASE, l, h);
-	if (l & MSR_IA32_APICBASE_ENABLE)
-		mp_lapic_addr = l & MSR_IA32_APICBASE_BASE;
+	if (boot_cpu_data.x86 != 5) {
+		/*
+		 * Only P6 and later cores implement the MSR_IA32_APICBASE. For
+		 * all previous ones, we need to rely on the APIC residing at
+		 * its default address.
+		 */
+
+		rdmsr(MSR_IA32_APICBASE, l, h);
+		if (l & MSR_IA32_APICBASE_ENABLE)
+			mp_lapic_addr = l & MSR_IA32_APICBASE_BASE;
+	}
 
 	pr_info("Found and enabled local APIC!\n");
 	return 0;
diff -urN linux-3.1.4/arch/x86/kernel/cpu/common.c linux-3.1.4-scc/arch/x86/kernel/cpu/common.c
--- linux-3.1.4/arch/x86/kernel/cpu/common.c	2011-11-28 23:48:14.000000000 +0100
+++ linux-3.1.4-scc/arch/x86/kernel/cpu/common.c	2011-12-20 15:27:07.578352779 +0100
@@ -439,8 +439,10 @@
 		l2size = this_cpu->c_size_cache(c, l2size);
 
 	/* Allow user to override all this if necessary. */
-	if (cachesize_override != -1)
-		l2size = cachesize_override;
+	//if (cachesize_override != -1)
+	//	l2size = cachesize_override;
+	printk(KERN_INFO "Setting l2size\n");
+	l2size = 0x3FC00;
 
 	if (l2size == 0)
 		return;		/* Again, no L2 cache is possible */
diff -urN linux-3.1.4/arch/x86/kernel/cpu/intel_cacheinfo.c linux-3.1.4-scc/arch/x86/kernel/cpu/intel_cacheinfo.c
--- linux-3.1.4/arch/x86/kernel/cpu/intel_cacheinfo.c	2011-11-28 23:48:14.000000000 +0100
+++ linux-3.1.4-scc/arch/x86/kernel/cpu/intel_cacheinfo.c	2011-12-20 15:27:07.578352779 +0100
@@ -736,6 +736,13 @@
 #endif
 	}
 
+#ifdef CONFIG_X86_SCC
+	l3 = 0;
+	l2 = 256;
+	l1i = 16;
+	l1d = 0;
+#endif
+
 	c->x86_cache_size = l3 ? l3 : (l2 ? l2 : (l1i+l1d));
 
 	return l2;
diff -urN linux-3.1.4/arch/x86/kernel/cpu/Makefile linux-3.1.4-scc/arch/x86/kernel/cpu/Makefile
--- linux-3.1.4/arch/x86/kernel/cpu/Makefile	2011-11-28 23:48:14.000000000 +0100
+++ linux-3.1.4-scc/arch/x86/kernel/cpu/Makefile	2011-12-20 15:27:07.578352779 +0100
@@ -30,6 +30,7 @@
 
 obj-$(CONFIG_X86_MCE)			+= mcheck/
 obj-$(CONFIG_MTRR)			+= mtrr/
+obj-$(CONFIG_SCCSYS)			+= sccsys.o
 
 obj-$(CONFIG_X86_LOCAL_APIC)		+= perfctr-watchdog.o
 
diff -urN linux-3.1.4/arch/x86/kernel/cpu/sccsys.c linux-3.1.4-scc/arch/x86/kernel/cpu/sccsys.c
--- linux-3.1.4/arch/x86/kernel/cpu/sccsys.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-3.1.4-scc/arch/x86/kernel/cpu/sccsys.c	2011-12-20 15:27:07.578352779 +0100
@@ -0,0 +1,657 @@
+/*
+ *  linux/arch/x86/kernel/cpu/sccsys.c
+ *
+ *  SCC system driver, partially based on the original sccmem.c driver
+ *  distributed as part of SCC Linux.
+ */
+
+#include <linux/module.h>
+#ifdef MODVERSIONS
+#  include <linux/modversions.h>
+#endif
+#include <linux/spinlock.h>
+#include <linux/clocksource.h>
+#include <asm/io.h>
+
+#include <linux/sccsys.h>
+
+/* DEBUG messages */
+#define DEBUG_MSG 0
+#define PRINTD(format, args...) if (DEBUG_MSG) { printk(format, ##args); }
+
+/* Symbols */
+#define FIRST_MPB   0xc0000000
+#define MPBADDRBITS 13
+#define MPBSIZE     (1<<MPBADDRBITS)
+
+
+/* Module parameters */
+static int crb_offset = 0xE0000000;
+module_param(crb_offset, int, 0644);
+MODULE_PARM_DESC(crb_offset, "Physical start address of the register bank memory range");
+
+static int grb_offset = 0xF9000000;
+module_param(grb_offset, int, 0644);
+MODULE_PARM_DESC(grb_offset, "Start address of the global register bank");
+
+static int local_crb_offset = 0xF8000000;
+module_param(local_crb_offset, int, 0644);
+MODULE_PARM_DESC(local_crb_offset, "Start address of the local register bank");
+
+static int disable_locking = 0;
+module_param(disable_locking, int, 0644);
+MODULE_PARM_DESC(disable_locking, "Enable/disable use of the test&set bits");
+
+/* SCC System Context */
+struct sccsys {
+	void*			grb;
+	void*			own_crb;
+	int			own_tileid;
+	scc_coord_t		own_coord;
+	int			own_pid;
+	void*			crb[SCC_TILECOUNT];
+#ifdef CONFIG_SCCSYS_RECURSIVE_PID_LOCK
+	spinlock_t		lock[SCC_CORECOUNT];
+	unsigned int		lock_count[SCC_CORECOUNT];
+#endif // CONFIG_SCCSYS_RECURSIVE_PID_LOCK
+};
+
+static struct sccsys		sccsys_buffer;
+static struct sccsys*		sccsys = &sccsys_buffer;
+
+
+#ifdef CONFIG_SCCSYS_FPGA_CLOCK
+/* clocksource */
+static struct clocksource clocksource_scc;
+
+static cycle_t sccsys_read_clock(struct clocksource *cs)
+{
+	unsigned long lo, hi;
+
+	/* Read global timestamp counter from GRB address space. We read the two
+	 * halves separately, then check whether the high word has changed in
+	 * between: if it has, the low word has overflowed, so we retry the
+	 * operation. Otherwise, we are guaranteed to get a consistent pair of
+	 * low and high word, and can construct a cycle_t from it. */
+	do {
+		hi = sccsys_read_grb_entry(0x8228);
+		lo = sccsys_read_grb_entry(0x8224);
+	} while (hi != sccsys_read_grb_entry(0x8228));
+
+	return (cycle_t)(((unsigned long long)hi << 32) | lo);
+}
+
+static void sccsys_resume_clock(struct clocksource *cs)
+{
+	clocksource_scc.cycle_last = 0;
+}
+
+static struct clocksource clocksource_scc = {
+	.name                   = "scc",
+	.rating                 = 500,
+	.read                   = sccsys_read_clock,
+	.resume			= sccsys_resume_clock,
+	.mask                   = CLOCKSOURCE_MASK(64),
+	.flags                  = CLOCK_SOURCE_IS_CONTINUOUS,
+};
+#endif
+
+/* cleanup */
+static void sccsys_cleanup(void) {
+	int i;
+
+	for (i = 0; i < SCC_TILECOUNT; i++) {
+		if (sccsys->crb[i]) {
+			iounmap(sccsys->crb[i]);
+			sccsys->crb[i] = NULL;
+		}
+	}
+	if (sccsys->own_crb) {
+		iounmap(sccsys->own_crb);
+		sccsys->own_crb = NULL;
+	}
+	if (sccsys->grb) {
+		iounmap(sccsys->grb);
+		sccsys->grb = NULL;
+	}
+}
+
+/* module initialization - called at module load time */
+static int __init sccsys_init(void) {
+	int i;
+	void* own_mpb;
+	int loopMPB;
+
+	PRINTD(KERN_INFO "Starting up SCC system driver...\n");
+
+	/* Zero out the sccsys structure */
+	memset(sccsys, 0, sizeof(struct sccsys));
+
+#ifdef CONFIG_SCCSYS_RECURSIVE_PID_LOCK
+	for (i = 0; i < SCC_CORECOUNT; i++) {
+		spin_lock_init(&sccsys->lock[i]);
+	}
+#endif // CONFIG_SCCSYS_RECURSIVE_PID_LOCK
+
+
+	/* Begin with some sanity checks.
+	 * *) We need to be running on a GaussLake CPU, which looks like a
+	 *    regular P54C by its CPUID.
+	 * *) The kernel must not have enabled PSE, as our MB bit shares its
+	 *    location in the page tables with the PS bit. Enabling both
+	 *    architectural extensions at the same time is not possible, neither
+	 *    is to tell the kernel that PSE is not usable afterwards.
+	 * *) We are running in a bare-metal environment, not paravirtualized
+	 */
+	if (!(boot_cpu_data.x86 == 5 && boot_cpu_data.x86_model == 2)) {
+		printk(KERN_ERR "sccsys: Unknown CPU (%d:%d). This driver can only be used on GaussLake cores.\n",
+			boot_cpu_data.x86,
+			boot_cpu_data.x86_model);
+		return -EINVAL;
+	}
+	if (read_cr4() & X86_CR4_PSE) {
+		printk(KERN_ERR "sccsys: Unable to activate GaussLake extensions because the kernel has set CR4.PSE.\n");
+		return -EINVAL;
+	}
+	if (!scc_bare_metal()) {
+		printk(KERN_INFO "sccsys: Running on paravirtualized kernel.\n");
+		return -EINVAL;
+	}
+
+	/* Map our own configuration registers and read the tileid */
+	sccsys->own_crb = ioremap_nocache(local_crb_offset, SCC_CRB_SIZE);
+	if (!sccsys->own_crb) {
+		printk(KERN_ERR "sccsys: unable to map own CRB @%08x\n", local_crb_offset);
+		return -ENOMEM;
+	}
+
+	sccsys->own_tileid = readl(sccsys->own_crb + SCC_TILEID);
+	sccsys->own_coord  = scc_tileid_to_coord(sccsys->own_tileid);
+	sccsys->own_pid    = scc_tileid_to_pid(sccsys->own_tileid);
+	PRINTD(KERN_INFO "sccsys_init: starting on pid %02d (x=%d, y=%d, z=%d)\n",
+		sccsys->own_pid,
+		sccsys->own_coord.x, sccsys->own_coord.y, sccsys->own_coord.z);
+
+	/* Map configuration registers of other tiles */
+	for (i = 0; i < SCC_TILECOUNT; i++) {
+		int crb_address = crb_offset + SCC_TILE_SIZE*i;
+		sccsys->crb[i] = ioremap_nocache(crb_address, SCC_CRB_SIZE);
+		if (!sccsys->crb[i]) {
+			printk(KERN_ERR "sccsys: unable to map CRB of tile %d @%08x\n", i, crb_address);
+			sccsys_cleanup();
+			return -ENOMEM;
+		}
+	}
+
+	/* Map global configuration registers */
+	sccsys->grb = ioremap_nocache(grb_offset, SCC_GRB_SIZE);
+	if (!sccsys->grb) {
+		printk(KERN_ERR "sccsys: unable to map GRB @%08x\n", grb_offset);
+		sccsys_cleanup();
+		return -ENOMEM;
+	}
+
+	/* Initialize own message passing buffer */
+	own_mpb = ioremap_nocache(FIRST_MPB+(sccsys->own_coord.x*0x01000000)+(sccsys->own_coord.y*0x06000000), 2*MPBSIZE);
+	if (!own_mpb) {
+		printk(KERN_ERR "sccsys: unable to map own MPB. GaussLake extensions will NOT be enabled on this core.\n");
+		sccsys_cleanup();
+		return -ENOMEM;
+	}
+
+	for (loopMPB = sccsys->own_coord.z*MPBSIZE; loopMPB < ((sccsys->own_coord.z+1)*MPBSIZE); loopMPB += 4) {
+		writel(0, own_mpb + loopMPB);
+	}
+	iounmap(own_mpb);
+
+	/* Enable GaussLake extensions. */
+	PRINTD(KERN_INFO "sccsys_init: about to set CR4.MPE (%08x)\n", X86_CR4_MPE);
+	set_in_cr4(X86_CR4_MPE);
+	//__supported_pte_mask |= _PAGE_PMB;
+	printk(KERN_INFO "sccsys_init: GaussLake extensions enabled.\n");
+
+#ifdef CONFIG_SCCSYS_FPGA_CLOCK
+	/* Register SCC clocksource */
+	clocksource_register_khz(&clocksource_scc, 125000);
+#endif
+
+	return 0;
+}
+
+/* module unload */
+static void __exit sccsys_exit(void)
+{
+	PRINTD(KERN_INFO "sccsys_exit: Shutting down SCC system driver...\n");
+	sccsys_cleanup();
+}
+
+
+/*
+ * Functions exported to other modules
+ */
+
+/* Get address of configuration register for a tile identified by a pid.
+ * This routine is intended for tile-global registers that do not depend on
+ * the target processor number. If a per-cpu configuration register is needed,
+ * you can use sccsys_get_crb_entry_for_pid instead.
+ */
+static void* sccsys_get_crb_entry_for_tile_of_pid(scc_pid_t pid, int offset)
+{
+	void* address;
+
+	if (pid < 0 || pid >= SCC_CORECOUNT) {
+		printk(KERN_ERR "sccsys: get_crb_entry: invalid pid %02d\n", pid);
+		BUG_ON(1);
+		return NULL;
+	}
+
+	address = sccsys->crb[pid / 2];
+	if (!address) {
+		printk(KERN_ERR "sccsys: get_crb_entry: crb of pid %02d not mapped\n", pid);
+		BUG_ON(1);
+		return NULL;
+	}
+
+	return address + offset;
+}
+
+/* Get address of configuration register for the specified processor.
+ * This routine is intended for per-cpu registers, so it takes two offsets.
+ * cpu0_offset is used for the first processor of the tile (z=0),
+ * cpu1_offset is used for the second one.
+ */
+static void* sccsys_get_crb_entry_for_pid(scc_pid_t pid, int cpu0_offset, int cpu1_offset)
+{
+	void* address;
+
+	address = sccsys_get_crb_entry_for_tile_of_pid(pid, 0);
+	if (!address) {
+		return NULL;
+	}
+
+	if ((pid % 2) == 0) {
+		return address + cpu0_offset;
+	} else {
+		return address + cpu1_offset;
+	}
+}
+
+
+/* Get value of own tileid. The format is 0...0_00000yyy_yxxxxzzz (in bits).
+ * Tile IDs are not consecutive; if a consecutive number is needed, consider
+ * using the PID instead.
+ */
+int sccsys_get_tileid(void)
+{
+	return sccsys->own_tileid;
+}
+EXPORT_SYMBOL(sccsys_get_tileid);
+
+/* Get own processor id. This is a consecutive number from 0 to SCC_CORECOUNT-1.
+ */
+scc_pid_t sccsys_get_pid(void)
+{
+	return sccsys->own_pid;
+}
+EXPORT_SYMBOL(sccsys_get_pid);
+
+/* Get own coordinates. This is the decoded version of the TILEID returned by
+ * sccsys_get_tileid. */
+scc_coord_t sccsys_get_coord(void)
+{
+	return sccsys->own_coord;
+}
+EXPORT_SYMBOL(sccsys_get_coord);
+
+/* Get logically next processor id. This is a consecutive number from 0 to
+ * SCC_CORECOUNT-1, but is not neccessarily ((pid+1) % SCC_CORECOUNT). The
+ * number is usually chosen to reflect the adjacent core having the shortest
+ * distance, although it guarantees that the cycle closes after exactly
+ * SCC_CORECOUNT invocations.
+ */
+scc_pid_t sccsys_get_next_pid(scc_pid_t pid)
+{
+#if SCC_CORECOUNT == 48
+	if (pid >= 0 && pid < 11) {
+		return pid + 1;
+	} else if (pid == 11) {
+		return 22;
+	} else if ((pid == 12) || (pid >= 14 && pid <= 23)) {
+		return (pid % 2) ? (pid - 3) : (pid + 1);
+	} else if (pid == 13) {
+		return 24;
+	} else if (pid >= 24 && pid < 35) {
+		return pid + 1;
+	} else if (pid == 35) {
+		return 46;
+	} else if ((pid == 36) || (pid >= 38 && pid <= 47)) {
+		return (pid % 2) ? (pid - 3) : (pid + 1);
+	} else if (pid == 37) {
+		return 0;
+	} else {
+		BUG();
+	}
+#else
+	#error Unknown value for SCC_CORECOUNT.
+#endif
+}
+EXPORT_SYMBOL(sccsys_get_next_pid);
+
+/* Acquire the test&set register of the specified PID. This call returns 1 if
+ * the lock has successfully been acquired, or 0 otherwise.
+ *
+ * In the current implementation, the call does only fail if the PID is invalid
+ * (i.e., outside of the range from 0 to SCC_CORECOUNT-1) or mapping of the
+ * configuration registers has failed.
+ */
+int sccsys_acquire_pid_lock(scc_pid_t pid)
+{
+#ifdef CONFIG_SCCSYS_RECURSIVE_PID_LOCK
+	unsigned long flags;
+#endif // CONFIG_SCCSYS_RECURSIVE_PID_LOCK
+
+	void* lock_address;
+
+	/* Simply ignore the test&set register if locking is disabled */
+	if (disable_locking) {
+		return 1;
+	}
+  
+	/* Get address of test&set register for target  */
+	lock_address = sccsys_get_crb_entry_for_pid(pid, SCC_LOCK0, SCC_LOCK1);
+	if (!lock_address) {
+		return 0;
+	}
+
+#ifdef CONFIG_SCCSYS_RECURSIVE_PID_LOCK
+	spin_lock_irqsave(&sccsys->lock[pid], flags);
+
+	/* Check for recursive acquire */
+	if (++sccsys->lock_count[pid] != 1) {
+		spin_unlock_irqrestore(&sccsys->lock[pid], flags);
+		return 1;
+	}
+#endif // CONFIG_SCCSYS_RECURSIVE_PID_LOCK
+
+	/* The LOCK bit is clear-on-read i.e. we have exclusive access when
+	 * reading a '1'.
+	 */
+	while (!(readb((void*)lock_address) & 0x1)) cpu_relax();
+
+#ifdef CONFIG_SCCSYS_RECURSIVE_PID_LOCK
+	/* Release the spinlock after acquiring the PID lock, to guard against
+	 * simultaneous calls from different threads. */
+	spin_unlock_irqrestore(&sccsys->lock[pid], flags);
+#endif // CONFIG_SCCSYS_RECURSIVE_PID_LOCK
+
+	return 1;
+}
+EXPORT_SYMBOL(sccsys_acquire_pid_lock);
+
+/* Release the test&set register of the specified PID. This call returns 1 if
+ * the lock has successfully been released, or 0 otherwise.
+ *
+ * In the current implementation, the call does only fail if the PID is invalid
+ * (i.e., outside of the range from 0 to SCC_CORECOUNT-1) or mapping of the
+ * configuration registers has failed.
+ */
+int sccsys_release_pid_lock(scc_pid_t pid)
+{
+#ifdef CONFIG_SCCSYS_RECURSIVE_PID_LOCK
+	unsigned long flags;
+#endif // CONFIG_SCCSYS_RECURSIVE_PID_LOCK
+
+	void* lock_address;
+
+	/* Simply ignore the test&set register if locking is disabled */
+	if (disable_locking) {
+		return 1;
+	}
+  
+	/* Get address of test&set register for target  */
+	lock_address = sccsys_get_crb_entry_for_pid(pid, SCC_LOCK0, SCC_LOCK1);
+	if (!lock_address) {
+		return 0;
+	}
+
+#ifdef CONFIG_SCCSYS_RECURSIVE_PID_LOCK
+	spin_lock_irqsave(&sccsys->lock[pid], flags);
+
+	/* Check for recursive release */
+	if (--sccsys->lock_count[pid] != 0) {
+		spin_unlock_irqrestore(&sccsys->lock[pid], flags);
+		return 1;
+	}
+#endif // CONFIG_SCCSYS_RECURSIVE_PID_LOCK
+
+	/* The LOCK bit can be set by writing an arbitrary value to the
+	   register.
+	 */
+	writeb(0, lock_address);
+
+#ifdef CONFIG_SCCSYS_RECURSIVE_PID_LOCK
+	/* Release the spinlock after releasing the PID lock, to guard against
+	 * simultaneous calls from different threads. */
+	spin_unlock_irqrestore(&sccsys->lock[pid], flags);
+#endif // CONFIG_SCCSYS_RECURSIVE_PID_LOCK
+
+	return 1;
+}
+EXPORT_SYMBOL(sccsys_release_pid_lock);
+
+#define SCCSYS_TRIGGER_MODE_SET		0
+#define SCCSYS_TRIGGER_MODE_EDGE_IRQ	1
+#define SCCSYS_TRIGGER_MODE_LEVEL_IRQ	2
+
+/* Trigger a bit in a core's configuration register */
+static int sccsys_trigger_config_bits(scc_pid_t pid, unsigned int bits, unsigned int irq_clear_mask, int mode)
+{
+	void* conf_address;
+	unsigned int value;
+
+	/* Get address of per-cpu configuration register */
+	conf_address = sccsys_get_crb_entry_for_pid(pid, SCC_GLCFG0, SCC_GLCFG1);
+	if (!conf_address) {
+		return 0;
+	}
+
+	/* Acquire the lock */
+	if (!sccsys_acquire_pid_lock(pid)) {
+		return 0;
+	}
+
+	value = readl(conf_address);
+	if ((mode == SCCSYS_TRIGGER_MODE_LEVEL_IRQ) && (value & bits)) {
+		value &= ~irq_clear_mask;
+		writel(value, conf_address);
+	}
+
+	value |= bits;
+	writel(value, conf_address);
+
+	if (mode == SCCSYS_TRIGGER_MODE_EDGE_IRQ) {
+		value &= ~irq_clear_mask;
+		writel(value, conf_address);
+	}
+
+	/* Release the lock */
+	sccsys_release_pid_lock(pid);
+
+	return 1;
+}
+
+/* Set a bit in a core's configuration register */
+int sccsys_set_config_bits(scc_pid_t pid, unsigned int bits)
+{
+	return sccsys_trigger_config_bits(pid, bits, 0, SCCSYS_TRIGGER_MODE_SET);
+}
+EXPORT_SYMBOL(sccsys_set_config_bits);
+
+/* Trigger an interrupt to a core (directly via the configuration register) */
+int sccsys_trigger_irq_direct(scc_pid_t pid, unsigned int bits, int edgeIrq)
+{
+	return sccsys_trigger_config_bits(pid, bits,
+		SCC_INTR_MASK | SCC_NMI_MASK,
+		edgeIrq ? SCCSYS_TRIGGER_MODE_EDGE_IRQ :
+			  SCCSYS_TRIGGER_MODE_LEVEL_IRQ);
+}
+EXPORT_SYMBOL(sccsys_trigger_irq_direct);
+
+/* Clear a bit for a core's configuration register */
+int sccsys_clear_config_bits(scc_pid_t pid, unsigned int bits)
+{
+	void* conf_address;
+
+	/* Get address of per-cpu configuration register */
+	conf_address = sccsys_get_crb_entry_for_pid(pid, SCC_GLCFG0, SCC_GLCFG1);
+	if (!conf_address) {
+		return 0;
+	}
+
+	/* Acquire the lock */
+	if (!sccsys_acquire_pid_lock(pid)) {
+		return 0;
+	}
+
+	/* Clear the bits */
+	writel(readl(conf_address) & ~bits, conf_address);
+
+	/* Release the lock */
+	sccsys_release_pid_lock(pid);
+
+	return 1;
+}
+EXPORT_SYMBOL(sccsys_clear_config_bits);
+
+/* Clear an interrupt request bit for a processor identified by pid. */
+int sccsys_clear_irq_direct(scc_pid_t pid, unsigned int bits)
+{
+	return sccsys_clear_config_bits(pid, bits);
+}
+EXPORT_SYMBOL(sccsys_clear_irq_direct);
+
+/* Read LUT entry */
+scc_lut_t sccsys_read_lut_entry(scc_pid_t pid, unsigned int index)
+{
+	void* lut_address;
+	scc_lut_t lut;
+
+	lut.raw = 0;
+	if (index > 255) {
+		return lut;
+	}
+
+	/* Get address of per-cpu configuration register */
+	lut_address = sccsys_get_crb_entry_for_pid(pid, SCC_LUT0, SCC_LUT1);
+	if (!lut_address) {
+		return lut;
+	}
+
+	lut.raw = readl(lut_address + SCC_LUT_STRIDE * index);
+	return lut;
+}
+EXPORT_SYMBOL(sccsys_read_lut_entry);
+
+/* Write LUT entry */
+int sccsys_write_lut_entry(scc_pid_t pid, unsigned int index, scc_lut_t lut)
+{
+	void* lut_address;
+
+	if (index > 255) {
+		return 0;
+	}
+
+	/* Get address of per-cpu configuration register */
+	lut_address = sccsys_get_crb_entry_for_pid(pid, SCC_LUT0, SCC_LUT1);
+	if (!lut_address) {
+		return 0;
+	}
+
+	writel(lut.raw, lut_address + SCC_LUT_STRIDE * index);
+	return 1;
+}
+EXPORT_SYMBOL(sccsys_write_lut_entry);
+
+/* Read Global Clock Unit (GCU) configuration register */
+scc_gckcfg_t sccsys_read_gcbcfg(scc_pid_t pid)
+{
+	void* address;
+	scc_gckcfg_t value;
+
+	address = sccsys_get_crb_entry_for_tile_of_pid(pid, SCC_GCBCFG);
+	if (!address) {
+		value.raw = 0;
+	} else {
+		value.raw = readl(address);
+	}
+
+	return value;
+}
+EXPORT_SYMBOL(sccsys_read_gcbcfg);
+
+/* Write Global Clock Unit (GCU) configuration register */
+int sccsys_write_gcbcfg(scc_pid_t pid, scc_gckcfg_t cfg)
+{
+	void* address;
+
+	address = sccsys_get_crb_entry_for_tile_of_pid(pid, SCC_GCBCFG);
+	if (!address) {
+		return 0;
+	}
+
+	writel(cfg.raw, address);
+	return 1;
+}
+EXPORT_SYMBOL(sccsys_write_gcbcfg);
+
+/* Get address of mapped global configuration register bank */
+void* sccsys_get_grb(void)
+{
+	return sccsys->grb;
+}
+EXPORT_SYMBOL(sccsys_get_grb);
+
+/* Read global configuration register */
+unsigned sccsys_read_grb_entry(unsigned int offset)
+{
+	return readl(sccsys->grb + offset);
+}
+EXPORT_SYMBOL(sccsys_read_grb_entry);
+
+/* Write global configuration register */
+void sccsys_write_grb_entry(unsigned int offset, unsigned value)
+{
+	writel(value, sccsys->grb + offset);
+}
+EXPORT_SYMBOL(sccsys_write_grb_entry);
+
+/* Convert a node-local physical address into a system address */
+scc_addr_t sccsys_physical_to_system(scc_pid_t pid, unsigned long pa)
+{
+	unsigned int lut_index = (pa >> 24) & 0xFF;
+	scc_lut_t lut = sccsys_read_lut_entry(pid, lut_index);
+	scc_addr_t sccaddr;
+
+	sccaddr.bypass = lut.bypass;
+	sccaddr.y = lut.y;
+	sccaddr.x = lut.x;
+	sccaddr.subdest = lut.subdest;
+	sccaddr.address = lut.address;
+	sccaddr.offset = pa & 0x00FFFFFF;
+
+	return sccaddr;
+}
+EXPORT_SYMBOL(sccsys_physical_to_system);
+
+/* Read frequency of the fast clock from the global configuration register bank */
+unsigned short sccsys_read_grb_fastclock(void)
+{
+	return (unsigned short)(sccsys_read_grb_entry(SCCGRB_CLKFREQ) & 0xFFFF);
+}
+EXPORT_SYMBOL(sccsys_read_grb_fastclock);
+
+module_init(sccsys_init);
+module_exit(sccsys_exit);
+MODULE_DESCRIPTION("SCC system driver");
+MODULE_AUTHOR("Jan-Arne Sobania <jan-arne.sobania@hpi.uni-potsdam.de>");
+
diff -urN linux-3.1.4/arch/x86/kernel/early_printk.c linux-3.1.4-scc/arch/x86/kernel/early_printk.c
--- linux-3.1.4/arch/x86/kernel/early_printk.c	2011-11-28 23:48:14.000000000 +0100
+++ linux-3.1.4-scc/arch/x86/kernel/early_printk.c	2011-12-20 15:27:07.578352779 +0100
@@ -169,6 +169,37 @@
 	.index =	-1,
 };
 
+#ifdef CONFIG_X86_SCC
+spinlock_t scclock;
+
+static int scc_putc(unsigned char ch)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&scclock, flags);
+	outb(ch, 0x80);
+	spin_unlock_irqrestore(&scclock, flags);
+	return 0;
+}
+
+static void scc_write(struct console *con, const char *s, unsigned n)
+{
+	while (*s && n-- > 0) {
+		if (*s == '\n')
+			scc_putc('\r');
+		scc_putc(*s);
+		s++;
+	}
+}
+
+static struct console scc_console = {
+	.name =		"scccon",
+	.write =	scc_write,
+	.flags =	CON_PRINTBUFFER,
+	.index =	-1,
+};
+#endif
+
 /* Direct interface for emergencies */
 static struct console *early_console = &early_vga_console;
 static int __initdata early_console_initialized;
@@ -251,6 +282,10 @@
 			early_console_register(&early_hsu_console, keep);
 		}
 #endif
+#ifdef CONFIG_X86_SCC
+		if (!strncmp(buf, "scc", 3))
+			early_console_register(&scc_console, keep);
+#endif
 		buf++;
 	}
 	return 0;
diff -urN linux-3.1.4/arch/x86/kernel/head32.c linux-3.1.4-scc/arch/x86/kernel/head32.c
--- linux-3.1.4/arch/x86/kernel/head32.c	2011-11-28 23:48:14.000000000 +0100
+++ linux-3.1.4-scc/arch/x86/kernel/head32.c	2011-12-20 15:27:07.578352779 +0100
@@ -54,6 +54,9 @@
 	case X86_SUBARCH_CE4100:
 		x86_ce4100_early_setup();
 		break;
+	case X86_SUBARCH_SCC:
+		x86_scc_early_setup();
+		break;
 	default:
 		i386_default_early_setup();
 		break;
diff -urN linux-3.1.4/arch/x86/kernel/head_32.S linux-3.1.4-scc/arch/x86/kernel/head_32.S
--- linux-3.1.4/arch/x86/kernel/head_32.S	2011-11-28 23:48:14.000000000 +0100
+++ linux-3.1.4-scc/arch/x86/kernel/head_32.S	2011-12-20 15:27:07.578352779 +0100
@@ -259,6 +259,8 @@
 	.long lguest_entry		/* lguest hypervisor */
 	.long xen_entry			/* Xen hypervisor */
 	.long default_entry		/* Moorestown MID */
+	.long default_entry		/* CE4100 */
+	.long default_entry		/* SCC */
 num_subarch_entries = (. - subarch_entries) / 4
 .previous
 #else
diff -urN linux-3.1.4/arch/x86/kernel/rtc.c linux-3.1.4-scc/arch/x86/kernel/rtc.c
--- linux-3.1.4/arch/x86/kernel/rtc.c	2011-11-28 23:48:14.000000000 +0100
+++ linux-3.1.4-scc/arch/x86/kernel/rtc.c	2011-12-20 15:27:07.578352779 +0100
@@ -157,6 +157,7 @@
 /* Routines for accessing the CMOS RAM/RTC. */
 unsigned char rtc_cmos_read(unsigned char addr)
 {
+#ifndef CONFIG_X86_SCC
 	unsigned char val;
 
 	lock_cmos_prefix(addr);
@@ -165,15 +166,20 @@
 	lock_cmos_suffix(addr);
 
 	return val;
+#else
+	return 0;
+#endif
 }
 EXPORT_SYMBOL(rtc_cmos_read);
 
 void rtc_cmos_write(unsigned char val, unsigned char addr)
 {
+#ifndef CONFIG_X86_SCC
 	lock_cmos_prefix(addr);
 	outb(addr, RTC_PORT(0));
 	outb(val, RTC_PORT(1));
 	lock_cmos_suffix(addr);
+#endif
 }
 EXPORT_SYMBOL(rtc_cmos_write);
 
diff -urN linux-3.1.4/arch/x86/kernel/tsc.c linux-3.1.4-scc/arch/x86/kernel/tsc.c
--- linux-3.1.4/arch/x86/kernel/tsc.c	2011-11-28 23:48:14.000000000 +0100
+++ linux-3.1.4-scc/arch/x86/kernel/tsc.c	2011-12-20 15:27:07.578352779 +0100
@@ -668,6 +668,9 @@
 }
 
 #ifdef CONFIG_CPU_FREQ
+#ifdef CONFIG_X86_SCC_TSC_TRACK_CPU_FREQ
+static struct clocksource clocksource_tsc;
+#endif
 
 /* Frequency scaling support. Adjust the TSC based timer when the cpu frequency
  * changes.
@@ -716,6 +719,20 @@
 
 	set_cyc2ns_scale(tsc_khz, freq->cpu);
 
+#ifdef CONFIG_X86_SCC_TSC_TRACK_CPU_FREQ
+	/* Update the clocksource with the new TSC frequency. */
+	__clocksource_updatefreq_khz(&clocksource_tsc, tsc_khz);
+
+	/* If the clock source is currently being used, simply updating its
+	 * frequency will not make the timekeeper honor the new values. Instead,
+	 * we need to switch to another time source (which can be anything),
+	 * then switch back to the TSC. */
+	if (timekeeping_get_clock() == &clocksource_tsc) {
+		timekeeping_notify(clocksource_default_clock());
+		timekeeping_notify(&clocksource_tsc);
+	}
+#endif // CONFIG_X86_SCC_TSC_TRACK_CPU_FREQ
+
 	return 0;
 }
 
diff -urN linux-3.1.4/arch/x86/mm/dump_pagetables.c linux-3.1.4-scc/arch/x86/mm/dump_pagetables.c
--- linux-3.1.4/arch/x86/mm/dump_pagetables.c	2011-11-28 23:48:14.000000000 +0100
+++ linux-3.1.4-scc/arch/x86/mm/dump_pagetables.c	2011-12-20 15:27:07.578352779 +0100
@@ -124,6 +124,14 @@
 				seq_printf(m, "PSE ");
 			else
 				seq_printf(m, "    ");
+#ifdef CONFIG_X86_SCC
+		} else if ((level == 4) && ((read_cr4() & (X86_CR4_PSE |
+			X86_CR4_MPE)) == X86_CR4_MPE))  {
+			if (pr & _PAGE_PMB)
+				seq_printf(m, "PMB ");
+			else
+				seq_printf(m, "    ");
+#endif
 		} else {
 			if (pr & _PAGE_PAT)
 				seq_printf(m, "pat ");
diff -urN linux-3.1.4/arch/x86/mm/ioremap.c linux-3.1.4-scc/arch/x86/mm/ioremap.c
--- linux-3.1.4/arch/x86/mm/ioremap.c	2011-11-28 23:48:14.000000000 +0100
+++ linux-3.1.4-scc/arch/x86/mm/ioremap.c	2011-12-20 15:27:07.578352779 +0100
@@ -71,6 +71,7 @@
 	pgprot_t prot;
 	int retval;
 	void __iomem *ret_addr;
+	int mpbt_flag = (prot_val & _PAGE_PSE) != 0;
 
 	/* Don't allow wraparound or zero size */
 	last_addr = phys_addr + size - 1;
@@ -145,6 +146,11 @@
 		break;
 	}
 
+	/* SCC: set the MPBT flag if it has been requested before */
+	if (mpbt_flag) {
+		prot = __pgprot(pgprot_val(prot) | _PAGE_PSE);
+	}
+
 	/*
 	 * Ok, go for it..
 	 */
@@ -250,6 +256,13 @@
 }
 EXPORT_SYMBOL(ioremap_prot);
 
+void __iomem *ioremap_mpbt(resource_size_t phys_addr, unsigned long size)
+{
+	return __ioremap_caller(phys_addr, size, _PAGE_PSE | _PAGE_CACHE_WC,
+				__builtin_return_address(0));
+}
+EXPORT_SYMBOL(ioremap_mpbt);
+
 /**
  * iounmap - Free a IO remapping
  * @addr: virtual address from ioremap_*
@@ -567,6 +580,13 @@
 	return __early_ioremap(phys_addr, size, PAGE_KERNEL_IO);
 }
 
+/* Remap an IO device (without caching) */
+void __init __iomem *
+early_ioremap_nocache(resource_size_t phys_addr, unsigned long size)
+{
+	return __early_ioremap(phys_addr, size, PAGE_KERNEL_IO_NOCACHE);
+}
+
 /* Remap memory */
 void __init __iomem *
 early_memremap(resource_size_t phys_addr, unsigned long size)
diff -urN linux-3.1.4/arch/x86/mm/pageattr.c linux-3.1.4-scc/arch/x86/mm/pageattr.c
--- linux-3.1.4/arch/x86/mm/pageattr.c	2011-11-28 23:48:14.000000000 +0100
+++ linux-3.1.4-scc/arch/x86/mm/pageattr.c	2011-12-20 15:27:07.578352779 +0100
@@ -998,6 +998,37 @@
 }
 EXPORT_SYMBOL(set_memory_uc);
 
+#ifdef CONFIG_X86_SCC
+int set_memory_mpbt(unsigned long start, int numpages)
+{
+	unsigned int i, level;
+	unsigned long addr;
+
+	BUG_ON(irqs_disabled());
+	WARN_ON(PAGE_ALIGN(start) != start);
+
+	on_each_cpu(__cpa_flush_range, NULL, 1);
+
+	for (i = 0, addr = start; i < numpages; i++, addr += PAGE_SIZE) {
+		pte_t *pte = lookup_address(addr, &level);
+
+		if (pte) {
+			pte_mkhuge(*pte);
+		}
+	}
+	return 0;
+	//return change_page_attr_set(&start, numpages, __pgprot(_PAGE_PSE), 0);
+}
+EXPORT_SYMBOL(set_memory_mpbt);
+
+int set_memory_wt_mpbt(unsigned long addr, int numpages)
+{
+	return change_page_attr_set(&addr, numpages,
+				    __pgprot(_PAGE_CACHE_WC | _PAGE_PMB), 0);
+}
+EXPORT_SYMBOL(set_memory_wt_mpbt);
+#endif
+
 int _set_memory_array(unsigned long *addr, int addrinarray,
 		unsigned long new_type)
 {
@@ -1172,6 +1203,24 @@
 }
 EXPORT_SYMBOL(set_pages_uc);
 
+#ifdef CONFIG_X86_SCC
+int set_pages_mpbt(struct page *page, int numpages)
+{
+	unsigned long addr = (unsigned long)page_address(page);
+
+	return set_memory_mpbt(addr, numpages);
+}
+EXPORT_SYMBOL(set_pages_mpbt);
+
+int set_pages_wt_mpbt(struct page *page, int numpages)
+{
+	unsigned long addr = (unsigned long)page_address(page);
+
+	return set_memory_wt_mpbt(addr, numpages);
+}
+EXPORT_SYMBOL(set_pages_wt_mpbt);
+#endif
+
 static int _set_pages_array(struct page **pages, int addrinarray,
 		unsigned long new_type)
 {
diff -urN linux-3.1.4/arch/x86/platform/Makefile linux-3.1.4-scc/arch/x86/platform/Makefile
--- linux-3.1.4/arch/x86/platform/Makefile	2011-11-28 23:48:14.000000000 +0100
+++ linux-3.1.4-scc/arch/x86/platform/Makefile	2011-12-20 15:27:07.578352779 +0100
@@ -8,3 +8,4 @@
 obj-y	+= sfi/
 obj-y	+= visws/
 obj-y	+= uv/
+obj-y	+= scc/
diff -urN linux-3.1.4/arch/x86/platform/scc/Makefile linux-3.1.4-scc/arch/x86/platform/scc/Makefile
--- linux-3.1.4/arch/x86/platform/scc/Makefile	1970-01-01 01:00:00.000000000 +0100
+++ linux-3.1.4-scc/arch/x86/platform/scc/Makefile	2011-12-20 15:27:07.578352779 +0100
@@ -0,0 +1 @@
+obj-$(CONFIG_X86_SCC)		+= scc.o
diff -urN linux-3.1.4/arch/x86/platform/scc/scc.c linux-3.1.4-scc/arch/x86/platform/scc/scc.c
--- linux-3.1.4/arch/x86/platform/scc/scc.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-3.1.4-scc/arch/x86/platform/scc/scc.c	2011-12-20 15:27:07.578352779 +0100
@@ -0,0 +1,290 @@
+/*
+ * scc.c: SCC platform specific setup code
+ *
+ * (C) Copyright 2011 Jan-Arne Sobania, Hasso-Plattner-Institut
+ * Author: Jan-Arne Sobania (jan-arne.sobania@hpi.uni-potsdam.de)
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; version 2
+ * of the License.
+ */
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/irq.h>
+#include <linux/interrupt.h>
+#include <linux/module.h>
+#include <linux/mm_types.h>
+
+#include <asm/setup.h>
+#include <asm/apic.h>
+#include <asm/lapic.h>
+#include <asm/io.h>
+#include <asm/i8259.h>
+#include <asm/e820.h>
+#include <asm/paravirt.h>
+
+#include <linux/sccsys.h>	/* for CRB/GRB definitions only */
+
+
+static int scc_subarch_enabled = 0;
+static unsigned long scc_boot_busclock = CONFIG_SCC_BUSCLOCK;
+
+/*
+ * Here we do all the Local APIC initialization and stuff required to get the wormhole
+ * IRQ delivered through LINT0 / 1
+ *
+ * LINT 0 services the UART at address 0x3f8 using IRQ4,
+ * LINT 1 services the UART at address 0x2f8 using IRQ3.
+ */
+
+struct sccirq_entry {
+	u32 reg;
+	const char* name;
+};
+
+struct sccirq_entry sccirqs[] = {
+	{ APIC_LVT1, "LINT1" },
+	{ APIC_LVT0, "LINT0" }
+};
+
+#define SCCIRQ_FIRST_VECTOR	3
+#define SCCIRQ_LAST_VECTOR	4
+
+#define LVT0_IRQ 4
+#define LVT1_IRQ 3
+
+static struct sccirq_entry* sccirq_get_entry(int irq)
+{
+	if (irq >= SCCIRQ_FIRST_VECTOR && irq <= SCCIRQ_LAST_VECTOR) {
+		return &sccirqs[irq - SCCIRQ_FIRST_VECTOR];
+	} else {
+		panic("SCC: unable to map irq %d to LVT register.\n", irq);
+	}
+}
+
+static void sccirq_mask_lapic(struct irq_data *data)
+{
+	set_lapic_mask(sccirq_get_entry(data->irq)->reg, data->irq);
+}
+
+static void sccirq_unmask_lapic(struct irq_data *data)
+{
+	unset_lapic_mask(sccirq_get_entry(data->irq)->reg, data->irq);
+}
+
+static void sccirq_ack_lapic(struct irq_data *data)
+{
+	ack_APIC_irq();
+}
+
+static struct irq_chip sccirq_lapic_chip __read_mostly = {
+	.name		= "LAPIC-WORMHOLE",
+	.irq_mask	= sccirq_mask_lapic,
+	.irq_unmask	= sccirq_unmask_lapic,
+	.irq_ack	= sccirq_ack_lapic,
+	.irq_disable    = sccirq_mask_lapic,
+};
+
+
+void __init scc_setup_local_APIC_LINT(void)
+{
+	// set up LVT0 and LVT1 to point to IRQ4/3 vector
+	// set up LINT 0 and LINT 1 IRQ handling through LAPIC
+	apic_write(APIC_LVT0, (IRQ0_VECTOR + LVT0_IRQ) | APIC_LVT_MASKED | (APIC_MODE_FIXED << 8));
+	apic_write(APIC_LVT1, (IRQ0_VECTOR + LVT1_IRQ) | APIC_LVT_MASKED | (APIC_MODE_FIXED << 8));
+}
+
+#ifdef CONFIG_SCC_QUERY_FREQUENCY_FROM_FPGA
+/*
+ * Read a single unsigned int from a physical address. This routine maps the
+ * page of physical memory using a non-caching version of early_ioremap, so it
+ * should be called sparingly, and only until the full ioremap is available. */
+static unsigned int __init scc_early_readl_phys(unsigned int phys)
+{
+	unsigned int phys_page = phys & ~(PAGE_SIZE - 1);
+	unsigned int phys_offset = phys - phys_page;
+	unsigned int value;
+	void* base;
+
+	base = early_ioremap_nocache(phys_page, PAGE_SIZE);
+	value = readl(base + phys_offset);
+	early_iounmap(base, PAGE_SIZE);
+
+	return value;
+}
+
+/*
+ * Query tile clock. This code is an exact duplicate of the one from sccfreq,
+ * but we need to include it here because the driver is not initialized at this
+ * point, or may never be loaded at all.
+ */
+static unsigned int __init scc_early_query_tile_frequency(void)
+{
+	unsigned int fastclock = 0;
+	unsigned int divider = 0;
+	unsigned int freq = 0;
+
+	/* Get fastclock */
+	fastclock = scc_early_readl_phys(0xF9000000 + SCCGRB_CLKFREQ) & 0xFFFF;
+
+	/* Get core divider */
+	divider = scc_early_readl_phys(0xF8000000 + SCC_GCBCFG);
+	divider = (divider >> 8) & 0xF;
+	divider++;
+
+	/* Fallback to 533MHz */
+	if (!divider || !fastclock) {
+		freq = 533000;
+	} else {
+		/* Calculate new frequency */
+		freq = fastclock / divider * 1000;
+	}
+
+	/* return frequency */
+	return freq;
+}
+#endif // CONFIG_SCC_QUERY_FREQUENCY_FROM_FPGA
+
+/*
+ * SCC architecture initialization.
+ */
+static void __init scc_arch_setup(void)
+{
+	unsigned long real_busclock_khz;
+
+	if (boot_cpu_data.x86 == 5 && boot_cpu_data.x86_model == 2) {
+		printk(KERN_NOTICE "SCC: Intel GaussLake/P54C identified\n");
+
+#ifdef CONFIG_SCC_QUERY_FREQUENCY_FROM_FPGA
+		/* Read busclock from FPGA */
+		real_busclock_khz = scc_early_query_tile_frequency();
+
+		if (scc_boot_busclock == real_busclock_khz * 1000) {
+			printk(KERN_NOTICE "     Configured busclock of %lu.%03lu mHz matches detected busclock.\n",
+				scc_boot_busclock / 1000000,
+				(scc_boot_busclock / 1000) % 1000);
+		} else {
+			printk(KERN_NOTICE "     Overwriting configured busclock of %lu.%03lu mHz with detected busclock of %lu.%03lu mHz.\n",
+				scc_boot_busclock / 1000000,
+				(scc_boot_busclock / 1000) % 1000,
+				real_busclock_khz / 1000,
+				real_busclock_khz % 1000);
+			scc_boot_busclock = real_busclock_khz * 1000;
+		}
+#endif // CONFIG_SCC_QUERY_FREQUENCY_FROM_FPGA
+	} else
+		printk(KERN_NOTICE "SCC: Unknown CPU (%d:%d)\n",
+			boot_cpu_data.x86,
+			boot_cpu_data.x86_model);
+}
+
+/* Get bus frequency at boot time. */
+unsigned long scc_get_boot_busclock(void)
+{
+	return scc_boot_busclock;
+}
+EXPORT_SYMBOL_GPL(scc_get_boot_busclock);
+
+/*
+ * There is no legacy interrupt controller in the system, so there is also no
+ * need to perform initialization of ISA interrupts.
+ */
+static void __init scc_pre_vector_init(void)
+{
+	int irq;
+
+	/* Register wormhole interrupts. */
+	for (irq = SCCIRQ_FIRST_VECTOR; irq <= SCCIRQ_LAST_VECTOR; irq++) {
+		/* Add vector to IRQ mapping */
+		per_cpu(vector_irq, 0)[IRQ0_VECTOR + irq] = irq;
+
+		/* Set flags and install handler */
+		irq_clear_status_flags(irq, IRQ_LEVEL);
+		irq_set_chip_and_handler_name(irq, &sccirq_lapic_chip,
+			handle_edge_irq, sccirq_get_entry(irq)->name);
+	}
+}
+
+/*
+ * The per-cpu clock source is the LAPIC clock. However, this routine also gets
+ * called just in time to complete initialization of the wormhole interrupts.
+ */
+static void __init scc_setup_boot_cpu_clockev(void)
+{
+	scc_setup_local_APIC_LINT();
+	setup_boot_APIC_clock();
+}
+
+/*
+ * SCC systems don't have a secondary clock source, so we cannot perform the
+ * default calibration protocol for the TSC. Instead, we rely on the kernel
+ * configuration specifying the real bus block.
+ */
+static unsigned long __init scc_calibrate_tsc(void)
+{
+	unsigned long tsc_khz = scc_boot_busclock / 1000; /* in kHz */
+
+	printk(KERN_INFO "SCC: TSC calibration skipped. Returning pre-configured BUSCLOCK value of %lu.%03lu mHz.\n",
+		tsc_khz / 1000, tsc_khz % 1000);
+
+	return tsc_khz;
+}
+
+/* SCC systems don't have an i8042 controller */
+static int scc_i8042_detect(void)
+{
+	return 0;
+}
+
+/* Check whether we are running on the lowest layer on SCC hardware */
+int scc_bare_metal(void)
+{
+	return scc_subarch_enabled && !paravirt_enabled();
+}
+EXPORT_SYMBOL(scc_bare_metal);
+
+/*
+ * SCC specific x86_init function overrides and early setup
+ * calls.
+ */
+void __init x86_scc_early_setup(void)
+{
+	/* We are about to use the SCC sub-architecture */
+	scc_subarch_enabled = 1;
+
+	/* Architecture initialization */
+	x86_init.oem.arch_setup = scc_arch_setup;
+
+	/* Static configuration of interrupt vectors */
+	x86_init.irqs.pre_vector_init = scc_pre_vector_init;
+	legacy_pic = &null_legacy_pic;
+
+	/* Calibrate timestamp counter */
+	x86_platform.calibrate_tsc = scc_calibrate_tsc;
+
+	/* Timer. NOPed to prevent the legacy PIT from being initialized.
+	 * Please note that legacy_pic->nr_legacy_itqs must be equal to 0,
+	 * or io_apic.c!print_PIC will try to access PIT registers anyway.
+	 */
+	x86_init.timers.timer_init = x86_init_noop;
+
+	/* Per-CPU clock source. This eventually delegates to the default
+	 * routine of setup_boot_APIC_clock, but we need to complete LAPIC
+	 * initialization first by registering the wormhole interrupts.
+	 */
+	x86_init.timers.setup_percpu_clockev = scc_setup_boot_cpu_clockev;
+
+	/* Detect 8042 (keyboard controller). NOPed. */
+	x86_platform.i8042_detect = scc_i8042_detect;
+
+	/* Avoid searching for BIOS MP tables */
+	x86_init.mpparse.find_smp_config = x86_init_noop;
+	x86_init.mpparse.get_smp_config = x86_init_uint_noop;
+
+	/* Avoid searching for extension ROMs */
+	x86_init.resources.probe_roms = x86_init_noop;
+
+	/* Avoid reserving legacy PC resources */
+	x86_init.resources.reserve_resources = x86_init_noop;
+}
diff -urN linux-3.1.4/drivers/char/Kconfig linux-3.1.4-scc/drivers/char/Kconfig
--- linux-3.1.4/drivers/char/Kconfig	2011-11-28 23:48:14.000000000 +0100
+++ linux-3.1.4-scc/drivers/char/Kconfig	2011-12-20 15:27:07.578352779 +0100
@@ -4,6 +4,31 @@
 
 menu "Character devices"
 
+config SCCMEM
+	bool "SCC system memory driver"
+	default n
+	depends on SCCSYS
+	---help---
+	  SCC memory driver. This driver provides the rckncm, rckmpb and
+	  rckdcm devices to applications.
+
+config SCCPERF
+	tristate "SCC performance meter driver"
+	default n
+	depends on SCCSYS
+	---help---
+	  This driver reports the core's CPU utilization to the MCPC via shared
+	  memory. It can be enabled if using the modified cpuutil program is
+	  not desired.
+
+config SCCPROC
+	bool "SCC procfs driver"
+	default n
+	depends on SCCSYS
+	---help---
+	  This driver provides SCC-specific entries in the /proc file
+	  system.
+
 source "drivers/tty/Kconfig"
 
 config DEVKMEM
diff -urN linux-3.1.4/drivers/char/Makefile linux-3.1.4-scc/drivers/char/Makefile
--- linux-3.1.4/drivers/char/Makefile	2011-11-28 23:48:14.000000000 +0100
+++ linux-3.1.4-scc/drivers/char/Makefile	2011-12-20 15:27:07.578352779 +0100
@@ -2,6 +2,10 @@
 # Makefile for the kernel character device drivers.
 #
 
+obj-$(CONFIG_SCCMEM)		+= sccmem.o
+obj-$(CONFIG_SCCPERF)		+= sccperf.o
+obj-y				+= sccproc/
+
 obj-y				+= mem.o random.o
 obj-$(CONFIG_TTY_PRINTK)	+= ttyprintk.o
 obj-y				+= misc.o
diff -urN linux-3.1.4/drivers/char/sccmem.c linux-3.1.4-scc/drivers/char/sccmem.c
--- linux-3.1.4/drivers/char/sccmem.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-3.1.4-scc/drivers/char/sccmem.c	2011-12-20 15:27:07.578352779 +0100
@@ -0,0 +1,498 @@
+/*
+ *  linux/drivers/char/sccmem.c
+ *  
+ *  -> derived from:
+ *  linux/drivers/char/mem.c
+ *
+ *  Copyright (C) 1991, 1992  Linus Torvalds
+ *
+ *  Added devfs support. 
+ *    Jan-11-1998, C. Scott Ananian <cananian@alumni.princeton.edu>
+ *  Shared /dev/zero mmaping support, Feb 2000, Kanoj Sarcar <kanoj@sgi.com>
+ */
+
+#include <linux/version.h>
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/fs.h>
+#include <linux/cdev.h>
+#include <linux/slab.h>
+#include <linux/vmalloc.h>
+#include <linux/mm.h>
+#include <linux/miscdevice.h>
+#include <linux/highmem.h>
+
+#ifdef MODVERSIONS
+#  include <linux/modversions.h>
+#endif
+#include <asm/io.h>
+
+#include <linux/sccsys.h>
+
+/* DEBUG messages */
+#define DEBUG_MSG 0
+#define MAJOR_ADDR 222
+#define PRINTD(format, args...) if (DEBUG_MSG) { printk(format, ##args); }
+
+/* Symbols */
+#define OWN_MPB     0xd8000000
+
+#define MPBADDRBITS 13
+#define MPBSIZE     (1<<MPBADDRBITS)
+
+/* PSE bit for Pentium+ equals MPE (message buffer enable) flag in SCC! So, use it to create _PAGE_MPB symbol... */
+#define _PAGE_MPE _PAGE_PSE
+
+/* In order to force write-backs of modified data from the L2 cache we have
+ * to read new lines into every way. At 256kB, 4-way, 32B/line there are
+ * 2048 sets, i.e. the distances between addresses in a pair of ways is
+ * 2048*32B = 64kB.
+ */
+#define L2_LINESIZE 32UL
+#define L2_WAYS     4UL
+#define L2_CAPACITY (256*1024UL)
+#define L2_WBSTRIDE (L2_CAPACITY/L2_WAYS)
+
+/* Enable this to have the L2 flush routine execute a 'wbinvd'
+ * instruction before doing the actual flush. */
+// #define FLUSH_USES_WBINVD
+
+
+/* Virtual address and current offset in our flush-data block,
+ * which will be in an unused MPB address range */
+static size_t OWN_MPB_vaddr = 0;
+static size_t OWN_MPB_offset = 0;
+
+
+/* Methods of the character device */
+static int sccmem_open(struct inode *inode, struct file *filp);
+static int sccmem_release(struct inode *inode, struct file *filp);
+static int sccdcm_mmap(struct file *filp, struct vm_area_struct *vma);
+static int sccncm_mmap(struct file *filp, struct vm_area_struct *vma);
+static int sccmpb_mmap(struct file *filp, struct vm_area_struct *vma);
+/* For simplicity reasons, exising file operations are overloaded with
+ * new functionality:
+ * -read()  returns the physical address of the buffer
+ * -write() force write-backs of modified data from the caches
+ */
+static ssize_t sccdcm_read(struct file *filp,
+                           char __user *buf, 
+                           size_t count, 
+                           loff_t *ppos);
+static ssize_t sccdcm_write(struct file *filp, 
+                            const char __user *buf, 
+                            size_t count, 
+                            loff_t *ppos);
+
+/* The file operations, i.e. all character device methods */
+static struct file_operations sccdcm_fops = {
+	.open = sccmem_open, 
+        .release = sccmem_release, 
+        .mmap = sccdcm_mmap, 
+        .read = sccdcm_read,
+        .write = sccdcm_write,
+        .owner = THIS_MODULE
+};
+
+static struct file_operations sccncm_fops = {
+	.open = sccmem_open, 
+        .release = sccmem_release, 
+        .mmap = sccncm_mmap, 
+        .owner = THIS_MODULE
+};
+
+static struct file_operations sccmpb_fops = {
+	.open = sccmem_open, 
+        .release = sccmem_release, 
+        .mmap = sccmpb_mmap, 
+        .owner = THIS_MODULE
+};
+
+/* character device open method */
+static int sccmem_open(struct inode *inode, struct file *filp) {
+  return 0;
+}
+
+/* character device last close method */
+static int sccmem_release(struct inode *inode, struct file *filp) {
+  return 0;
+}
+
+/* Character device mmap method for noncachable memory mapped registers like CRB->GLCFG0 */
+static int sccncm_mmap(struct file *filp, struct vm_area_struct *vma) {
+  int minor = iminor(filp->f_dentry->d_inode);
+  int major = imajor(filp->f_dentry->d_inode);
+  size_t size = vma->vm_end - vma->vm_start;
+
+  PRINTD(KERN_DEBUG "sccncm_mmap: Device 0x%08lx (%d:%d),\n", (unsigned long)filp->f_dentry->d_inode->i_rdev, major, minor);
+  PRINTD(KERN_DEBUG "             VM Start: 0x%08lx, size: 0x%08lx, offset: 0x%08lx\n", (unsigned long) vma->vm_start, (unsigned long) size, (unsigned long) vma->vm_pgoff);
+
+  /* Mark the page protection value as "uncacheable" */
+  vma->vm_page_prot = __pgprot(pgprot_val(vma->vm_page_prot) | _PAGE_PCD | _PAGE_PWT);
+  PRINTD(KERN_DEBUG "             VM pgprot value is 0x%02lx!\n", pgprot_val(vma->vm_page_prot));
+
+  /* Remap-pfn-range will mark the range VM_IO and VM_RESERVED */
+  if (remap_pfn_range(vma,  vma->vm_start, vma->vm_pgoff, size, vma->vm_page_prot)) return -EAGAIN;
+
+  return 0;
+}
+
+/* Character device mmap method for cachable regular memory */
+static int sccdcm_mmap(struct file *filp, struct vm_area_struct *vma) {
+  int minor = iminor(filp->f_dentry->d_inode);
+  int major = imajor(filp->f_dentry->d_inode);
+  size_t size = vma->vm_end - vma->vm_start;
+
+  PRINTD(KERN_DEBUG "sccdcm_mmap: Device 0x%08lx (%d:%d),\n", (unsigned long)filp->f_dentry->d_inode->i_rdev, major, minor);
+  PRINTD(KERN_DEBUG "             VM Start: 0x%08lx, size: 0x%08lx, offset: 0x%08lx\n", (unsigned long) vma->vm_start, (unsigned long) size, (unsigned long) vma->vm_pgoff);
+
+  /* Mark the page protection value as "cacheable" */
+  vma->vm_page_prot = __pgprot((pgprot_val(vma->vm_page_prot) & ~(_PAGE_PCD | _PAGE_PWT)) );
+  PRINTD(KERN_DEBUG "             VM pgprot value is 0x%02lx!\n", pgprot_val(vma->vm_page_prot));
+
+  /* Remap-pfn-range will mark the range VM_IO and VM_RESERVED */
+  if (remap_pfn_range(vma, vma->vm_start, vma->vm_pgoff, size, vma->vm_page_prot)) return -EAGAIN;
+
+  return 0;
+}
+
+/* Character device mmap method for cachable MPB-type memory */
+static int sccmpb_mmap(struct file *filp, struct vm_area_struct *vma) {
+  int minor = iminor(filp->f_dentry->d_inode);
+  int major = imajor(filp->f_dentry->d_inode);
+  size_t size = vma->vm_end - vma->vm_start;
+
+  PRINTD(KERN_DEBUG "sccmpb_mmap: Device 0x%08lx (%d:%d),\n", (unsigned long)filp->f_dentry->d_inode->i_rdev, major, minor);
+  PRINTD(KERN_DEBUG "             VM Start: 0x%08lx, size: 0x%08lx, offset: 0x%08lx\n", (unsigned long) vma->vm_start, (unsigned long) size, (unsigned long) vma->vm_pgoff);
+
+  /* Mark the page protection value as "cacheable" MPB type */
+  vma->vm_page_prot = __pgprot((pgprot_val(vma->vm_page_prot) & ~(_PAGE_PCD | _PAGE_PWT)) | _PAGE_MPE);
+  PRINTD(KERN_DEBUG "             VM pgprot value is 0x%02lx!\n", pgprot_val(vma->vm_page_prot));
+
+  /* Remap-pfn-range will mark the range VM_IO and VM_RESERVED */
+  if (remap_pfn_range(vma, vma->vm_start, vma->vm_pgoff, size, vma->vm_page_prot)) return -EAGAIN;
+
+  return 0;
+}
+
+
+
+/* Character device read method to return the physical address
+ */
+static ssize_t sccdcm_read(struct file *filp, 
+                           char __user *buf, 
+                           size_t count, 
+                           loff_t *ppos)
+{
+  struct task_struct  *tsk;
+  struct mm_struct    *mm;
+  
+  pgd_t *pgd;
+  pud_t *pud;
+  pmd_t *pmd;
+  pte_t *pte;
+  
+  unsigned long physAddress;
+  unsigned long address = (unsigned long)buf;
+  
+  /* First get our process and memory information */
+  pte = 0;
+  tsk = current;
+  mm  = tsk->mm;
+    
+  pgd = pgd_offset(mm, address);
+  if (pgd_none(*pgd) ) return -EADDRNOTAVAIL; 
+  pud = pud_offset(pgd, address);
+  if (pud_none(*pud) ) return -EADDRNOTAVAIL;
+  pmd = pmd_offset(pud, address);
+  if (pmd_none(*pmd) ) return -EADDRNOTAVAIL;
+  pte = pte_offset_map(pmd, address);
+
+  if (pte_none(*pte) )
+  {
+    pte_unmap(pte);
+    return -EADDRNOTAVAIL;
+  }
+
+  physAddress = pte_val(*pte);
+  physAddress &= PAGE_MASK;
+  physAddress |= (address & ~PAGE_MASK);
+  
+  pte_unmap(pte);
+  
+  return physAddress;
+}
+
+
+/* Helper function to purge a specific set from the caches
+ * by reading invalid data into all ways
+ */
+__attribute__((always_inline)) static inline void sccdcm_purgeSet(unsigned long set)
+{
+  register char   tmp;
+
+  /* Translate the set to a kernel space virtual address */
+  const volatile char*  dummyData = (volatile char*)set;
+  
+  /* Now read new data into all four ways */
+  tmp = *dummyData;
+  tmp = *(dummyData + L2_WBSTRIDE);
+  tmp = *(dummyData + L2_WBSTRIDE * 2);
+  tmp = *(dummyData + L2_WBSTRIDE * 3);
+}
+
+
+/* Character device write method to write back modified data from the caches
+ * to main memory (cacheable memory, only)
+ */
+static ssize_t sccdcm_write(struct file *filp, 
+                            const char __user *buf, 
+                            size_t count, 
+                            loff_t *ppos)
+{
+  struct task_struct  *tsk;
+  struct mm_struct    *mm;
+
+  pgd_t *pgd;
+  pud_t *pud;
+  pmd_t *pmd;
+  pte_t *pte;
+
+  size_t        pos         = 0;
+  size_t        purgebase   = 0;
+  unsigned long physAddress = 0;
+  unsigned long address     = (unsigned long)buf;
+  unsigned long flags;
+
+
+  PRINTD(KERN_INFO "sccdcm_write(): Flushing %d bytes from 0x%08lX\n",
+                    count, address);
+
+  /* First disable interrupts to be sure nobody else accesses the cache
+   * in parallel and messes up the LRU state bits.
+   */
+  local_irq_save(flags);
+
+  /* Flip-flop between two parts of the flush area */
+  if(OWN_MPB_offset == 0)
+    OWN_MPB_offset = L2_CAPACITY;
+  else
+    OWN_MPB_offset = 0;
+
+  /* Set basepointer to flush area we will use in this flush */
+  purgebase = OWN_MPB_vaddr + OWN_MPB_offset;
+
+#ifdef FLUSH_USES_WBINVD
+  /* Now write-back and invalidate the L1 content so subsequent reads
+   * cannot trigger evictions which could again mess up L2 LRU state.
+   * It is not clear that this is required - disabling it saves about 10K cycles */
+  __asm__ volatile ( "wbinvd;\n\t" );
+#endif
+
+  /* In order to improve performance, check whether the entire cache has to
+   * be flushed first; use count=0 as shortcut.
+   */
+  if ((count==0) || (count>=L2_WBSTRIDE))
+  {
+    for (physAddress=purgebase; physAddress<purgebase+L2_WBSTRIDE; physAddress+=L2_LINESIZE)
+      sccdcm_purgeSet(physAddress);
+  }
+  else
+  {
+    /* First get our process and memory information */
+    pte = 0;
+    tsk = current;
+    mm  = tsk->mm;
+
+    /* Align the address to cacheline boundaries */
+    address &= ~(L2_LINESIZE-1);
+
+    /* Loop over the entire range which shall be purged from the caches */
+    pos = 0;
+    while (pos < count)
+    {
+      /* Get the page table information so we can calculate the physical
+       * address. This is only necessary when moving to a new page.
+       */
+      if (pos==0 || (address+pos)%PAGE_SIZE==0)
+      {
+        /* Enable interrupts for the address lookup in order to handle
+         * potential faults and errors.
+         */
+        local_irq_restore(flags);
+
+        pgd = pgd_offset(mm, address);
+        if (pgd_none(*pgd) ) return -EADDRNOTAVAIL;
+        pud = pud_offset(pgd, address);
+        if (pud_none(*pud) ) return -EADDRNOTAVAIL;
+        pmd = pmd_offset(pud, address);
+        if (pmd_none(*pmd) ) return -EADDRNOTAVAIL;
+        pte = pte_offset_map(pmd, address);
+              if (pte_none(*pte) )
+        {
+          pte_unmap(pte);
+          return -EADDRNOTAVAIL;
+        }
+        physAddress = pte_val(*pte);
+        pte_unmap(pte);
+
+        /* Disable interrupts again while purging the data from the cache */
+        local_irq_save(flags);
+      }
+
+      /* Determine the physical address by taking the MSBs from the PTE
+       * and the offset within the page from the virtual space address.
+       */
+      physAddress &= PAGE_MASK;
+      physAddress += ((address+pos) & ~PAGE_MASK);
+
+      sccdcm_purgeSet((physAddress % L2_WBSTRIDE) + purgebase);
+
+      /* We only have to read once per cacheline */
+      pos += L2_LINESIZE;
+    }
+  }
+
+
+  /* Enable normal operation again */
+  local_irq_restore(flags);
+
+  return 0;
+}
+
+
+
+static struct miscdevice sccncm_dev = {
+	.minor = MISC_DYNAMIC_MINOR,
+	.name = "rckncm",
+	.fops = &sccncm_fops,
+};
+
+static struct miscdevice sccmpb_dev = {
+	.minor = MISC_DYNAMIC_MINOR,
+	.name = "rckmpb",
+	.fops = &sccmpb_fops,
+};
+
+static struct miscdevice sccdcm_dev = {
+	.minor = MISC_DYNAMIC_MINOR,
+	.name = "rckdcm",
+	.fops = &sccdcm_fops,
+};
+
+/* module initialization - called at module load time */
+static int __init sccmem_init(void) {
+  int ret = 0;
+
+  PRINTD(KERN_INFO "Starting up SCC memory driver...\n");
+  /* This driver does only work in a bare-metal environment */
+  if (!scc_bare_metal()) {
+    printk(KERN_INFO "sccmem: startup in non-SCC or paravirtualized environment.\n");
+    return -EINVAL;
+  }
+
+  ret = misc_register(&sccncm_dev);
+  if (ret != 0) {
+    printk(KERN_ERR "sccmem_init: could not register sccncm device. err = %d\n",
+	ret);
+    goto out;
+  }
+
+  ret = misc_register(&sccmpb_dev);
+  if (ret != 0) {
+    printk(KERN_ERR "sccmem_init: could not register sccmpb device. err = %d\n",
+	ret);
+    goto out_dereg_ncm;
+  }
+
+  ret = misc_register(&sccdcm_dev);
+  if (ret != 0) {
+    printk(KERN_ERR "sccmem_init: could not register sccdcm device. err = %d\n",
+	ret);
+    goto out_dereg_mpb;
+  }
+
+  /* Make a memory mapping for invalid MPB data to flush the L2 cache with,
+     this region starts at one L2 size beyond the MPB */
+  OWN_MPB_vaddr = (size_t) ioremap_prot(OWN_MPB + L2_CAPACITY, L2_CAPACITY * 2, 0);
+  OWN_MPB_offset = 0;
+
+  if(!OWN_MPB_vaddr)
+    goto out_dereg_dcm;
+
+  printk(KERN_INFO "sccmem_init(): Mapped MPB at %x for flushing to %x\n", (OWN_MPB + MPBSIZE), OWN_MPB_vaddr);
+
+  return 0;
+
+out_dereg_dcm:
+  misc_deregister(&sccdcm_dev);
+out_dereg_mpb:
+  misc_deregister(&sccmpb_dev);
+out_dereg_ncm:
+  misc_deregister(&sccncm_dev);
+out:
+  return ret;
+}
+
+/* module unload */
+static void __exit sccmem_exit(void)
+{
+  PRINTD(KERN_INFO "sccmem_init(): Shutting down SCC memory driver...\n");
+
+  /* remove the misc devices */
+  misc_deregister(&sccncm_dev);
+  misc_deregister(&sccmpb_dev);
+  misc_deregister(&sccdcm_dev);
+
+  /* Unmap invalid MPB memory used by L2 flush */
+  iounmap((void*)OWN_MPB_vaddr);
+}
+
+/* Perform write-back/invalidate for all cachelines holding the specified pfn */
+void scc_cop_wbinv_pfn(pfn_t pfn)
+{
+	unsigned long purgebase, physAddress, offset;
+	unsigned long flags;
+
+	/* First disable interrupts to be sure nobody else accesses the cache
+	 * in parallel and messes up the LRU state bits. */
+	local_irq_save(flags);
+
+	/* Flip-flop between two parts of the flush area */
+	if (OWN_MPB_offset == 0) {
+		OWN_MPB_offset = L2_CAPACITY;
+	} else {
+		OWN_MPB_offset = 0;
+	}
+
+	/* Set basepointer to flush area we will use in this flush */
+	purgebase = OWN_MPB_vaddr + OWN_MPB_offset;
+
+#ifdef FLUSH_USES_WBINVD
+	/* Begin by flushing the entire L1. We don't want any stale data in it,
+	 * as the reads issued by sccdcm_purgeAddress need to go to L2. */
+	__asm__ __volatile__ ("wbinvd;\n\t");
+#endif
+
+	/* Now flush the entire pfn out of the cache. It is no longer in L1 by
+	 * definition, so all we need to do now is kick it out of L2...
+	 * Beware of race conditions here, as no other thread may access the
+	 * page afterwards or cachelines seem to reappear from nowhere. The
+	 * safest way is to unmap all page table entries referring to the page.
+	 * Alternatively, just disable interrupts around the current method. */
+	physAddress = pfn << PAGE_SHIFT;
+
+	for (offset = 0; offset < (1 << PAGE_SHIFT); offset += L2_LINESIZE) {
+		sccdcm_purgeSet(purgebase + ((physAddress + offset) % L2_WBSTRIDE));
+	}
+
+	/* Re-enable interrupts */
+	local_irq_restore(flags);
+}
+EXPORT_SYMBOL(scc_cop_wbinv_pfn);
+
+module_init(sccmem_init);
+module_exit(sccmem_exit);
+MODULE_DESCRIPTION("SCC system memory driver...");
+MODULE_AUTHOR("Michael Riepen <michael.riepen@intel.com>");
+
diff -urN linux-3.1.4/drivers/char/sccperf.c linux-3.1.4-scc/drivers/char/sccperf.c
--- linux-3.1.4/drivers/char/sccperf.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-3.1.4-scc/drivers/char/sccperf.c	2011-12-20 15:27:07.578352779 +0100
@@ -0,0 +1,170 @@
+/*
+ *  linux/drivers/scc/sccperf.c
+ *
+ * SCC performance meter driver, based on fs/proc/stat.c and the
+ * modified CPUUTIL program distributed as part of SCC Linux.
+ */
+
+#include <linux/module.h>
+#include <linux/kernel_stat.h>
+#include <linux/time.h>
+#include <asm/cputime.h>
+#include <asm/io.h>
+#include <linux/sccsys.h>
+
+#ifndef arch_irq_stat_cpu
+#define arch_irq_stat_cpu(cpu) 0
+#endif
+#ifndef arch_irq_stat
+#define arch_irq_stat() 0
+#endif
+#ifndef arch_idle_time
+#define arch_idle_time(cpu) 0
+#endif
+
+static struct timer_list sccperf_timer;
+static volatile char* loadRegs = NULL;
+
+/* Timer callback. This routine calculates CPU utilization by reading kernel
+ * tick counters directly, then writes the result into shared memory where the
+ * MCPC can read it.
+ *
+ * Code to read the timer ticks has been copied from the proc filesystem driver
+ * (/fs/proc/stat.c!show_stat). The calculation of CPU utilization is based on
+ * the modified CPUUTIL program distributed as part of SCC Linux by Intel.
+ */
+static void sccperf_timer_function(unsigned long data)
+{
+	static cputime64_t last_user, last_nice, last_system, last_idle;
+	static cputime64_t last_iowait, last_irq, last_softirq, last_steal;
+	static cputime64_t last_guest, last_guest_nice;
+	static int first_callback = 1;
+
+	if (!loadRegs) {
+		printk(KERN_ERR "sccmem_cpuutil: unable to map SHM_X0_Y0\n");
+	} else {
+		int i;
+
+		/* Get ticks, just like /fs/proc/stat.c */
+		cputime64_t user, nice, system, idle, iowait, irq, softirq, steal;
+		cputime64_t guest, guest_nice;
+
+		user = nice = system = idle = iowait =
+			irq = softirq = steal = cputime64_zero;
+		guest = guest_nice = cputime64_zero;
+
+		for_each_possible_cpu(i) {
+			user = cputime64_add(user, kstat_cpu(i).cpustat.user);
+			nice = cputime64_add(nice, kstat_cpu(i).cpustat.nice);
+			system = cputime64_add(system, kstat_cpu(i).cpustat.system);
+			idle = cputime64_add(idle, kstat_cpu(i).cpustat.idle);
+			idle = cputime64_add(idle, arch_idle_time(i));
+			iowait = cputime64_add(iowait, kstat_cpu(i).cpustat.iowait);
+			irq = cputime64_add(irq, kstat_cpu(i).cpustat.irq);
+			softirq = cputime64_add(softirq, kstat_cpu(i).cpustat.softirq);
+			steal = cputime64_add(steal, kstat_cpu(i).cpustat.steal);
+			guest = cputime64_add(guest, kstat_cpu(i).cpustat.guest);
+			guest_nice = cputime64_add(guest_nice,
+				kstat_cpu(i).cpustat.guest_nice);
+		}
+
+		/* Calculate differences to last iteration */
+		if (!first_callback) {
+			char utilization = 0;
+
+			cputime64_t total;
+			cputime64_t diff_user, diff_nice, diff_system;
+			cputime64_t diff_idle, diff_iowait, diff_irq;
+			cputime64_t diff_softirq, diff_steal, diff_guest;
+			cputime64_t diff_guest_nice;
+
+			diff_user = cputime64_sub(user, last_user);
+			total = diff_user;
+			diff_nice = cputime64_sub(nice, last_nice);
+			total = cputime64_add(total, diff_nice);
+			diff_system = cputime64_sub(system, last_system);
+			total = cputime64_add(total, diff_system);
+			diff_idle = cputime64_sub(idle, last_idle);
+			total = cputime64_add(total, diff_idle);
+			diff_iowait = cputime64_sub(iowait, last_iowait);
+			total = cputime64_add(total, diff_iowait);
+			diff_irq = cputime64_sub(irq, last_irq);
+			total = cputime64_add(total, diff_irq);
+			diff_softirq = cputime64_sub(softirq, last_softirq);
+			total = cputime64_add(total, diff_softirq);
+			diff_steal = cputime64_sub(steal, last_steal);
+			total = cputime64_add(total, diff_steal);
+			diff_guest = cputime64_sub(guest, last_guest);
+			total = cputime64_add(total, diff_guest);
+			diff_guest_nice = cputime64_sub(guest_nice, last_guest_nice);
+			total = cputime64_add(total, diff_guest_nice);
+
+			/* Calculate local CPU utilization */
+			if ((unsigned long)total == 0) {
+				utilization = -1;
+			} else {
+				/* Division of unsigned long long seems not be
+				 * available, so unsigned long is the best we
+				 * can use for this. If the timer period is not
+				 * too long, this should work...
+				 */
+				utilization = (char)((unsigned long)(
+					diff_user + diff_nice + diff_system
+					) * 100 / (unsigned long)total);
+			}
+
+			*(loadRegs + sccsys_get_pid()) = utilization;
+		}
+
+		/* Save current values for next iteration */
+		last_user = user;
+		last_nice = nice;
+		last_system = system;
+		last_idle = idle;
+		last_iowait = iowait;
+		last_irq = irq;
+		last_softirq = softirq;
+		last_steal = steal;
+		last_guest = guest;
+		last_guest_nice = guest_nice;
+		first_callback = 0;
+	}
+
+	mod_timer(&sccperf_timer, jiffies + HZ);
+}
+
+/* module initialization - called at module load time */
+static int __init sccperf_init(void) {
+	/* This driver does only work in a bare-metal environment */
+	if (!scc_bare_metal()) {
+		printk(KERN_INFO "sccperf: startup in non-SCC or paravirtualized environment.\n");
+		return -EINVAL;
+	}
+
+	/* Map shared memory section belonging to the performance meter */
+	loadRegs = ioremap_nocache(/*SHM_X0_Y0*/ 0x80000000 + 0x900, 0x1000);
+	if (!loadRegs) {
+		printk(KERN_ERR "sccperf_init: Could not map shared memory section\n");
+		return -ENOMEM;
+	}
+
+	/* Start polling timer */
+	init_timer(&sccperf_timer);
+	sccperf_timer.function = sccperf_timer_function;
+	mod_timer(&sccperf_timer, jiffies + HZ);
+
+	return 0;
+}
+
+/* module unload */
+static void __exit sccperf_exit(void)
+{
+	del_timer_sync(&sccperf_timer);
+	iounmap(loadRegs);
+}
+
+module_init(sccperf_init);
+module_exit(sccperf_exit);
+MODULE_DESCRIPTION("SCC performance meter driver...");
+MODULE_AUTHOR("Jan-Arne Sobania <jan-arne.sobania@hpi.uni-potsdam.de");
+
diff -urN linux-3.1.4/drivers/char/sccproc/base.c linux-3.1.4-scc/drivers/char/sccproc/base.c
--- linux-3.1.4/drivers/char/sccproc/base.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-3.1.4-scc/drivers/char/sccproc/base.c	2011-12-20 15:27:07.578352779 +0100
@@ -0,0 +1,100 @@
+/*
+ *  linux/drivers/char/sccproc/base.c
+ *
+ *  proc filesystem extension for SCC hardware
+ */
+
+#include "sccproc.h"
+
+/* Root directory for SCC /proc entries. /proc/scc/ */
+struct proc_dir_entry* sccproc_root_dir;
+static struct proc_dir_entry* sccproc_pid_entry;
+static struct proc_dir_entry* sccproc_tileid_entry;
+
+/* Get root directory SCC /proc entries: /proc/scc/ */
+struct proc_dir_entry* sccproc_get_root_dir(void)
+{
+	return sccproc_root_dir;
+}
+EXPORT_SYMBOL(sccproc_get_root_dir);
+
+/* /proc/scc/pid */
+static int sccproc_pid_show(struct seq_file *m, void *v)
+{
+	int pid;
+
+	pid = sccsys_get_pid();
+
+	seq_printf(m, "%02d", pid);
+
+	return 0;
+}
+
+static int sccproc_pid_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, sccproc_pid_show, NULL);
+}
+
+static const struct file_operations sccproc_pid_fops = {
+	.owner		= THIS_MODULE,
+	.open		= sccproc_pid_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+/* /proc/scc/[core/<pid>/]tileid */
+static int sccproc_hex_show(struct seq_file *m, void *v)
+{
+	int value = (int)m->private;
+
+	seq_printf(m, "0x%08x", value);
+
+	return 0;
+}
+
+static int sccproc_hex_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, sccproc_hex_show, PDE(inode)->data);
+}
+
+const struct file_operations sccproc_hex_fops = {
+	.owner		= THIS_MODULE,
+	.open		= sccproc_hex_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+int sccproc_init_base(void)
+{
+	/* Create /proc/scc/ directory */
+	sccproc_root_dir = proc_mkdir("scc", NULL);
+	if (!sccproc_root_dir) {
+		printk(KERN_ERR "sccproc_init: unable to create /proc/scc/ directory.\n");
+		return -ENOMEM;
+	}
+
+	/* Create /proc/scc/pid file */
+	if (!(sccproc_pid_entry = proc_create_data("pid", S_IFREG | S_IRUGO,
+				sccproc_root_dir, &sccproc_pid_fops, NULL))) {
+		printk(KERN_ERR "sccproc_init: unable to create /proc/scc/pid file.\n");
+		return -ENOMEM;
+	}
+
+	/* Create /proc/scc/tileid file */
+	if (!(sccproc_tileid_entry = proc_create_data("tileid", S_IFREG | S_IRUGO,
+				sccproc_root_dir, &sccproc_hex_fops, (void*)sccsys_get_tileid()))) {
+		printk(KERN_ERR "sccproc_init: unable to create /proc/scc/tileid file.\n");
+		return -ENOMEM;
+	}
+
+	return 0;
+}
+
+void sccproc_cleanup_base(void)
+{
+	SCCPROC_DELETE(sccproc_pid_entry);
+	SCCPROC_DELETE(sccproc_tileid_entry);
+	SCCPROC_DELETE(sccproc_root_dir);
+}
diff -urN linux-3.1.4/drivers/char/sccproc/core.c linux-3.1.4-scc/drivers/char/sccproc/core.c
--- linux-3.1.4/drivers/char/sccproc/core.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-3.1.4-scc/drivers/char/sccproc/core.c	2011-12-20 15:27:07.578352779 +0100
@@ -0,0 +1,248 @@
+/*
+ *  linux/drivers/char/sccproc/core.c
+ *
+ *  proc filesystem extension for SCC hardware
+ */
+
+#include "sccproc.h"
+
+/* Root directory for SCC core entries. /proc/scc/core/ */
+static struct proc_dir_entry *sccproc_core_dir;
+
+struct sccproc_core_entry {
+	struct proc_dir_entry* core_dir;
+	struct proc_dir_entry* lutinfo;
+	struct proc_dir_entry* lut_dir;
+	struct proc_dir_entry* tileid;
+	struct proc_dir_entry* lut[256];
+};
+
+struct proc_dir_entry* sccproc_core_self_entry;
+struct sccproc_core_entry sccproc_core_entries[SCC_CORECOUNT];
+
+/* /proc/scc/core/<pid>/lutinfo */
+static void sccproc_print_lut(struct seq_file *m, scc_lut_t lut)
+{
+	int has_error = 0;
+
+#define SET_LUT_ERROR(d...)	\
+do { \
+	if (!has_error) { \
+		seq_printf(m, "    !" d); has_error = 1; \
+	} else { \
+		seq_printf(m, d); \
+	} \
+} while(0);
+
+	/* Decode LUT entry */
+	seq_printf(m, "%d_0x%x%x_%d(%s)_0x%03x",
+		lut.bypass, lut.y, lut.x, lut.subdest,
+		sccsys_subdest_names[lut.subdest], lut.address);
+
+	/* Mark common configuration errors */
+
+	/* Bypass does not work. MARC BUG #46 */
+	if (lut.bypass) SET_LUT_ERROR("B");
+
+	/* Tile coordinates: X = 0..5, Y = 0..3*/
+	if ((lut.y > 3) || (lut.x > 5))  SET_LUT_ERROR("T");
+
+	/*
+	 * CORE0+CORE1 cannot really be accessed from scc
+	 * hardware because they do not generate response packets, thus hanging
+	 * the requesting core; note that the MCPC *CAN* access these, as it can
+	 * just ignore that a response is missing.
+	 */
+
+	if ((lut.subdest == SCC_SUBDEST_CORE0) ||
+	    (lut.subdest == SCC_SUBDEST_CORE1)) SET_LUT_ERROR("C");
+
+	/* Nothing is connected to PERIN on any router */
+	if ((lut.subdest == SCC_SUBDEST_PERIN) ||
+
+	/* PERIS connects the VRC and SysIF (YX=(0,0) and (0,3), respectively) */
+	    ((lut.subdest == SCC_SUBDEST_PERIS) && !(
+		(lut.y == 0) && ((lut.x == 0) || (lut.x == 3))
+	    )) ||
+
+	/* PERIE is for memory controllers (YX=(0,5) and (2,5) only) */
+	    ((lut.subdest == SCC_SUBDEST_PERIE) && !(
+		((lut.y == 0) && (lut.x == 5)) || ((lut.y == 2) && (lut.x == 5))
+	    )) ||
+
+	/* PERIW is for memory controllers (YX=(0,0) and (2,0) only) */
+	    ((lut.subdest == SCC_SUBDEST_PERIW) && !(
+		((lut.y == 0) && (lut.x == 0)) || ((lut.y == 2) && (lut.x == 0))
+	    )) ||
+
+	/* That's it. MPB and CRB are available on all tiles. */
+	    0) SET_LUT_ERROR("S");
+
+	seq_printf(m, "\n");
+
+#undef SET_LUT_ERROR
+}
+
+static int sccproc_lutinfo_show(struct seq_file *m, void *v)
+{
+	int pid = (int)m->private;
+	unsigned int i;
+	scc_lut_t lut;
+	
+	for (i = 0; i < 256; i++) {
+		lut = sccsys_read_lut_entry(pid, i);
+
+		seq_printf(m, "0x%02x: ", i);
+		sccproc_print_lut(m, lut);
+	}
+
+	return 0;
+}
+
+static int sccproc_lutinfo_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, sccproc_lutinfo_show, PDE(inode)->data);
+}
+
+static const struct file_operations sccproc_lutinfo_fops = {
+	.owner		= THIS_MODULE,
+	.open		= sccproc_lutinfo_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+/* /proc/scc/core/<pid>/lut/<index> */
+static int sccproc_lutentry_show(struct seq_file *m, void *v)
+{
+	int pid = ((int)m->private >> 8);
+	int i = (int)m->private & 0xFF;
+	scc_lut_t lut;
+	
+	lut = sccsys_read_lut_entry(pid, i);
+
+	sccproc_print_lut(m, lut);
+
+	return 0;
+}
+
+static int sccproc_lutentry_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, sccproc_lutentry_show, PDE(inode)->data);
+}
+
+static const struct file_operations sccproc_lutentry_fops = {
+	.owner		= THIS_MODULE,
+	.open		= sccproc_lutentry_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+/* /proc/scc/core/ * */
+static int sccproc_create_core_entries(int core_index)
+{
+	struct sccproc_core_entry* core = &sccproc_core_entries[core_index];
+	char dname[16];
+	int i;
+
+	/* /proc/scc/core/<pid>/ */
+	snprintf(dname, sizeof(dname), "%02d", core_index);
+
+	core->core_dir = proc_mkdir(dname, sccproc_core_dir);
+	if (!core->core_dir) {
+		return -ENOMEM;
+	}
+
+	/* /proc/scc/core/<pid>/tileid */
+	core->tileid = proc_create_data("tileid", S_IFREG | S_IRUGO,
+		core->core_dir, &sccproc_hex_fops, (void*)
+		scc_pid_to_tileid(core_index));
+	if (!core->tileid) {
+		return -ENOMEM;
+	}
+
+	/* /proc/scc/core/<pid>/lutinfo */
+	core->lutinfo = proc_create_data("lutinfo", S_IFREG | S_IRUGO,
+		core->core_dir, &sccproc_lutinfo_fops, (void*)core_index);
+	if (!core->lutinfo) {
+		return -ENOMEM;
+	}
+
+	/* /proc/scc/core/<pid>/lut/ */
+	core->lut_dir = proc_mkdir("lut", core->core_dir);
+	if (!core->lut_dir) {
+		return -ENOMEM;
+	}
+
+	/* /proc/scc/core/<pid>/lut/<lut_index> */
+	for (i = 0; i < 256; i++) {
+		snprintf(dname, sizeof(dname), "%02x", i);
+		core->lut[i] = proc_create_data(dname, S_IFREG | S_IRUGO,
+			core->lut_dir, &sccproc_lutentry_fops, (void*)(
+			(core_index << 8) | i));
+		if (!core->lut[i]) {
+			return -ENOMEM;
+		}
+	}
+
+	return 0;
+}
+
+static void sccproc_delete_core_entries(int core_index)
+{
+	int i;
+	struct sccproc_core_entry* core = &sccproc_core_entries[core_index];
+
+	if (core->lut_dir != NULL) {
+		for (i = 0; i < 256; i++) {
+			SCCPROC_DELETE(core->lut[i]);
+		}
+		SCCPROC_DELETE(core->lut_dir);
+	}
+	SCCPROC_DELETE(core->lutinfo);
+	SCCPROC_DELETE(core->tileid);
+	SCCPROC_DELETE(core->core_dir);
+}
+
+int sccproc_init_core(void)
+{
+	int i;
+	char dname[16];
+
+	/* Create /proc/scc/core/ directory */
+	if (!(sccproc_core_dir = proc_mkdir("core", sccproc_root_dir))) {
+		printk(KERN_ERR "sccproc_init: unable to create /proc/scc/core directory.\n");
+		return -ENOMEM;
+	}
+
+	/* Create symlink /proc/scc/core/self  --->  [/proc/scc/core/]<pid>/ */
+	snprintf(dname, sizeof(dname), "%02d", sccsys_get_pid());
+	if (!(sccproc_core_self_entry = proc_symlink("self", sccproc_core_dir, dname))) {
+		printk(KERN_ERR "sccproc_init: unable to create /proc/scc/core/self symlink to %s.\n", dname);
+		return -ENOMEM;
+	}
+
+	/* Create per-core entries */
+	for (i = 0; i < SCC_CORECOUNT; i++) {
+		int err = sccproc_create_core_entries(i);
+
+		if (err < 0) {
+			return err;
+		}
+	}
+
+	return 0;
+}
+
+void sccproc_cleanup_core(void)
+{
+	int i;
+
+	for (i = 0; i < SCC_CORECOUNT; i++) {
+		sccproc_delete_core_entries(i);
+	}
+
+	SCCPROC_DELETE(sccproc_core_self_entry);
+	SCCPROC_DELETE(sccproc_core_dir);
+}
diff -urN linux-3.1.4/drivers/char/sccproc/Makefile linux-3.1.4-scc/drivers/char/sccproc/Makefile
--- linux-3.1.4/drivers/char/sccproc/Makefile	1970-01-01 01:00:00.000000000 +0100
+++ linux-3.1.4-scc/drivers/char/sccproc/Makefile	2011-12-20 15:27:07.578352779 +0100
@@ -0,0 +1,9 @@
+#
+# Makefile for SCC proc filesystem extension driver
+#
+
+obj-$(CONFIG_SCCPROC)		+= sccproc.o
+
+sccproc-objs = proc.o base.o core.o net.o
+
+$(sccproc-objs) : sccproc.h
diff -urN linux-3.1.4/drivers/char/sccproc/net.c linux-3.1.4-scc/drivers/char/sccproc/net.c
--- linux-3.1.4/drivers/char/sccproc/net.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-3.1.4-scc/drivers/char/sccproc/net.c	2011-12-20 15:27:07.578352779 +0100
@@ -0,0 +1,254 @@
+/*
+ *  linux/drivers/char/sccproc/net.c
+ *
+ *  proc filesystem extension for SCC hardware
+ */
+
+#include "sccproc.h"
+
+/* /proc/scc/net/emac<idx>/ * */
+static struct proc_dir_entry* sccproc_net_dir;
+static struct proc_dir_entry* sccproc_base_ip_entry;
+static struct proc_dir_entry* sccproc_host_entry;
+static struct proc_dir_entry* sccproc_gw_entry;
+static struct proc_dir_entry* sccproc_emac_ports_entry;
+
+struct sccproc_emac_entry {
+	struct proc_dir_entry* net_dir;
+	struct proc_dir_entry* stat_entry;
+};
+
+static struct sccproc_emac_entry sccproc_emac_entries[4];
+
+/* /proc/scc/net/[base_ip|host|gw] */
+static int sccproc_ip_show(struct seq_file *m, void *v)
+{
+	unsigned ip = (unsigned)m->private;
+
+	seq_printf(m, "%d.%d.%d.%d", (ip >> 24) & 0xFF, (ip >> 16) & 0xFF,
+		(ip >> 8) & 0xFF, ip & 0xFF);
+
+	return 0;
+}
+
+static int sccproc_ip_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, sccproc_ip_show, PDE(inode)->data);
+}
+
+static const struct file_operations sccproc_ip_fops = {
+	.owner		= THIS_MODULE,
+	.open		= sccproc_ip_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+/* EMAC statistic names, copied from emactool.c */
+static const char *emac_stat_name[46]= {
+	"TRBYTE",
+	"REBYTE",
+	"UFREC",
+	"FRFRREC",
+	"64BREC",
+	"127BREC",
+	"255BREC",
+	"511BREC",
+	"1023BREC",
+	"MAXBREC",
+	"OVFROK",
+	"64BTRA",
+	"127BTRA",
+	"255BTRA",
+	"511BTRA",
+	"1023BTRA",
+	"MAXBTRA",
+	"OVSZTX",
+	"FRRXOK",
+	"FRCHERR",
+	"BCFRRXOK",
+	"MCFRRXOK",
+	"CTFRRXOK",
+	"LGOUTRG",
+	"VLFRRXOK",
+	"PFFRRXOK",
+	"CTRRXBAD",
+	"LGOUTRG",
+	"VLFFRXOK",
+	"PFRRXOK",
+	"CTRRXBAD",
+	"FRTRANOK",
+	"BCFRTXOK",
+	"MCFRTXOK",
+	"UNDERR",
+	"CTFRTXOK",
+	"VLFRTXOK",
+	"PSFRTXOK",
+	"SGLCOLFR",
+	"MLTCOLFR",
+	"DEFTRANS",
+	"LATCOLL",
+	"EXCOLL",
+	"FRWEXCD",
+	"FRRXAERR",
+	"UNDCOUNT"
+};
+
+/* Base of EMAC statistics in GRB, copied from emactool.h */
+#define STAT0_TRBYTE							0x00003400
+#define STAT1_TRBYTE							0x00004400
+#define STAT2_TRBYTE							0x00005400
+#define STAT3_TRBYTE							0x00006400
+
+static int sccproc_emac_stat_show(struct seq_file *m, void *v)
+{
+	int emac_index = (int)m->private, i;
+	unsigned base;
+
+	switch (emac_index) {
+	case 0:
+		base = STAT0_TRBYTE;
+		break;
+	case 1:
+		base = STAT1_TRBYTE;
+		break;
+	case 2:
+		base = STAT2_TRBYTE;
+		break;
+	case 3:
+		base = STAT3_TRBYTE;
+		break;
+	default:
+		seq_printf(m, "Unexpected call for emac%d\n", emac_index);
+		return 0;
+	}
+
+	seq_printf(m, "Ethernet statistic for emac%d: \n", emac_index);
+	seq_printf(m, "----------------------------\n");
+
+	for (i = 0; i < 46; i++) {
+		unsigned value = sccsys_read_grb_entry(base + i * 8);
+		seq_printf(m, "%8.8s (0x%4x)  - %10d\n", emac_stat_name[i], base + i * 8, value);
+	}
+	seq_printf(m, "----------------------------\n");
+
+	return 0;
+}
+
+static int sccproc_emac_stat_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, sccproc_emac_stat_show, PDE(inode)->data);
+}
+
+static const struct file_operations sccproc_emac_stat_fops = {
+	.owner		= THIS_MODULE,
+	.open		= sccproc_emac_stat_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+static int sccproc_create_emac_entries(int emac_index)
+{
+	struct sccproc_emac_entry* net = &sccproc_emac_entries[emac_index];
+	char dname[16];
+
+	/* Make sure we only activate EMACs that are present in the bitstream. */
+	if (!((sccsys_read_grb_entry(SCCGRB_FPGA_CONFIG) >> 9) &
+		(1 << emac_index))) {
+		memset(net, 0, sizeof(struct sccproc_emac_entry));
+		return 0;
+	}
+
+	/* /proc/scc/net/emac<idx>/ */
+	snprintf(dname, sizeof(dname), "emac%d", emac_index);
+
+	net->net_dir = proc_mkdir(dname, sccproc_net_dir);
+	if (!net->net_dir) {
+		return -ENOMEM;
+	}
+
+	/* /proc/scc/net/emac<idx>/stat */
+	net->stat_entry = proc_create_data("stat", S_IFREG | S_IRUGO,
+		net->net_dir, &sccproc_emac_stat_fops, (void*)emac_index);
+	if (!net->stat_entry) {
+		return -ENOMEM;
+	}
+
+	return 0;
+}
+
+static void sccproc_delete_emac_entries(int emac_index)
+{
+	struct sccproc_emac_entry* net = &sccproc_emac_entries[emac_index];
+
+	SCCPROC_DELETE(net->stat_entry);
+	SCCPROC_DELETE(net->net_dir);
+}
+
+int sccproc_init_net(void)
+{
+	int i;
+
+	/* Create /proc/scc/net/ directory */
+	if (!(sccproc_net_dir = proc_mkdir("net", sccproc_root_dir))) {
+		printk(KERN_ERR "sccproc_init: unable to create /proc/scc/net directory.\n");
+		return -ENOMEM;
+	}
+
+	/* Create /proc/scc/net/base_ip file */
+	if (!(sccproc_base_ip_entry = proc_create_data("base_ip", S_IFREG | S_IRUGO,
+				sccproc_net_dir, &sccproc_ip_fops, (void*)sccsys_read_grb_entry(SCCGRB_EMAC_IP_START)))) {
+		printk(KERN_ERR "sccproc_init: unable to create /proc/scc/net/base_ip file.\n");
+		return -ENOMEM;
+	}
+
+	/* Create /proc/scc/net/gw file */
+	if (!(sccproc_gw_entry = proc_create_data("gw", S_IFREG | S_IRUGO,
+				sccproc_net_dir, &sccproc_ip_fops, (void*)sccsys_read_grb_entry(SCCGRB_EMAC_GW_IP)))) {
+		printk(KERN_ERR "sccproc_init: unable to create /proc/scc/net/gw file.\n");
+		return -ENOMEM;
+	}
+
+	/* Create /proc/scc/net/host file */
+	if (!(sccproc_host_entry = proc_create_data("host", S_IFREG | S_IRUGO,
+				sccproc_net_dir, &sccproc_ip_fops, (void*)sccsys_read_grb_entry(SCCGRB_EMAC_HOST_IP)))) {
+		printk(KERN_ERR "sccproc_init: unable to create /proc/scc/net/host file.\n");
+		return -ENOMEM;
+	}
+
+	/* Create /proc/scc/net/emac_ports file */
+	if (!(sccproc_emac_ports_entry = proc_create_data("emac_ports", S_IFREG | S_IRUGO,
+				sccproc_net_dir, &sccproc_hex_fops, (void*)((sccsys_read_grb_entry(SCCGRB_FPGA_CONFIG) >> 16) >> 9 & 0xF)))) {
+		printk(KERN_ERR "sccproc_init: unable to create /proc/scc/net/emac_ports file.\n");
+		return -ENOMEM;
+	}
+
+	/* Create individual emac entries */
+	for (i = 0; i < 4; i++) {
+		int err = sccproc_create_emac_entries(i);
+
+		if (err < 0) {
+			return err;
+		}
+	}
+
+	return 0;
+}
+
+void sccproc_cleanup_net(void)
+{
+	int i;
+
+	/* Delete entries from /proc/scc/net directory */
+	for (i = 0; i < 4; i++) {
+		sccproc_delete_emac_entries(i);
+	}
+
+	SCCPROC_DELETE(sccproc_base_ip_entry);
+	SCCPROC_DELETE(sccproc_host_entry);
+	SCCPROC_DELETE(sccproc_gw_entry);
+	SCCPROC_DELETE(sccproc_emac_ports_entry);
+
+	SCCPROC_DELETE(sccproc_net_dir);
+}
diff -urN linux-3.1.4/drivers/char/sccproc/proc.c linux-3.1.4-scc/drivers/char/sccproc/proc.c
--- linux-3.1.4/drivers/char/sccproc/proc.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-3.1.4-scc/drivers/char/sccproc/proc.c	2011-12-20 15:27:07.588922327 +0100
@@ -0,0 +1,62 @@
+/*
+ *  linux/drivers/char/sccproc/proc.c
+ *
+ *  proc filesystem extension for SCC hardware
+ */
+
+#include "sccproc.h"
+
+/* cleanup /proc/scc directory */
+static void sccproc_cleanup(void)
+{
+	sccproc_cleanup_core();
+	sccproc_cleanup_net();
+	sccproc_cleanup_base();
+}
+
+/* module initialization - called at module load time */
+static int __init sccproc_init(void)
+{
+	int err;
+
+	/* This driver does only work in a bare-metal environment */
+	if (!scc_bare_metal()) {
+		printk(KERN_INFO "sccproc: startup in non-SCC or paravirtualized environment.\n");
+		return -EINVAL;
+	}
+
+	/* Init /proc/scc/ */
+	err = sccproc_init_base();
+	if (err < 0) {
+		sccproc_cleanup();
+		return err;
+	}
+
+	/* Init /proc/scc/core/ */
+	err = sccproc_init_core();
+	if (err < 0) {
+		sccproc_cleanup();
+		return err;
+	}
+
+	/* Init /proc/scc/net/ */
+	err = sccproc_init_net();
+	if (err < 0) {
+		sccproc_cleanup();
+		return err;
+	}
+
+	return 0;
+}
+
+/* module unload */
+static void __exit sccproc_exit(void)
+{
+	sccproc_cleanup();
+}
+
+module_init(sccproc_init);
+module_exit(sccproc_exit);
+MODULE_DESCRIPTION("SCC procfs driver");
+MODULE_AUTHOR("Jan-Arne Sobania <jan-arne.sobania@hpi.uni-potsdam.de>");
+
diff -urN linux-3.1.4/drivers/char/sccproc/sccproc.h linux-3.1.4-scc/drivers/char/sccproc/sccproc.h
--- linux-3.1.4/drivers/char/sccproc/sccproc.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-3.1.4-scc/drivers/char/sccproc/sccproc.h	2011-12-20 15:27:07.588922327 +0100
@@ -0,0 +1,22 @@
+#include <linux/module.h>
+#include <linux/proc_fs.h>
+#include <linux/seq_file.h>
+#include <linux/sccsys.h>
+
+extern const struct file_operations sccproc_hex_fops;
+extern struct proc_dir_entry* sccproc_root_dir;
+
+#define SCCPROC_DELETE(entry) \
+	if (entry) { \
+		remove_proc_entry((entry)->name, (entry)->parent); \
+		entry = NULL; \
+	}
+
+extern int sccproc_init_base(void);
+extern void sccproc_cleanup_base(void);
+
+extern int sccproc_init_core(void);
+extern void sccproc_cleanup_core(void);
+
+extern int sccproc_init_net(void);
+extern void sccproc_cleanup_net(void);
diff -urN linux-3.1.4/drivers/char/sccproc.h linux-3.1.4-scc/drivers/char/sccproc.h
--- linux-3.1.4/drivers/char/sccproc.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-3.1.4-scc/drivers/char/sccproc.h	2011-12-20 15:27:07.588922327 +0100
@@ -0,0 +1,3 @@
+/* Get root directory of SCC /proc entries: /proc/scc/ */
+extern struct proc_dir_entry* sccproc_get_root_dir(void);
+
diff -urN linux-3.1.4/drivers/cpufreq/Kconfig.x86 linux-3.1.4-scc/drivers/cpufreq/Kconfig.x86
--- linux-3.1.4/drivers/cpufreq/Kconfig.x86	2011-11-28 23:48:14.000000000 +0100
+++ linux-3.1.4-scc/drivers/cpufreq/Kconfig.x86	2011-12-20 15:27:07.588922327 +0100
@@ -201,6 +201,12 @@
 
 	  If in doubt, say N.
 
+config X86_SCCFREQ
+	tristate "SCC Freq"
+	depends on X86_SCC
+	help
+	  This adds the CPUFreq driver for Single-chip cloud computer.
+
 config X86_LONGRUN
 	tristate "Transmeta LongRun"
 	depends on X86_32
diff -urN linux-3.1.4/drivers/cpufreq/Makefile linux-3.1.4-scc/drivers/cpufreq/Makefile
--- linux-3.1.4/drivers/cpufreq/Makefile	2011-11-28 23:48:14.000000000 +0100
+++ linux-3.1.4-scc/drivers/cpufreq/Makefile	2011-12-20 15:27:07.588922327 +0100
@@ -35,6 +35,7 @@
 obj-$(CONFIG_X86_SPEEDSTEP_SMI)		+= speedstep-smi.o
 obj-$(CONFIG_X86_SPEEDSTEP_CENTRINO)	+= speedstep-centrino.o
 obj-$(CONFIG_X86_P4_CLOCKMOD)		+= p4-clockmod.o
+obj-$(CONFIG_X86_SCCFREQ)		+= sccfreq.o
 obj-$(CONFIG_X86_CPUFREQ_NFORCE2)	+= cpufreq-nforce2.o
 
 ##################################################################################
diff -urN linux-3.1.4/drivers/cpufreq/sccfreq.c linux-3.1.4-scc/drivers/cpufreq/sccfreq.c
--- linux-3.1.4/drivers/cpufreq/sccfreq.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-3.1.4-scc/drivers/cpufreq/sccfreq.c	2011-12-20 15:27:07.588922327 +0100
@@ -0,0 +1,244 @@
+/*******************************************************************************
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms of the GNU General Public License as published by the Free
+  Software Foundation; either version 2 of the License, or (at your option)
+  any later version.
+
+  This program is distributed in the hope that it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+  more details.
+
+  You should have received a copy of the GNU General Public License along with
+  this program; if not, write to the Free Software Foundation, Inc., 59
+  Temple Place - Suite 330, Boston, MA  02111-1307, USA.
+
+  The full GNU General Public License is included in this distribution in the
+  file called LICENSE.
+
+  Contact Information:
+  Jan-Michael Brummer <jan-michael.brummer@intel.com>
+  Intel Braunschweig
+
+*******************************************************************************/
+
+#include <linux/version.h>
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/pci.h>
+#include <linux/init.h>
+
+#include <linux/cpufreq.h>
+#include <linux/sccsys.h>
+
+#include <asm/io.h>
+
+#define MODVERSION	"0.2b"
+
+/** Frequency mapping table */
+/* The index member encodes the tile clock divisor for GCBCFG */
+static struct cpufreq_frequency_table scc_freq_table[] = {
+	{0x01, 800000},
+	{0x02, 533000},
+	{0x03, 400000},
+	{0x04, 320000},
+	{0x05, 266000},
+	{0x06, 228000},
+	{0x07, 200000},
+	{0x08, 178000},
+	{0x09, 160000},
+	{0x0A, 145000},
+	{0x0B, 133000},
+	{0x0C, 123000},
+	{0x0D, 114000},
+	{0x0E, 106000},
+	{0x0F, 100000},
+	{0, CPUFREQ_TABLE_END},
+};
+
+/**
+ * \brief Get current cpu frequency
+ * \param cpu selected cpu (0)
+ * \return frequency in khz
+ */
+static unsigned int scc_freq_get_cpu_frequency(unsigned int cpu) {
+	unsigned int fastclock = 0;
+	unsigned int divider = 0;
+	unsigned int freq = 0;
+
+	/* Get fastclock */
+	fastclock = sccsys_read_grb_fastclock();
+
+	/* Get core divider */
+	divider = sccsys_read_gcbcfg(sccsys_get_pid()).divider;
+	divider++;
+
+	/* Fallback to 533MHz */
+	if (!divider || !fastclock) {
+		freq = 533000;
+	} else {
+		/* Calculate new frequency */
+		freq = fastclock / divider * 1000;
+	}
+
+	/* return frequency */
+	return freq;
+}
+
+/**
+ * \brief Set current cpu state
+ * \param state transition state
+ */
+static void scc_freq_set_cpu_state(unsigned int state) {
+	struct cpufreq_freqs freqs;
+	scc_gckcfg_t gcbcfg;
+	unsigned int divider;
+
+	/* Save old data and setup new values */
+	freqs.old = scc_freq_get_cpu_frequency(0);
+	freqs.new = scc_freq_table[state].frequency;
+	freqs.cpu = 0;
+
+	cpufreq_notify_transition(&freqs, CPUFREQ_PRECHANGE);
+
+	printk(KERN_INFO "attempting to set frequency to %i kHz\n",
+		scc_freq_table[state].frequency);
+
+	local_irq_disable();
+
+	/* Calculate new gcbcfg value and set it */
+	gcbcfg = sccsys_read_gcbcfg(sccsys_get_pid());
+
+	/* Get new divider value from frequency table. If it is out of range
+	 * OR indicates a tile clock of 1600MHz, we force it down to 800MHz,
+	 * although that should never happen unless we have serious memory
+	 * corruption. */
+	divider = (unsigned int)scc_freq_table[state].index;
+	if (!divider || divider > 15) {
+		divider = 2;
+	}
+
+	gcbcfg.divider = divider;
+	gcbcfg.router = 7 * (divider + 1);
+
+	sccsys_write_gcbcfg(sccsys_get_pid(), gcbcfg);
+
+	local_irq_enable();
+
+	cpufreq_notify_transition(&freqs, CPUFREQ_POSTCHANGE);
+}
+
+/**
+ * \brief Verify selected frequency
+ * \param policy pointer to cpufreq policy
+ * \return result
+ */
+static int scc_freq_verify(struct cpufreq_policy *policy) {
+	return cpufreq_frequency_table_verify(policy, &scc_freq_table[0]);
+}
+
+/**
+ * \brief Get frequency target
+ * \param policy pointer to cpufreq policy
+ * \param target_freq target frequency
+ * \param relation relation
+ * \return 0 on success, otherwise error
+ */
+static int scc_freq_target(struct cpufreq_policy *policy, unsigned int target_freq, unsigned int relation) {
+	unsigned int newstate = 0;
+
+	/* On fault return -EINVAL */
+	if (cpufreq_frequency_table_target(policy, scc_freq_table, target_freq, relation, &newstate)) {
+		return -EINVAL;
+	}
+
+	scc_freq_set_cpu_state(newstate);
+
+	return 0;
+}
+
+/**
+ * \brief Initialize cpu frequency policy
+ * \param policy pointer to cpufreq policy
+ * \return 0 on success, otherwise error
+ */
+static int scc_freq_cpu_init(struct cpufreq_policy *policy) {
+	struct cpuinfo_x86 *c = &cpu_data(0);
+	int result;
+
+	if (c->x86_vendor != 0 || c->x86 != 5 || c->x86_model != 2) {
+		return -EINVAL;
+	}
+
+	/* cpuinfo and default policy values */
+	policy->cpuinfo.transition_latency = 10000;
+	policy->cur = scc_freq_get_cpu_frequency(0);
+
+	result = cpufreq_frequency_table_cpuinfo(policy, scc_freq_table);
+	if (result) {
+		return result;
+	}
+
+	cpufreq_frequency_table_get_attr(scc_freq_table, policy->cpu);
+
+	return 0;
+}
+
+/**
+ * \brief Exit cpu frequency policy
+ * \param policy pointer to cpufreq policy
+ * \return 0
+ */
+static int scc_freq_cpu_exit(struct cpufreq_policy *policy) {
+	cpufreq_frequency_table_put_attr(policy->cpu);
+	return 0;
+}
+
+/** Frequency attribute table */
+static struct freq_attr *scc_freq_attr[] = {
+	&cpufreq_freq_attr_scaling_available_freqs,
+	NULL,
+};
+
+/** CPU Frequency driver structure */
+static struct cpufreq_driver scc_freq_driver = {
+	.get = scc_freq_get_cpu_frequency,
+	.verify = scc_freq_verify,
+	.target = scc_freq_target,
+	.init = scc_freq_cpu_init,
+	.exit = scc_freq_cpu_exit,
+	.name = "scc_freq",
+	.owner = THIS_MODULE,
+	.attr = scc_freq_attr,
+};
+
+/**
+ * \brief Initialze cpu frequency module
+ * \return error code
+ */
+static int __init scc_freq_init(void) {
+	struct cpuinfo_x86 *c = &cpu_data(0);
+
+	if (c->x86_vendor != 0 || c->x86 != 5 || c->x86_model != 2) {
+		return -EINVAL;
+	}
+
+	return cpufreq_register_driver(&scc_freq_driver);
+}
+
+/**
+ * \brief Exit cpu frequency module
+ */
+static void __exit scc_freq_exit(void) {
+	cpufreq_unregister_driver(&scc_freq_driver);
+}
+
+module_init(scc_freq_init);
+module_exit(scc_freq_exit);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Jan-Michael Brummer");
+MODULE_VERSION(MODVERSION);
+MODULE_DESCRIPTION("scc cpufreq driver");
diff -urN linux-3.1.4/drivers/net/Kconfig linux-3.1.4-scc/drivers/net/Kconfig
--- linux-3.1.4/drivers/net/Kconfig	2011-11-28 23:48:14.000000000 +0100
+++ linux-3.1.4-scc/drivers/net/Kconfig	2011-12-20 15:27:07.588922327 +0100
@@ -2044,6 +2044,51 @@
 
 if NETDEV_1000
 
+choice
+	prompt "SCC Message Buffer Driver"
+	default SCCMB
+	depends on SCCSYS && NET
+	---help---
+	  Select which implementation of the message buffer driver is used.
+
+config SCCMB
+	bool "Original"
+	---help---
+	  Default SCC driver for the on-die network via the message
+	  buffers.
+
+config SCCMBX
+	bool "Extended"
+	---help---
+	  Extended message buffer driver that provides a side-channel for
+	  kernel components.
+
+endchoice
+
+config SCCMBX_DUMP_NONNET_PACKETS
+	bool "Enable packet tracing for on-die non-NET packets"
+	default n
+	depends on SCCMBX
+
+config SCCPC
+	tristate "SCC-HostPC NIC"
+	default n
+	depends on SCCSYS && NET
+	---help---
+	  SCC driver for the MCPC network via the System Interface FPGA.
+
+config SCCPC_DUMP_PACKETS
+	bool "Enable packet tracing for MCPC network"
+	default n
+	depends on SCCPC
+
+config SCCEMAC
+	tristate "SCC EMAC NIC"
+	default n
+	depends on SCCSYS && NET
+	---help---
+	  SCC driver for the off-die network via Ethernet ports.
+
 config ACENIC
 	tristate "Alteon AceNIC/3Com 3C985/NetGear GA620 Gigabit support"
 	depends on PCI
diff -urN linux-3.1.4/drivers/net/Makefile linux-3.1.4-scc/drivers/net/Makefile
--- linux-3.1.4/drivers/net/Makefile	2011-11-28 23:48:14.000000000 +0100
+++ linux-3.1.4-scc/drivers/net/Makefile	2011-12-20 15:27:07.609021371 +0100
@@ -6,6 +6,11 @@
 obj-$(CONFIG_MDIO) += mdio.o
 obj-$(CONFIG_PHYLIB) += phy/
 
+obj-$(CONFIG_SCCMB)		+= sccmb.o
+obj-$(CONFIG_SCCMBX)		+= sccmbx.o
+obj-$(CONFIG_SCCPC) 		+= sccpc.o
+obj-$(CONFIG_SCCEMAC)		+= sccemac.o
+
 obj-$(CONFIG_TI_DAVINCI_EMAC) += davinci_emac.o
 obj-$(CONFIG_TI_DAVINCI_MDIO) += davinci_mdio.o
 obj-$(CONFIG_TI_DAVINCI_CPDMA) += davinci_cpdma.o
diff -urN linux-3.1.4/drivers/net/sccemac.c linux-3.1.4-scc/drivers/net/sccemac.c
--- linux-3.1.4/drivers/net/sccemac.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-3.1.4-scc/drivers/net/sccemac.c	2011-12-20 15:27:07.609021371 +0100
@@ -0,0 +1,1133 @@
+/*******************************************************************************
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms of the GNU General Public License as published by the Free
+  Software Foundation; either version 2 of the License, or (at your option)
+  any later version.
+
+  This program is distributed in the hope that it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+  more details.
+
+  You should have received a copy of the GNU General Public License along with
+  this program; if not, write to the Free Software Foundation, Inc., 59
+  Temple Place - Suite 330, Boston, MA  02111-1307, USA.
+
+  The full GNU General Public License is included in this distribution in the
+  file called LICENSE.
+
+  Contact Information:
+  Jan-Michael Brummer <jan-michael.brummer@intel.com>
+  Intel Braunschweig
+
+*******************************************************************************/
+
+#include "sccemac.h"
+
+/* Change Log
+ * 0.3.0	01/14/2011
+ *   o ported driver to NAPI structure
+ * 0.2.1	12/08/2010
+ *   o fix possible rx packet lost
+ * 0.2.0	11/19/2010
+ *   o added irq handling
+ * 0.1.4	09/06/2010
+ *   o added fpga emac port detection
+ *   o fix rx illegal packet length bug
+ * 0.1.3	08/27/2010
+ *   o create general configuration functions
+ *   o added support for emac2 and emac3
+ * 0.1.2	08/26/2010
+ *   o fix emac1 transfer setup
+ * 0.1.1	08/19/2010
+ *   o handle more than one packet in receive
+ *   o overflow check
+ * 0.1.0	08/18/2010
+ *   o first release
+ */
+
+#define MODVERSTRING	"0.3.0"
+#define OVERFLOW_CHECK	1
+
+#define IRQ_STATUS	0xD000
+#define IRQ_MASK	0xD200
+#define IRQ_RESET	0xD400
+#define IRQ_CONFIG	0xD800
+
+/** emac0 network driver structure */
+static struct net_device *emac0_dev = NULL;
+/** emac1 network driver structure */
+static struct net_device *emac1_dev = NULL;
+/** emac2 network driver structure */
+static struct net_device *emac2_dev = NULL;
+/** emac3 network driver structure */
+static struct net_device *emac3_dev = NULL;
+
+/** optional select ethernet port */
+static int ethernet_port = EMAC0 | EMAC1 | EMAC2 | EMAC3;
+module_param(ethernet_port, int, 0644);
+MODULE_PARM_DESC(ethernet_port,
+	"Ethernet ports the driver should use (0x01=emac0, 0x02=emac1,"
+	" 0x04=emac2, 0x08=emac3)");
+
+/** debug level */
+static int debug_level = 0;
+module_param(debug_level, int, 0644);
+MODULE_PARM_DESC(debug_level, "Debug level");
+
+/** override sccKit configuration */
+static int override = 0;
+module_param(override, int, 0644);
+MODULE_PARM_DESC(override, "Override sccKit configuration");
+
+static int core0 = 0;
+
+/* grb address */
+static void* grb;
+
+/**
+ * \brief Read long from emac
+ * \param pAddr address we want to read
+ * \return value stored in address
+ */
+static int emac_readl(void *addr) {
+	int ret;
+
+	ret = readl(addr);
+	/* no error: read twice, as xilinx ip need some time... */
+	ret = readl(addr);
+
+	return ret;
+}
+
+/**
+ * \brief Write long to emac
+ * \param value entry for address
+ * \param pAddr address we want to write
+ */
+static void emac_writel(int value, void *addr) {
+	writel(value, addr);
+}
+
+static void init_xilinx_port(int emac, int base) {
+	int flow_control = 0;
+	int transmitter_addr = 0;
+	int receiver1_addr = 0;
+	int config_add = 0;
+	int add_filter_mod = 0;
+
+	transmitter_addr = emac_readl(RA(base + TRANSMITTER_ADDRESS, 0));
+	receiver1_addr = emac_readl(RA(base + RECEIVER1_ADDRESS, 0));
+
+	/* Check if IP is already configured */
+	//if (!(transmitter_addr & 0x10000000) ||
+       //    !(receiver1_addr & 0x10000000)) {
+    if (core0 == 0)
+       {
+		EPRINTK(DEBUG_INFO, "Config eMAC RX %d\n", emac);
+
+		/* Disable tx and rx flow control of eMAC */
+		EPRINTK(DEBUG_INFO, "Disabling tx/rx flow control of eMAC%d\n", emac);
+		flow_control = emac_readl(RA(base + CONFIG_FLOW_CONTROL_ADD, 0));
+
+		/* Set top 3 bits of the flow control configuration to zero,
+		 * therefore disabling tx and rx flow control
+		 */
+		flow_control &= 0x7FFFFFF;
+		emac_writel(flow_control, RA(base + CONFIG_FLOW_CONTROL_ADD, 0));
+
+		/* Sanity check */
+		flow_control = emac_readl(RA(base + CONFIG_FLOW_CONTROL_ADD, 0));
+		EPRINTK(DEBUG_INFO, "  CONFIG_FLOW_CONTROL_ADD set: 0x%x\n",
+                flow_control);
+
+		/* Setting the tx configuration bit to enable the transmitter and
+		 * set to full duplex mode.
+		 */
+		EPRINTK(DEBUG_INFO, "Setting rx configuration of eMAC%d\n", emac);
+		transmitter_addr = emac_readl(RA(base + TRANSMITTER_ADDRESS, 0));
+
+		/* Now set the relevant bits and write back into the register:
+		 * 26 (half duplex) = 0, 28 (transmit enable) = 1, 31 (reset) = 0
+		 */
+		transmitter_addr &= ~(1 << 31);
+		transmitter_addr &= ~(1 << 26);
+		transmitter_addr |= (1 << 28);
+		emac_writel(transmitter_addr, RA(base + TRANSMITTER_ADDRESS, 0));
+
+		transmitter_addr = emac_readl(RA(base + TRANSMITTER_ADDRESS, 0));
+		EPRINTK(DEBUG_INFO, "  TRANSMITTER_ADDRESS set: %x\n",
+                transmitter_addr);
+
+		/* Setting the rx configuration bit to enable the transmitter and
+		 * set to full duplex mode.
+		 */
+		EPRINTK(DEBUG_INFO, "Setting IP configuration of EMAC%d\n", emac);
+
+		/* Read the current config value from the register */
+		receiver1_addr = emac_readl(RA(base + RECEIVER1_ADDRESS, 0));
+
+		/* Now set the relevant bits and write back into the register:
+		 *  25 = 1, 26 = 0, 28 = 1, 31 = 0
+		 */
+		/* Length/Type Error Check Disable */
+		receiver1_addr |= (1 << 25);
+		/* Disable Half Duplex => Full Duplex */
+		receiver1_addr &= ~(1 << 26);
+		/* Receiver enable */
+		receiver1_addr |= (1 << 28);
+		/* Reset */
+		receiver1_addr &= ~(1 << 31);
+		emac_writel(receiver1_addr, RA(base + RECEIVER1_ADDRESS, 0));
+
+		receiver1_addr = emac_readl(RA(base + RECEIVER1_ADDRESS, 0));
+		EPRINTK(DEBUG_INFO, "  RECEIVER1_ADDRESS set: %x\n",
+                receiver1_addr);
+
+		/* Setting the speed to eMAC to 1Gb/s */
+		EPRINTK(DEBUG_INFO, "Setting speed of EMAC%d to 1Gb/s\n", emac);
+
+		/* Read the current config value from register */
+		config_add = emac_readl(RA(base + CONFIG_ADD, 0));
+
+		/* Now set the relevant bits and write back into the register:
+		 * 31 = 1, 30 = 0
+		 */
+		/* MAC Speed Configuration: 00 - 10Mbps, 01 - 100Mbps, 10 - 1Gbps */
+		config_add |= (1 << 31);
+		config_add &= ~(1 << 30);
+		emac_writel(config_add, RA(base + CONFIG_ADD, 0));
+
+		config_add = emac_readl(RA(base + CONFIG_ADD, 0));
+		EPRINTK(DEBUG_INFO, "  CONFIG_ADD set: %x\n", config_add);
+
+		/* Read the current config addr filter mode */
+		add_filter_mod = emac_readl(RA(base + ADD_FILTER_MOD, 0));
+
+		/* Not set the relevant bits and write back into the register:
+		 * 31 (promiscuous mode) = 1 not working, but thats ok!
+		 */
+		add_filter_mod |= (1 << 31);
+		emac_writel(add_filter_mod, RA(base + ADD_FILTER_MOD, 0));
+
+		add_filter_mod = emac_readl(RA(base + ADD_FILTER_MOD, 0));
+		EPRINTK(DEBUG_INFO, "  ADD_FILTER_MOD set: %x\n", add_filter_mod);
+	//} else {
+	//	EPRINTK(DEBUG_INFO, "EMAC%d already configured!\n", emac);
+	}
+}
+
+/**
+ * \brief Allocate and return clean mpb
+ * \param order order of memory pages
+ * \return new buffer
+ */
+static unsigned char *alloc_buffer(int order) {
+	struct page *page = NULL;
+	unsigned char *buffer = NULL;
+	int num = 1 << order;
+
+	/* We are using alloc_pages and change_page_attr here, as the other
+	 * functions do not allow setting our attributes (NC/PSE)
+	 */
+	page = alloc_pages(GFP_KERNEL, order);
+	buffer = page_address(page);
+
+	if (set_memory_wt_mpbt((unsigned long)buffer, 1 << order) < 0) {
+		EPRINTK(DEBUG_INFO, "Failed");
+		return NULL;
+	}
+
+	memset(buffer, 0x00, 0x20);
+	memset(buffer + 0x20, 0xDA, num * PAGE_SIZE - 0x20);
+
+	return buffer;
+}
+
+/**
+ * \brief Get mac address
+ * \param dev network device pointer
+ */
+static inline unsigned long long get_mac_address(struct net_device *dev) {
+	struct emac_priv *priv = netdev_priv(dev);
+	unsigned long long mac;
+	int mac1 = readl(RA(0x7E00, 0));
+	int mac2 = readl(RA(0x7E04, 0));
+
+	mac = (((unsigned long long)mac1) << 32) + ( unsigned long long ) mac2;
+
+	if (mac == 0x00) {
+		mac = MAC_ADDRESS;
+	}
+
+	/* Calculate mac address of core depending on selected emac device */
+	return mac + priv->device * 0x100 + priv->pid;
+}
+
+/**
+ * \brief Setup application register of selected emac port
+ * \param dev network device pointer
+ * \param pid core pid
+ * \param tile_offset tile memory offset
+ * \param pos core route
+ * \param mode contains route and destination to memory controller
+ */
+static void setup_emac(struct net_device *dev, int pid,
+                 unsigned long long tile_offset, int pos, int mode) {
+	struct emac_priv *priv = netdev_priv(dev);
+	unsigned long long addr_offset = tile_offset;
+	unsigned long long mac = 0;
+	u32 tmp = 0;
+	u16 write_offset = 0;
+	u16 read_offset = 0;
+	unsigned char core = 0;
+
+	/* store own id */
+	priv->pid = pid;
+
+	EPRINTK(DEBUG_INFO, "Initialize eMAC 0x%x (pid %d)\n", priv->device, priv->pid);
+
+	/**** Receive configuration ****/
+
+	/* Set up ring buffer space */
+	priv->rx_buffer_max = CLINE_PACKETS(BUFFER_SIZE) - 1;
+	priv->rx_buffer = alloc_buffer(BUFFER_ORDER);
+
+	/* Start address */
+	EPRINTK(DEBUG_INFO, "  RX Buffer %p (%p phys)\n", priv->rx_buffer,
+            (void*)virt_to_phys(priv->rx_buffer));
+
+	tmp = virt_to_phys(priv->rx_buffer);
+	addr_offset = tile_offset + tmp;
+	addr_offset >>= 5;
+	emac_writel(addr_offset, RA(priv->base + EMAC_RX_BUFFER_START_ADDRESS,
+                priv->pid));
+	tmp = emac_readl(RA(priv->base + EMAC_RX_BUFFER_START_ADDRESS, priv->pid));
+	EPRINTK(DEBUG_INFO, "  RX Buffer set to @%x\n", tmp);
+
+	/* Set buffer write offset */
+	write_offset = emac_readl(RA(priv->base + EMAC_RX_BUFFER_WRITE_OFFSET,
+                              priv->pid));
+	EPRINTK(DEBUG_INFO, "  RX Buffer write offset at: %d\n", write_offset);
+
+	/* Set buffer read offset to write offset */
+	emac_writel(write_offset, RA(priv->base + EMAC_RX_BUFFER_READ_OFFSET,
+                priv->pid));
+	EPRINTK(DEBUG_INFO, "  RX Buffer read offset set to: %d\n",
+            emac_readl(RA(priv->base + EMAC_RX_BUFFER_READ_OFFSET, priv->pid)));
+	priv->rx_read_offset = write_offset;
+
+	/* Size */
+	emac_writel(priv->rx_buffer_max, RA(priv->base + EMAC_RX_BUFFER_SIZE,
+                priv->pid));
+	EPRINTK(DEBUG_INFO, "  RX Size set to %d\n",
+            emac_readl(RA(priv->base + EMAC_RX_BUFFER_SIZE, priv->pid)));
+
+	/* Threshold */
+	emac_writel(0x01, RA(priv->base + EMAC_RX_BUFFER_THRESHOLD, priv->pid));
+	EPRINTK(DEBUG_INFO, "  RX Threshold set to %x\n",
+            emac_readl(RA(priv->base + EMAC_RX_BUFFER_THRESHOLD, priv->pid)));
+
+	/* Route */
+	core = pid & 1;
+	emac_writel((core << 24) | (pos << 16) | mode, RA(priv->base + EMAC_RX_MODE, priv->pid));
+	EPRINTK(DEBUG_INFO, "  RX Mode set to %x\n",
+            emac_readl(RA(priv->base + EMAC_RX_MODE, priv->pid)));
+
+	/* MAC */
+	mac = get_mac_address(dev); 
+	emac_writel(MAC_HI(mac), RA(priv->base +
+                EMAC_RX_NETWORK_PORT_MAC_ADDRESS_HI, priv->pid));
+	EPRINTK(DEBUG_INFO, "  MAC1 set to %x\n",
+            emac_readl(RA(priv->base + EMAC_RX_NETWORK_PORT_MAC_ADDRESS_HI,
+                       priv->pid)));
+	emac_writel(MAC_LO(mac), RA(priv->base +
+                EMAC_RX_NETWORK_PORT_MAC_ADDRESS_LO, priv->pid));
+	EPRINTK(DEBUG_INFO, "  MAC2 set to %x\n",
+            emac_readl(RA(priv->base + EMAC_RX_NETWORK_PORT_MAC_ADDRESS_LO,
+                       priv->pid)));
+
+	/* Activate network port by setting enable bit */
+	emac_writel(0x01, RA(priv->base + EMAC_RX_NETWORK_PORT_ENABLE, priv->pid));
+	EPRINTK(DEBUG_INFO, "  RX Port enable set to %x\n",
+            emac_readl(RA(priv->base + EMAC_RX_NETWORK_PORT_ENABLE,
+                       priv->pid)));
+
+	/**** Transfer configuration ****/
+
+	/* Set up ring buffer space */
+	priv->tx_buffer_max = CLINE_PACKETS(BUFFER_SIZE) - 1;
+	priv->tx_buffer = alloc_buffer(BUFFER_ORDER);
+
+	/* Start address */
+	EPRINTK(DEBUG_INFO, "  TX Buffer %p (%p phys)\n", priv->tx_buffer,
+            (void*)virt_to_phys(priv->tx_buffer));
+	tmp = virt_to_phys(priv->tx_buffer);
+	emac_writel((tmp + tile_offset) >> 5, RA(priv->base +
+                EMAC_TX_BUFFER_START_ADDRESS, priv->pid));
+	tmp = emac_readl(RA(priv->base + EMAC_TX_BUFFER_START_ADDRESS, priv->pid));
+	EPRINTK(DEBUG_INFO, "  TX Buffer set to @%x\n", tmp);
+
+	/* Get buffer read offset */
+	read_offset = emac_readl(RA(priv->base + EMAC_TX_BUFFER_READ_OFFSET,
+                             priv->pid));
+	EPRINTK(DEBUG_INFO, "  TX Buffer read offset at: %d\n", read_offset);
+
+	/* Set buffer write offset to read offset */
+	emac_writel(read_offset, RA(priv->base + EMAC_TX_BUFFER_WRITE_OFFSET,
+                priv->pid));
+	EPRINTK(DEBUG_INFO, "  TX Buffer write offset set to: %d\n",
+            emac_readl(RA(priv->base + EMAC_TX_BUFFER_WRITE_OFFSET,
+                       priv->pid)));
+	priv->tx_write_offset = read_offset;
+
+	/* Size */
+	emac_writel(priv->tx_buffer_max, RA(priv->base + EMAC_TX_BUFFER_SIZE,
+                priv->pid));
+	EPRINTK(DEBUG_INFO, "  TX Size set to %d\n",
+            emac_readl(RA(priv->base + EMAC_TX_BUFFER_SIZE, priv->pid)));
+
+	/* Route */
+	emac_writel(mode, RA(priv->base + EMAC_TX_MODE, priv->pid));
+	EPRINTK(DEBUG_INFO, "  TX Mode set to %x\n",
+            emac_readl(RA(priv->base + EMAC_TX_MODE, priv->pid)));
+
+	/* Activate network port by setting enable bit */
+	emac_writel(0x01, RA(priv->base + EMAC_TX_NETWORK_PORT_ENABLE, priv->pid));
+	EPRINTK(DEBUG_INFO, "  TX Port enable set to %x\n",
+            emac_readl(RA(priv->base + EMAC_TX_NETWORK_PORT_ENABLE,
+                       priv->pid)));
+}
+
+/**
+ * \brief Resets local interrupt and then global
+ * \prev priv emac private data
+ */
+static void emac_clear_interrupt(struct emac_priv *priv) {
+	unsigned int tmp;
+
+	unset_lapic_mask(EMAC_LVT, priv->net_device->irq);
+
+	/* Clear interrupt bit */
+	sccsys_clear_irq_direct(sccsys_get_pid(), EMAC_IRQ_MASK);
+
+	/* Reset */
+	tmp = priv->device;
+	writel(tmp, RA(IRQ_RESET, priv->pid * 2));
+}
+
+/**
+ * \brief Interrupt handler
+ * \param irq current irq number
+ * \param dev_id private net_device pointer
+ * \return IRQ_HANDLED if we handled this interrupt, otherwise IRQ_NONE
+ */
+static irqreturn_t emac_interrupt(int irq, void *dev_id) {
+	struct net_device *dev = (struct net_device *)dev_id;
+	struct emac_priv *priv = netdev_priv(dev);
+	unsigned int status = 0;
+
+	if (!dev) {
+		printk(KERN_DEBUG "emac interrupt %d for unknown device\n", irq);
+		return IRQ_NONE;
+	}
+
+	status = readl(RA(IRQ_STATUS, priv->pid * 2));
+	//if (printk_ratelimit()) {
+	//	EPRINTK(DEBUG_READ, "Interrupt status Core: %x\n", status);
+	//}
+
+	if (!(status & priv->device)) {
+		//printk(KERN_WARNING "Not an emac interrupt (0x%x)\n", status);
+		return IRQ_NONE;
+	}
+
+	if (likely(napi_schedule_prep(&priv->napi))) {
+		/* disable interrupt */
+		set_lapic_mask(EMAC_LVT, priv->net_device->irq);
+		__napi_schedule(&priv->napi);
+	}
+
+	//if (status & ~priv->device) {
+		/* In case status indicates other int. devices... */
+	//	printk(KERN_INFO "Status was 0x%x, now is 0x%x\n", status, readl(RA(IRQ_STATUS, priv->pid * 2)));
+	//	return IRQ_NONE;
+	//}
+
+	return IRQ_HANDLED;
+}
+
+/**
+ * \brief Open network device
+ * \param dev network device
+ * \return error code
+ */
+int emac_open(struct net_device *dev) {
+	struct emac_priv *priv = netdev_priv(dev);
+	unsigned long long offset = 0;
+	unsigned long long mac = 0;
+	int tmp = 0;
+	scc_coord_t coord;
+	scc_lut_t phys_base;
+	int position = 0;
+	int mode = 0;
+	int subdest = 0;
+	int route = 0;
+	int i = 0;
+	int status;
+
+	netif_carrier_off(dev);
+
+	/* Get SCC processor coordinates and number */
+	coord = sccsys_get_coord();
+	position = sccsys_get_pid();
+
+	EPRINTK(DEBUG_INFO, "Location:\n");
+	EPRINTK(DEBUG_INFO, "  X: %d Y: %d, Z: %d => Position: %d\n",
+	    coord.x, coord.y, coord.z, position);
+
+	/* Depending on core location read own private data
+	 * (offset, subdest, route)
+	 */
+	phys_base = sccsys_read_lut_entry(sccsys_get_pid(), 0);
+
+	offset = (unsigned long long)((unsigned long long)phys_base.address) << 24;
+	subdest = phys_base.subdest;
+	route = sccsys_get_route(phys_base);
+	mode = (subdest << 8) + route;
+
+	EPRINTK(DEBUG_INFO, "Using offset: %llx\n", offset);
+
+	/* setup ethernet port */
+	setup_emac(dev, position, offset, sccsys_get_route(coord), mode);
+
+	/* set network addr */
+	mac = get_mac_address(dev);
+	for (i = 5; i != 0; i--) {
+		dev->dev_addr[i] = mac & 0xFF;
+		mac >>= 8;
+	}
+
+	status = request_irq(dev->irq, &emac_interrupt, IRQF_SHARED, "emac", dev);
+	if (status) {
+		printk(KERN_WARNING "Can't get interrupt #%d\n", dev->irq);
+		return status;
+	}
+
+	napi_enable(&priv->napi);
+
+	emac_clear_interrupt(priv);
+	/* Enable interrupt */
+	tmp = readl(RA(IRQ_MASK, priv->pid * 2));
+	writel(tmp & ~(priv->device), RA(IRQ_MASK, priv->pid * 2));
+	writel(EMAC_IRQ_CONFIG, RA(IRQ_CONFIG, priv->pid));
+	netif_carrier_on(dev);
+
+	netif_start_queue(priv->net_device);
+
+	return 0;
+}
+
+/**
+ * \brief Stop network device: stop queue
+ * \param dev network device
+ * \return error code (0)
+ */
+int emac_stop(struct net_device *dev) {
+	struct emac_priv *priv = netdev_priv(dev);
+
+	/* Shutdown poll */
+	EPRINTK(DEBUG_INFO, "shutdown\n");
+	napi_disable(&priv->napi);
+
+	/* stop queue */
+	EPRINTK(DEBUG_INFO, "stop queue\n");
+	netif_stop_queue(dev);
+
+	/* free irq */
+	free_irq(dev->irq, dev);
+
+	EPRINTK(DEBUG_INFO, "disable rx/tx ports\n");
+
+	/* Disable network tx/rx port */
+	emac_writel(0x00, RA(priv->base + EMAC_TX_NETWORK_PORT_ENABLE, priv->pid));
+	emac_writel(0x00, RA(priv->base + EMAC_RX_NETWORK_PORT_ENABLE, priv->pid));
+
+	netif_carrier_off(dev);
+
+	return 0;
+}
+
+/**
+ * \brief Get first bytes of addr0 (length)
+ * \param dev net device
+ * \return length of data or 0
+ */
+int get_addr0(struct net_device *dev) {
+	struct emac_priv *priv = NULL;
+
+	if (!dev) {
+		return 0;
+	}
+	priv = netdev_priv(dev);
+
+	if (priv && priv -> rx_buffer) {
+		return readl(priv->rx_buffer);
+	}
+
+	return 0;
+}
+
+/**
+ * \brief Receive and process packets from network device
+ * \param priv emac private data
+ * \param write_offset write offset
+ * \param max_num maximum number of packets we can read at once
+ * \return new read offset
+ */
+int emac_rx(struct emac_priv *priv, unsigned short write_offset, int max_num) {
+	struct sk_buff *skb = NULL;
+	unsigned short read_offset = priv->rx_read_offset;
+	void *addr = NULL;
+	unsigned short len = 0;
+	int packets = 0;
+
+again:
+	read_offset++;
+	if (read_offset < 1 || read_offset > priv->rx_buffer_max) {
+		EPRINTK(DEBUG_READ, "read_offset is %d\n", read_offset);
+		read_offset = 1;
+	}
+	addr = priv->rx_buffer + read_offset * 32;
+
+	len = U16(addr);
+
+	EPRINTK(DEBUG_READ, "device: %x current: start read at %d; write_offset "
+            "at %d; addr: %p, packet len: %d (num packets:%d)\n", priv->device,
+             read_offset, write_offset, addr, len, packets);
+
+	/* check for over/underflow */
+	if (len < sizeof(struct iphdr) || len > 1536) {
+		int i = 0;
+
+		printk(KERN_NOTICE "emac_rx(): illegal packet length %d => drop "
+               "(num packets: %d)\n", len, packets);
+		priv->stats.rx_errors++;
+		priv->stats.rx_dropped++;
+
+		read_offset = write_offset;
+		printk("Buffer:\n");
+		for (i = 0; i < 32; i++) {
+			printk("%2.2x ", ((char*)addr)[i] & 0xFF);
+		}
+		printk("\n");
+
+		printk("Buffer0:\n");
+		for (i = 0; i < 32; i++) {
+			printk("%2.2x ", ((char*)priv->rx_buffer)[i] & 0xFF);
+		}
+		printk("\n");
+
+		//napi_disable(&priv->napi);
+		//netif_stop_queue(priv->net_device);
+		//return 0;
+
+		goto rxDone;
+	}
+
+	/* allocate buffer */
+	skb = dev_alloc_skb(len);
+	if (!skb) {
+		if (printk_ratelimit()) {
+			printk(KERN_NOTICE "emac_rx(): low on mem - packet dropped\n");
+		}
+
+		priv->stats.rx_dropped++;
+		return 0;
+	}
+
+	skb_put(skb, len);
+
+	if (read_offset < write_offset) {
+		memcpy(skb->data, addr + 2, len);
+		read_offset += CLINE_PACKETS(skb->len + 2) - 1;
+	} else {
+		int rest;
+		int bytesLeft = len;
+		int bytesToCopy = len;
+
+		EPRINTK(DEBUG_READ, "case: read_offset > write_offset (%d > %d)\n",
+                read_offset, write_offset);
+		/* rest to the end of buffer - 2 bytes length information */
+		rest = (priv->rx_buffer_max - read_offset + 1) * 32 - 2;
+		if (len > rest) {
+			bytesToCopy = rest;
+		}
+		EPRINTK(DEBUG_READ, "bytes to copy: %d, bytesLeft: %d\n", bytesToCopy,
+                bytesLeft);
+		memcpy(skb->data, addr + 2, bytesToCopy);
+		bytesLeft -= bytesToCopy;
+
+		if (bytesLeft != 0) {
+			addr = priv->rx_buffer + 0x20;
+			EPRINTK(DEBUG_READ, "copying from %p, left: %d (%x)\n", addr,
+                    bytesLeft, ((u8*)addr)[0]);
+			memcpy(skb->data+bytesToCopy, addr, bytesLeft);
+			read_offset = CLINE_PACKETS(bytesLeft);
+		} else {
+			read_offset += CLINE_PACKETS(skb->len+2) - 1;
+		}
+	}
+
+	priv->stats.rx_packets++;
+	priv->stats.rx_bytes += len;
+	priv->net_device->last_rx = jiffies;
+
+	skb->dev = priv->net_device;
+	skb->protocol = eth_type_trans(skb, priv->net_device);
+	skb->ip_summed = CHECKSUM_UNNECESSARY;
+
+	/* now process the buffer */
+	netif_receive_skb(skb);
+
+	packets++;
+
+rxDone:
+	/* set new read pointer */
+	EPRINTK(DEBUG_READ, "Update rx read offset: %d\n", read_offset);
+	writel(read_offset, RA(priv->base + EMAC_RX_BUFFER_READ_OFFSET, priv->pid));
+	priv->rx_read_offset = read_offset;
+
+	if (read_offset != write_offset) {
+		if (packets < max_num) {
+			goto again;
+		}
+	}
+
+	return packets;
+}
+
+/**
+ * \brief Polling interface for emac
+ * \param napi napi structure
+ * \param budget budget
+ * \return error code (0=everything done, 1=still work todo)
+ */
+int emac_poll(struct napi_struct *napi, int budget) {
+	struct emac_priv *priv = container_of(napi, struct emac_priv, napi);
+	unsigned int write_offset = 0;
+	int read = 0;
+	int work_done = 0;
+
+	/* try to read packets */
+	while (work_done < budget) {
+		/* check for updated write offset */
+		CL1FLUSH;
+		write_offset = readl(priv->rx_buffer) & 0xFFFF;
+
+		if ((write_offset != 0) && (priv->rx_read_offset != write_offset)) {
+			/* Retrieve packets */
+			read = emac_rx(priv, write_offset, budget - work_done);
+			if (read > 0) {
+				work_done += read;
+			}
+		} else {
+			/* Tell the system we are done polling */
+			napi_complete(napi);
+
+			/* Clear the interrupt */
+			emac_clear_interrupt(priv);
+			break;
+		}
+	}
+
+	return work_done;
+}
+
+/**
+ * \brief Read and display ethernet statistics
+ */
+void show_statistic(void) {
+	int i;
+
+	printk("Ethernet statistic: \n");
+	printk("----------------------------\n");
+	for (i = 0; i < 46; i++) {
+		printk("0x%4x\t-\t%4d\n", STAT0_TRBYTE + i * 8,
+               readl(RA(STAT0_TRBYTE + i * 8, 0)));
+	}
+	printk("----------------------------\n");
+}
+
+/**
+ * \brief Transfer packet to network deivce
+ * \param skb buffer we want to transfer
+ * \param dev network device
+ * \return error code
+ */
+int emac_tx(struct sk_buff *skb, struct net_device *dev) {
+	struct emac_priv *priv = netdev_priv(dev);
+	void *addr = NULL;
+	u16 read_offset = 0;
+	int rest = 0;
+	int packets = 0;
+	int sum = 0;
+
+	EPRINTK(DEBUG_WRITE, "packet len: %d\n", skb->len);
+
+	/* check for over/underflow */
+	if (skb->len < sizeof(struct iphdr) || skb->len > 1536) {
+		printk(KERN_NOTICE "emac_tx(): illegal packet length %d => drop\n",
+               skb->len);
+		priv->stats.tx_errors++;
+		priv->stats.tx_dropped++;
+
+		return 0;
+	}
+
+	priv->tx_write_offset++;
+	/* check if we need to wrap */
+	if (priv->tx_write_offset > priv->tx_buffer_max) {
+		priv->tx_write_offset = 1;
+	}
+
+	packets = CLINE_PACKETS(skb->len + 2);
+
+	read_offset = readl(RA(priv->base + EMAC_TX_BUFFER_READ_OFFSET, priv->pid));
+#ifdef OVERFLOW_CHECK
+again:
+
+	if (read_offset < priv->tx_write_offset) {
+		sum = priv->tx_buffer_max - priv->tx_write_offset + read_offset - 1;
+	} else if (read_offset > priv->tx_write_offset) {
+		sum = read_offset - priv->tx_write_offset - 1;
+	}
+
+	if (sum < packets) {
+		EPRINTK(DEBUG_WRITE, "2. Warning: not enough space available, "
+                "retrying...\n");
+		goto again;
+	}
+#endif
+
+	addr = priv->tx_buffer + priv->tx_write_offset * 32;
+
+	/* Set frame length */
+	((u8*)addr)[0] = skb->len % 256;
+	((u8*)addr)[1] = skb->len / 256;
+
+	if (priv->tx_write_offset + packets - 1 <= priv->tx_buffer_max) {
+		/* enough space, just copy */
+		memcpy(addr + 2, skb->data, skb->len);
+
+		/* increment write ptr */
+		priv->tx_write_offset += packets - 1;
+	} else {
+		/* wrap in offsets. first copy to the end, second at the starting
+         * point
+         */
+		int bytes_left = skb->len;
+		int bytes_to_copy = (priv->tx_buffer_max - priv->tx_write_offset + 1) *
+                             32 - 2;
+
+		if (bytes_left < bytes_to_copy) {
+			bytes_to_copy = bytes_left;
+		}
+
+		EPRINTK(DEBUG_WRITE, "special case: copy last %d bytes\n",
+                bytes_to_copy);
+
+		memcpy(addr + 2, skb->data, bytes_to_copy);
+		bytes_left -= bytes_to_copy;
+
+		if (bytes_left != 0) {
+			priv->tx_write_offset = 1;
+			addr = priv->tx_buffer + 32;
+			EPRINTK(DEBUG_WRITE, "special case: copy remaining %d bytes\n",
+                    bytes_left);
+			memcpy(addr, skb->data + bytes_to_copy, bytes_left);
+
+			rest = bytes_left % 32;
+			if (rest != 0) {
+				rest = 32 - rest;
+			}
+			EPRINTK(DEBUG_WRITE, "Rest is %d\n", rest);
+			priv->tx_write_offset += CLINE_PACKETS(bytes_left + rest) - 1;
+		}
+	}
+
+	writel(2, priv->tx_buffer);
+
+	/* set new write offset */
+	EPRINTK(DEBUG_WRITE, "Update tx write offset: %d (read offset %d)\n",
+            priv->tx_write_offset, read_offset);
+
+	writel(priv->tx_write_offset, RA(priv->base + EMAC_TX_BUFFER_WRITE_OFFSET,
+                   priv->pid));
+
+	dev->trans_start = jiffies;
+	priv->stats.tx_packets++;
+	priv->stats.tx_bytes += skb->len;
+	dev_kfree_skb_any(skb);
+
+	return 0;
+}
+
+/**
+ * \brief Handler transmit timeout
+ * \param dev network device
+ */
+void emac_tx_timeout(struct net_device *dev) {
+	netif_wake_queue(dev);
+}
+
+/**
+ * \brief Return network statistics
+ * \param dev network device
+ * \return network statistic
+ */
+struct net_device_stats *emac_stats(struct net_device *dev) {
+	struct emac_priv *priv = netdev_priv(dev);
+
+	return &(priv->stats);
+}
+
+/**
+ * \brief Change mtu of network device
+ * \param dev network device
+ * \param new_mtu new mtu size
+ * \return error code
+ */
+int emac_change_mtu(struct net_device *dev, int new_mtu) {
+	if ((new_mtu < sizeof(struct iphdr)) || (new_mtu > BUFFER_SIZE - 1)) {
+		return -EINVAL;
+	}
+
+	dev->mtu = new_mtu;
+
+	return 0;
+}
+
+static const struct net_device_ops emac_netdev_ops = {
+	.ndo_open = emac_open,
+	.ndo_stop = emac_stop,
+	.ndo_start_xmit = emac_tx,
+	.ndo_tx_timeout = emac_tx_timeout,
+	.ndo_change_mtu = emac_change_mtu,
+	.ndo_get_stats = emac_stats,
+};
+
+/**
+ * \brief Initialize emac network device
+ * \param dev network device
+ */
+void emac_init(struct net_device *dev) {
+	/* set standard infos */
+	ether_setup(dev);
+
+	/* Network driver specific functions */
+	dev->netdev_ops = &emac_netdev_ops;
+	dev->watchdog_timeo = 5;
+
+	/* Note LVT1's vector is set to 3, LVT0 = 4 */
+	dev->irq = EMAC_IRQ_NR;
+
+	dev->hard_header_len = 2 + ETH_HLEN;
+}
+
+/**
+ * \brief Main network driver entry
+ * \return error code
+ */
+static int __init emac_module_init(void) {
+	struct emac_priv *priv = NULL;
+	int errorCode;
+	int macPorts = 0;
+	int tmp;
+
+	printk(KERN_DEBUG "eMAC driver %s\n", MODVERSTRING);
+
+	grb = sccsys_get_grb();
+
+	/* Get SCC processor id */
+	core0 = sccsys_get_pid();
+
+	tmp = readl(RA(0x822C, 0));
+	if (tmp & EMAC0) {
+		emac_writel(0x00, RA(EMAC0 + EMAC_RX_NETWORK_PORT_ENABLE, core0));
+		emac_writel(0x00, RA(EMAC0 + EMAC_TX_NETWORK_PORT_ENABLE, core0));
+	}
+	if (tmp & EMAC1) {
+		emac_writel(0x00, RA(EMAC1 + EMAC_RX_NETWORK_PORT_ENABLE, core0));
+		emac_writel(0x00, RA(EMAC1 + EMAC_TX_NETWORK_PORT_ENABLE, core0));
+	}
+	if (tmp & EMAC2) {
+		emac_writel(0x00, RA(EMAC2 + EMAC_RX_NETWORK_PORT_ENABLE, core0));
+		emac_writel(0x00, RA(EMAC2 + EMAC_TX_NETWORK_PORT_ENABLE, core0));
+	}
+	if (tmp & EMAC3) {
+		emac_writel(0x00, RA(EMAC3 + EMAC_RX_NETWORK_PORT_ENABLE, core0));
+		emac_writel(0x00, RA(EMAC3 + EMAC_TX_NETWORK_PORT_ENABLE, core0));
+	}
+
+	if (!override) {
+		tmp >>= 16;
+	}
+
+	macPorts = ((tmp >> 9) & 0xFF);
+
+	printk(KERN_DEBUG "eMAC0: %s eMAC1: %s eMAC2: %s eMAC3: %s\n",
+		(macPorts & EMAC0) != 0 ? "present" : "-",
+		(macPorts & EMAC1) != 0 ? "present" : "-",
+		(macPorts & EMAC2) != 0 ? "present" : "-",
+		(macPorts & EMAC3) != 0 ? "present" : "-");
+
+
+	/* Create emac0 if requested */
+	if ((macPorts & EMAC0) && (ethernet_port & EMAC0)) {
+		/* initialize xilinx ip */
+		init_xilinx_port(0, XILINX_EMAC0_BASE);
+
+		emac0_dev = alloc_netdev(sizeof(struct emac_priv), "emac0", emac_init);
+		if (emac0_dev==NULL) {
+			printk(KERN_ERR "alloc_netdev() failed\n");
+			return -ENOMEM;
+		}
+		priv = netdev_priv(emac0_dev);
+		memset(priv, 0, sizeof(struct emac_priv));
+		priv->device = EMAC0;
+		priv->base = EMAC0_BASE;
+		priv->net_device = emac0_dev;
+		netif_napi_add(emac0_dev, &priv->napi, emac_poll, EMAC_NAPI_WEIGHT);
+
+		errorCode = register_netdev(emac0_dev);
+		if (errorCode) {
+			printk(KERN_ERR "error %i registering device \"%s\"\n", errorCode,
+                   emac0_dev->name);
+			return -ENODEV;
+		}
+	}
+
+	/* Create emac1 if requested */
+	if ((macPorts & EMAC1) && (ethernet_port & EMAC1)) {
+		/* initialize xilinx ip */
+		init_xilinx_port(1, XILINX_EMAC1_BASE);
+
+		emac1_dev = alloc_netdev(sizeof(struct emac_priv), "emac1", emac_init);
+		if (emac1_dev==NULL) {
+			printk(KERN_ERR "alloc_netdev() failed\n");
+			return -ENOMEM;
+		}
+		priv = netdev_priv(emac1_dev);
+		memset(priv, 0, sizeof(struct emac_priv));
+		priv->device = EMAC1;
+		priv->base = EMAC1_BASE;
+		priv->net_device = emac1_dev;
+		netif_napi_add(emac1_dev, &priv->napi, emac_poll, EMAC_NAPI_WEIGHT);
+
+		errorCode = register_netdev(emac1_dev);
+		if (errorCode) {
+			printk(KERN_ERR "error %i registering device \"%s\"\n", errorCode,
+                    emac1_dev->name);
+			return -ENODEV;
+		}
+	}
+
+	/* Create emac2 if requested */
+	if ((macPorts & EMAC2) && (ethernet_port & EMAC2)) {
+		/* initialize xilinx ip */
+		init_xilinx_port(2, XILINX_EMAC2_BASE);
+
+		emac2_dev = alloc_netdev(sizeof(struct emac_priv), "emac2", emac_init);
+		if (emac2_dev==NULL) {
+			printk(KERN_ERR "alloc_netdev() failed\n");
+			return -ENOMEM;
+		}
+		priv = netdev_priv(emac2_dev);
+		memset(priv, 0, sizeof(struct emac_priv));
+		priv->device = EMAC2;
+		priv->base = EMAC2_BASE;
+		priv->net_device = emac2_dev;
+		netif_napi_add(emac2_dev, &priv->napi, emac_poll, EMAC_NAPI_WEIGHT);
+
+		errorCode = register_netdev(emac2_dev);
+		if (errorCode) {
+			printk(KERN_ERR "error %i registering device \"%s\"\n", errorCode,
+                    emac2_dev->name);
+			return -ENODEV;
+		}
+	}
+
+	/* Create emac3 if requested */
+	if ((macPorts & EMAC3) && (ethernet_port & EMAC3)) {
+		/* initialize xilinx ip */
+		init_xilinx_port(3, XILINX_EMAC3_BASE);
+
+		emac3_dev = alloc_netdev(sizeof(struct emac_priv), "emac3", emac_init);
+		if (emac3_dev==NULL) {
+			printk(KERN_ERR "alloc_netdev() failed\n");
+			return -ENOMEM;
+		}
+		priv = netdev_priv(emac3_dev);
+		memset(priv, 0, sizeof(struct emac_priv));
+		priv->device = EMAC3;
+		priv->base = EMAC3_BASE;
+		priv->net_device = emac3_dev;
+		netif_napi_add(emac3_dev, &priv->napi, emac_poll, EMAC_NAPI_WEIGHT);
+
+		errorCode = register_netdev(emac3_dev);
+		if (errorCode) {
+			printk(KERN_ERR "error %i registering device \"%s\"\n", errorCode,
+                    emac3_dev->name);
+			return -ENODEV;
+		}
+	}
+
+
+#ifdef NOTIFIER
+	register_netdevice_notifier(&emac_dev_notifier);
+#endif
+
+	return 0;
+}
+
+/**
+ * \brief Main network driver removal function
+ */
+static void __exit emac_module_exit(void) {
+#ifdef NOTIFIER
+	/* unregister netdevice notifier: event handler */
+	EPRINTK(DEBUG_INFO, "unregister netdevice notifier\n");
+	unregister_netdevice_notifier(&emac_dev_notifier);
+#endif
+
+	/* if network device has been created, remove it and free structure */
+	if (emac0_dev != NULL) {
+		unregister_netdev(emac0_dev);
+		free_netdev(emac0_dev);
+	}
+
+	if (emac1_dev != NULL) {
+		unregister_netdev(emac1_dev);
+		free_netdev(emac1_dev);
+	}
+
+	if (emac2_dev != NULL) {
+		unregister_netdev(emac2_dev);
+		free_netdev(emac2_dev);
+	}
+
+	if (emac3_dev != NULL) {
+		unregister_netdev(emac3_dev);
+		free_netdev(emac3_dev);
+	}
+}
+
+module_init(emac_module_init);
+module_exit(emac_module_exit);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Jan-Michael Brummer");
+MODULE_VERSION(MODVERSTRING);
+MODULE_DESCRIPTION("Intel(R) eMAC Network Driver");
diff -urN linux-3.1.4/drivers/net/sccemac.h linux-3.1.4-scc/drivers/net/sccemac.h
--- linux-3.1.4/drivers/net/sccemac.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-3.1.4-scc/drivers/net/sccemac.h	2011-12-20 15:27:07.609021371 +0100
@@ -0,0 +1,231 @@
+/*******************************************************************************
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms of the GNU General Public License as published by the Free
+  Software Foundation; either version 2 of the License, or (at your option)
+  any later version.
+
+  This program is distributed in the hope that it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+  more details.
+
+  You should have received a copy of the GNU General Public License along with
+  this program; if not, write to the Free Software Foundation, Inc., 59
+  Temple Place - Suite 330, Boston, MA  02111-1307, USA.
+
+  The full GNU General Public License is included in this distribution in the
+  file called LICENSE.
+
+  Contact Information:
+  Jan-Michael Brummer <jan-michael.brummer@intel.com>
+  Intel Braunschweig
+
+*******************************************************************************/
+
+#ifndef EMAC_H
+#define EMAC_H
+
+#include <linux/version.h>
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/pci.h>
+#include <linux/init.h>
+#include <linux/interrupt.h>
+
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/ip.h>
+
+#include <asm/cacheflush.h>
+#include <asm/apic.h>
+
+#include <asm/lapic.h>
+
+#include <linux/sccsys.h>
+
+#define DEBUG_EMAC
+
+#ifdef DEBUG_EMAC
+#define EPRINTK(_level, x...) do {\
+	if (_level & debug_level) {\
+			printk(KERN_INFO "%s(): ", __FUNCTION__);\
+			printk(x);\
+		}\
+	} while ( 0 )
+#else
+#define EPRINTK(_level, x...) //
+#endif
+
+/* Debugging */
+#define DEBUG_READ								0x01
+#define DEBUG_WRITE								0x02
+#define DEBUG_INFO								0x04
+#define DEBUG_TXBUFFER							0x100
+
+/* Read 16bit from buffer */
+//#define U16(_addr)								(256 * (*((volatile u8*)
+//							(_addr + 1))) + (*((volatile u8*)(_addr))))
+
+#define U16(_addr)								(256 * (*((u8*)\
+							(_addr + 1))) + (*((u8*)(_addr))))
+
+/* MAC Address */
+#define MAC_ADDRESS								0x00454D414331ULL
+#define MAC_HI(_x)								((((_x) >> 32)) & 0xFFFF)
+#define MAC_LO(_x)								(((_x) ) & 0xFFFFFFFF)
+
+/* Cache line wrappers */
+#define CLINE_SHIFT								5
+#define CLINE_SIZE								(1UL << CLINE_SHIFT)
+#define CLINE_MASK								(~(CLINE_SIZE - 1))
+#define CLINE_ALIGN(_x)							(((_x) + CLINE_SIZE - 1) & CLINE_MASK)
+#define CLINE_PACKETS(_x)						(CLINE_ALIGN(_x) >> CLINE_SHIFT)
+
+/* Flush */
+#define CL1FLUSH								__asm__ volatile (".byte 0x0F; .byte 0x0A;\n")
+
+/* Limits */
+/* Minimum buffer size must be 48 + 1! in order to handle a maximum ethernet frame */
+/** Order of 9 = 2MB, order of 4 = 64k */
+#define BUFFER_ORDER							9
+#define BUFFER_NUM								(1 << BUFFER_ORDER)
+#define BUFFER_SIZE								(BUFFER_NUM * PAGE_SIZE)
+
+/* Mapping */
+#define RA(_x, _y)								(grb + (_x) + (_y) * 4)
+
+/* Interrupt configuration */
+#ifdef USE_LVT0
+	#define EMAC_LVT							APIC_LVT0
+	#define EMAC_IRQ_MASK						0x00000002
+	#define EMAC_IRQ_NR							4
+	#define EMAC_IRQ_CONFIG						0
+#else
+	#define EMAC_LVT							APIC_LVT1
+	#define EMAC_IRQ_MASK						0x00000001
+	#define EMAC_IRQ_NR							3
+	#define EMAC_IRQ_CONFIG						1
+#endif
+
+#define EMAC0									0x01
+#define EMAC1									0x02
+#define EMAC2									0x04
+#define EMAC3									0x08
+
+#define EMAC0_BASE								0x9000
+#define EMAC1_BASE								0xA000
+#define EMAC2_BASE								0xB000
+#define EMAC3_BASE								0xC000
+
+/* NAPI weight */
+#define EMAC_NAPI_WEIGHT						(64)
+
+/* EMAC RX */
+#define EMAC_RX_BUFFER_START_ADDRESS			0x0000
+#define EMAC_RX_BUFFER_READ_OFFSET				0x0100
+#define EMAC_RX_BUFFER_WRITE_OFFSET				0x0200
+#define EMAC_RX_BUFFER_SIZE						0x0300
+#define EMAC_RX_BUFFER_THRESHOLD				0x0400
+#define EMAC_RX_MODE							0x0500
+#define EMAC_RX_NETWORK_PORT_MAC_ADDRESS_HI		0x0600
+#define EMAC_RX_NETWORK_PORT_MAC_ADDRESS_LO		0x0700
+#define EMAC_RX_NETWORK_PORT_ENABLE				0x0800
+
+/* EMAC TX */
+#define EMAC_TX_BUFFER_START_ADDRESS			0x0900
+#define EMAC_TX_BUFFER_READ_OFFSET				0x0A00
+#define EMAC_TX_BUFFER_WRITE_OFFSET				0x0B00
+#define EMAC_TX_BUFFER_SIZE						0x0C00
+#define EMAC_TX_MODE							0x0D00
+#define EMAC_TX_NETWORK_PORT_ENABLE				0x0E00
+
+/* Xilinx IP configuration - base address */
+#define XILINX_EMAC0_BASE						0x3200
+#define XILINX_EMAC1_BASE						0x4200
+#define XILINX_EMAC2_BASE						0x5200
+#define XILINX_EMAC3_BASE						0x6200
+
+/* Xilinx IP configuration - offsets */
+#define CONFIG_FLOW_CONTROL_ADD					0xC0
+#define TRANSMITTER_ADDRESS						0x80
+#define RECEIVER1_ADDRESS						0x40
+#define CONFIG_ADD								0x100
+#define ADD_FILTER_MOD							0x190
+
+/* Xilinx ethernet statistic */
+#define STAT0_TRBYTE							0x00004400
+#define STAT0_REBYTE							0x00004408
+#define STAT0_UFREC								0x00004410
+#define STAT0_FRFRREC							0x00004418
+#define STAT0_64BREC							0x00004420
+#define STAT0_127BREC							0x00004428
+#define STAT0_255BREC							0x00004430
+#define STAT0_511BREC							0x00004438
+#define STAT0_1023BREC							0x00004440
+#define STAT0_MAXBREC							0x00004448
+#define STAT0_OVFROK							0x00004450
+#define STAT0_64BTRA							0x00004458
+#define STAT0_127BTRA							0x00004460
+#define STAT0_255BTRA							0x00004468
+#define STAT0_511BTRA							0x00004470
+#define STAT0_1023BTRA							0x00004478
+#define STAT0_MAXBTRA							0x00004480
+#define STAT0_OVSZTX							0x00004488
+#define STAT0_FRRXOK							0x00004490
+#define STAT0_FRCHERR							0x00004498
+#define STAT0_BCFRRXOK							0x000044a0
+#define STAT0_MCFRRXOK							0x000044a8
+#define STAT0_CTFRRXOK							0x000044b0
+#define STAT0_LGOUTRG							0x000044b8
+#define STAT0_VLFRRXOK							0x000044c0
+#define STAT0_PFRRXOK							0x000044c8
+#define STAT0_CTRRXBAD							0x000044d0
+#define STAT0_FRTRANOK							0x000044d8
+#define STAT0_BCFRTXOK							0x000044e0
+#define STAT0_MCFRTXOK							0x000044e8
+#define STAT0_UNDERR							0x000044f0
+#define STAT0_CTFRTXOK							0x000044f8
+#define STAT0_VLFRTXOK							0x00004500
+#define STAT0_PSFRTXOK							0x00004508
+#define STAT0_SGLCOLFR							0x00004510
+#define STAT0_MLTCOLFR							0x00004518
+#define STAT0_DEFTRANS							0x00004520
+#define STAT0_LATCOLL							0x00004528
+#define STAT0_EXCCOLL							0x00004530
+#define STAT0_FRWEXCD							0x00004538
+#define STAT0_FRRXAERR							0x00004540
+#define STAT0_UNDCOUNT							0x00004548
+
+/** private network information */
+struct emac_priv {
+	/** network statistic */
+	struct net_device_stats stats;
+	/** indicates which device port is in use */
+	u8 device;
+	/* register base address */
+	u32 base;
+	/** flag for polling shutdown */
+	u8 shutdown;
+	/** own core id */
+	u8 pid;
+	/** rx ring buffer */
+	u8 *rx_buffer;
+	/** tx ring buffer */
+	u8 *tx_buffer;
+	/** maximum rx buffer level */
+	u32 rx_buffer_max;
+	/** current rx buffer level */
+	u32 rx_read_offset;
+	/** maximum tx buffer level */
+	u32 tx_buffer_max;
+	/** current tx buffer level */
+	u32 tx_write_offset;
+	/** napi structure */
+	struct napi_struct napi;
+	/** network device */
+	struct net_device *net_device;
+};
+
+#endif
diff -urN linux-3.1.4/drivers/net/sccmb.c linux-3.1.4-scc/drivers/net/sccmb.c
--- linux-3.1.4/drivers/net/sccmb.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-3.1.4-scc/drivers/net/sccmb.c	2011-12-20 15:27:07.618380472 +0100
@@ -0,0 +1,1275 @@
+/*
+ * sccmb.c -- SCC message buffer driver
+ *
+ * Portions Copyright (C) 2009 Intel Corp.
+ *
+ * The code is based on sccmb.c from the book "Linux Device
+ * Drivers" by Alessandro Rubini and Jonathan Corbet, published
+ * by O'Reilly & Associates.
+ */
+
+/* Notes:
+ * - This driver assumes that the SCC system memory driver is also
+ *   loaded as it performs the appropriate initialisation
+ */
+ 
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/moduleparam.h>
+
+#include <linux/sched.h>
+#include <linux/kernel.h>       /* printk() */
+#include <linux/slab.h>         /* kmalloc() */
+#include <linux/errno.h>        /* error codes */
+#include <linux/types.h>        /* size_t */
+#include <linux/interrupt.h>    /* mark_bh */
+
+#include <linux/in.h>
+#include <linux/netdevice.h>    /* struct device, and other headers */
+#include <linux/etherdevice.h>  /* eth_type_trans */
+#include <linux/ip.h>           /* struct iphdr */
+#include <linux/tcp.h>          /* struct tcphdr */
+#include <linux/skbuff.h>
+
+#include <linux/in6.h>
+#include <asm/checksum.h>
+
+#include <asm/io.h>             /* ioremap() & friends */
+#include <asm/pgtable.h>        /* page protection bits */
+#include <asm/apic.h>           /* apic_read() & apic_write() */
+#include <asm/lapic.h>          /* (un)set_lapic_mask */
+
+#include <linux/sccsys.h>
+
+MODULE_AUTHOR("Werner Haas");
+MODULE_LICENSE("GPL");
+
+/* DEBUG messages */
+#define DEBUG_MSG 0
+#define PRINTD(format, args...) if (DEBUG_MSG) { printk(format, ##args); }
+
+
+/*
+ * use the below define to aid in debugging
+ */
+/* #define DBG_INTERRUPT */
+
+/*
+ * runs better without NAPI poll methodology
+ */
+/* #define SCCMB_NO_NAPI*/
+
+
+
+/*
+ * Module parameters
+ */
+static int timeout = 5;
+module_param(timeout, int, 0644);
+MODULE_PARM_DESC(timeout, "Timeout period in jiffies of the NETDEV watchdog");
+
+static int mpb_size = SCC_MPB_SIZE;
+module_param(mpb_size, int, 0644);
+MODULE_PARM_DESC(mpb_size, "Available MPB space for the network layer");
+
+static int mpb_stride = SCC_TILE_SIZE;
+module_param(mpb_stride, int, 0644);
+MODULE_PARM_DESC(mpb_stride, "Stride between adjacent MPBs");
+
+static int mpb_offset = 0xC0000000;
+module_param(mpb_offset, int, 0644);
+MODULE_PARM_DESC(mpb_offset, "Start address of the on-die SRAM memory range");
+
+/* Optionally allow up to 4 packets in flight per destination */
+static int multiPacket = 0;
+module_param(multiPacket, int, 0644);
+MODULE_PARM_DESC(multiPacket, "Enable/disable multiple packets in flight");
+
+/* When interrupts are disabled the NAPI poll function is added to the queue 
+ * at startup never returns 0.
+ */
+static int noIrq = 0;
+module_param(noIrq, int, 0644);
+MODULE_PARM_DESC(noIrq, "Do not use interrupts to trigger receiver");
+
+/* Trigger mode of the receiver interrupt:
+ * level = sender sets IRQ bit, cleared by receiver
+ * edge  = sender sets and clears IRQ bit
+ * Note that the pulse generation may not work reliably on SCC!
+ */
+static int edgeIrq = 1;
+module_param(edgeIrq, int, 0644);
+MODULE_PARM_DESC(edgeIrq, "Generate an IRQ edge, i.e. set&clear status bit");
+
+/* Default: do not use the SCC test&set register so the driver does not 
+ * interfere with other software such as RCCE
+ */
+static int disable_locking = 0;
+module_param(disable_locking, int, 0644);
+MODULE_PARM_DESC(disable_locking, "Enable/disable use of the test&set bits");
+
+/* IP address of the local network the router core to enable packet forwarding.
+ * Note that the parameter format is octet0|octet1|octet2|octet3.
+ */
+static unsigned int ownIpAddress = 0xC0A80000;       /* 192.168.0.0 */
+module_param(ownIpAddress, uint, 0644);
+MODULE_PARM_DESC(ownIpAddress, "IP address of the local network");
+
+static unsigned int routerIpAddress = 0xC0A80001;    /* 192.168.0.1 */
+module_param(routerIpAddress, uint, 0644);
+MODULE_PARM_DESC(routerIpAddress, "IP address of a router core");
+
+static int retriggerInt = 1;
+#ifdef DBG_INTERRUPT
+module_param(retriggerInt, int, 0644);
+MODULE_PARM_DESC(retriggerInt, "Control retriggering of interrupt in case of timeout");
+
+
+static int pingPongTarget = 0;
+module_param(pingPongTarget, int, 0644);
+MODULE_PARM_DESC(pingPongTarget, "Ping Pong Target Core number (1 - 48)");
+#endif
+
+
+
+
+/*
+ * The SCC message buffer device
+ */
+struct net_device* sccmb_dev;
+
+struct sccmb_priv {
+  struct net_device_stats     stats;
+  struct napi_struct          napi;
+  spinlock_t                  lock;
+    
+  /* Device-specific constants:
+   * - the local IP address
+   * - the size of the data FIFO (in cache lines)
+   */
+  u8                          localIp;
+  u8                          mpb_buffersize;
+  /* SCC specific physical memory addresses are mapped into kernel space 
+   */
+  /* mpb = Message Passing Buffer */
+  void*                       mpb[SCC_CORECOUNT];
+  /* The FIFO data structure for the data transfer algorithm 
+   * - pointer to the next cache line in the circular buffer
+   * - busy flag for all peers
+   * - map indicating which peer might be accessing each cache line
+   */
+  u8                          mpb_next;
+  int                         mpb_busy[4*SCC_CORECOUNT];
+  u8                          mpb_map[SCC_MPB_SIZE/SCC_CLINE_SIZE];
+  u8                          mpb_nextDesc[SCC_CORECOUNT];
+  u8                          mpb_rxDesc[SCC_CORECOUNT];
+#ifdef DBG_PACKET_TRACE
+void*     packetTrace;
+unsigned  txTracePtr;
+unsigned  rxTracePtr;
+#endif
+
+#ifdef DBG_INTERRUPT
+unsigned txIntCount[SCC_CORECOUNT];
+unsigned txRetriggerCount[SCC_CORECOUNT];
+unsigned rxIntCount;
+#endif
+};
+
+
+
+/*
+ * SCC specific helper functions
+ */
+
+
+/* ip2phys calculates the physical address of the MPB for the
+ * core ID
+ */
+int sccmb_id2phys(u8 id)
+{
+  int tile_number = (id/2);
+  int core_number = (id%2);
+  int result;
+  
+  /* Only accept IDs for the available cores */
+  if (id >= SCC_CORECOUNT) return 0;
+  
+    result  = mpb_offset;
+    result += mpb_stride*tile_number;
+    result += SCC_MPB_SIZE*core_number;
+
+  return PAGE_ALIGN(result);
+}
+
+
+/* Calculate the descriptor address for a packet transfer from the given
+ * sender IP address. Note that the corresponding descriptor data is stored
+ * in the receiver's MPB.
+ */
+int sccmb_get_desc_address(struct net_device* dev, u8 sender_ip, u8 receiver_ip)
+{
+  struct sccmb_priv*          priv = netdev_priv(dev);
+  int                         address;
+  
+  /* The packet descriptors are stored at the receiver side i.e. start with
+   * the message passing buffer of the destination
+   */
+  address  = (int)(priv->mpb[receiver_ip-1]);
+  /* The descriptor area is located at the top i.e. skip the data range */
+  address += (SCC_CLINE_SIZE * priv->mpb_buffersize);
+  /* Evaluate the offset corresponding to the sender IP */
+  address += sender_ip-1;
+
+  return address;
+}
+
+
+/* lock/unlock access the LOCK bit in the core register bank of the
+ * given IP address
+ */
+void sccmb_lock(struct net_device* dev, u8 ip_address)
+{
+	int pid = ip_address - 1;
+
+	if (disable_locking) return;
+
+	sccsys_acquire_pid_lock(pid);
+}
+
+void sccmb_unlock(struct net_device* dev, u8 ip_address)
+{
+	int pid = ip_address - 1;
+
+	if (disable_locking) return;
+
+	sccsys_release_pid_lock(pid);
+}
+
+
+/* trigger_irq/clear_irq access the interrupt bit in the core register bank 
+ * of the given IP address which is connected to the processor's INTR pin
+ */
+void sccmb_trigger_irq(struct net_device* dev, u8 ip_address)
+{
+	int pid = ip_address - 1;
+	sccsys_trigger_irq_direct(pid, SCC_INTR_MASK, edgeIrq);
+}
+
+
+/* reset interrupt bit for core 
+ */
+void sccmb_clear_irq(struct net_device* dev, u8 ip_address)
+{
+	int pid = ip_address - 1;
+	sccsys_clear_irq_direct(pid, SCC_INTR_MASK);
+}
+
+
+/* Flush the write-combining buffer
+ */
+void sccmb_flush_wcbuffer(struct net_device* dev)
+{
+  struct sccmb_priv*          priv = netdev_priv(dev);
+  int                         address;
+  int                         value;
+  int                         i;
+  
+  /* Read & write back our own message buffer to flush the write-combining
+   * data. The area at the beginning of the MPB is read-only for other cores,
+   * i.e. we can safely access these cache lines. By reading/writing data
+   * from the first 2 cache lines we have at least 1 address change and thus
+   * the data is forwarded to its destination.
+   */
+  address = (int)priv->mpb[priv->localIp-1];
+  for (i=0; i<2; i++) {
+    value = *((volatile int*)address);
+    CL1FLUSHMB;
+    *((volatile int*)address) = value;
+    
+    address += SCC_CLINE_SIZE;
+  }
+}
+
+
+
+/* tx_pending returns the current status of send operations to the specified IP
+ * In case the local busy flag is set, the remote message passing buffer is
+ * queried to check whether the operation is still pending
+ */
+int sccmb_tx_pending(struct net_device* dev, u8 ip_address, int slot)
+{
+  struct sccmb_priv*          priv = netdev_priv(dev);
+  int                         desc_address;
+  int                         i;
+  int 			      timesent;
+  static int 		      timeout_count = 0;
+
+  /* 0 is not a valid IP address and marks unused circular buffer slots */
+  if (ip_address == 0) return 0;
+  
+  /* Select the next Tx descriptor slot if none is specified */
+  if (slot < 0) slot = priv->mpb_nextDesc[ip_address-1];
+  
+  /* Check whether there is an undelivered packet */
+  if (priv->mpb_busy[slot*SCC_CORECOUNT + ip_address-1]) {
+    /* Read the descriptor from the destination to see whether it is still
+     * valid. First we have to flush potentially stale data, though.
+     */
+    CL1FLUSHMB;
+    
+    /* Calculate the descriptor address for the packet transfer from this
+     * node to the given IP address.
+     */
+    desc_address = sccmb_get_desc_address(dev, priv->localIp, ip_address)
+                 + SCC_CORECOUNT * slot;
+    
+    /* Read the packet offset
+     * 0xFF is illegal because the descriptors are located at the top i.e.
+     * this value signals transfer completed.
+     */
+    if (*((u8*)desc_address) != 0xFF) {
+      /* Check the age of the transmission and delete the packet if it is
+       * too old.
+       */
+      timesent =  priv->mpb_busy[slot*SCC_CORECOUNT+ip_address-1];
+      if (jiffies > timesent + timeout)
+      {
+        timeout_count++;
+	if (timeout_count == 1000) {
+	  printk(KERN_DEBUG "sccmb_tx_pending(): Timeout at destination %d\n", 
+		 ip_address);
+	  timeout_count = 0;
+	}
+      
+        /* Free all cache lines holding data for that destination */
+        for (i=0; i<priv->mpb_buffersize; i++) {
+          if ((priv->mpb_map[i]) == ((slot<<6)+ip_address) ) 
+            priv->mpb_map[i] = 0;
+        }
+      }
+      /* Else signal that the transmission is still pending and retrigger the target with another interrupt */
+      else {
+        if  (retriggerInt && (jiffies > timesent)) {
+          sccmb_trigger_irq(dev, ip_address);
+#ifdef DBG_INTERRUPT
+	  priv->txRetriggerCount[ip_address - 1]++;
+#endif
+	}
+	return 1;
+      }
+    }
+    
+    /* Clear the busy flag */
+    priv->mpb_busy[slot*SCC_CORECOUNT + ip_address-1] = 0;
+  }
+  
+  return 0;
+}
+
+
+/* set_descriptor updates the start of packet pointer corresponding to our
+ * own ID at the given destination
+ */
+void sccmb_set_descriptor(struct net_device* dev, u8 ip_address, u8 offset)
+{
+  struct sccmb_priv*          priv = netdev_priv(dev);
+  int                         desc_address;
+  
+  /* Make sure that the write misses in the L1 */
+  CL1FLUSHMB;
+  
+  /* Calculate the descriptor address for the packet transfer from this
+   * node to the given IP address. In case multiple packets may be in
+   * flight, the address has to be adjusted according to the current
+   * descriptor slot.
+   */
+  desc_address = sccmb_get_desc_address(dev, priv->localIp, ip_address)
+               + SCC_CORECOUNT * priv->mpb_nextDesc[ip_address-1];
+    
+  /* Write the packet offset for the new transfer */
+  sccmb_lock(dev, ip_address);
+  (*((u8*)desc_address) ) = offset;
+  sccmb_flush_wcbuffer(dev);
+  sccmb_unlock(dev, ip_address);
+}
+
+
+/* clear_descriptor invalidates the packet offset in our own descriptor area,
+ * i.e. marks the data transfer from the specified IP address as completed.
+ */
+void sccmb_clear_descriptor(struct net_device* dev, u8 ip_address)
+{
+  struct sccmb_priv*          priv = netdev_priv(dev);
+  int                         desc_address;
+    
+  /* Make sure that the write misses in the L1 */
+  CL1FLUSHMB;
+  
+  /* Calculate the descriptor address for the packet transfer from the
+   * given IP address to our node and adjust it according to the descriptor
+   * slot where we found the packet.
+   */
+  desc_address = sccmb_get_desc_address(dev, ip_address, priv->localIp)
+               + SCC_CORECOUNT * priv->mpb_rxDesc[ip_address-1];
+
+  /* Write 0xFF to enable message transfers again */
+  sccmb_lock(dev, priv->localIp);
+  (*((u8*)desc_address) ) = 0xFF;
+  sccmb_flush_wcbuffer(dev);
+  sccmb_unlock(dev, priv->localIp);
+}
+
+
+
+/*
+ * Open and close
+ * These functions are called when an interface is activated/stopped. Thus,
+ * any system resources should be registered and the device itself should
+ * be initialized.
+ */
+static irqreturn_t sccmb_interrupt(int, void*);
+
+void sccmb_unmap(struct net_device* dev)
+{
+  struct sccmb_priv*          priv = netdev_priv(dev);
+  int                         i;
+
+  for (i=0; i<SCC_CORECOUNT; i++) {
+    iounmap(priv->mpb[i]);
+  }
+}
+
+int sccmb_open(struct net_device* dev)
+{
+  struct sccmb_priv*          priv = netdev_priv(dev);
+  int                         status = 0;
+  int                         i;
+  int                         address;
+  int                         tmp;
+  int                         pid;
+
+  /* MPB initialisation */
+  
+  /* Calculate how many cache lines are available for the circular buffer
+   * We need 1 status byte per core for message descriptors; in case
+   * support for multiple packets in-flight to the same destination is
+   * enabled, the descriptor range is expanded accordingly.
+   */
+  i = SCC_CORECOUNT/SCC_CLINE_SIZE;
+  if (multiPacket) i = 4*SCC_CORECOUNT/SCC_CLINE_SIZE;
+  if (i%SCC_CLINE_SIZE) i++;
+  priv->mpb_buffersize = (mpb_size/SCC_CLINE_SIZE) - i;
+      
+  /* Map all message buffers */
+  for (i=0; i<SCC_CORECOUNT; i++) {
+    /* Map the address directly, setting the PMB bit so the memory is indeed
+     * treated as message buffer by the core. Also mark it as Write-Through
+     * in order to avoid flushing the L1 cache to get the data into the
+     * message buffer.
+     */
+    address = sccmb_id2phys(i);
+    priv->mpb[i] = ioremap_mpbt(address, mpb_size);
+    if (!priv->mpb[i]) status = -EIO;
+  }
+
+  if (status) {
+    printk(KERN_WARNING "sccmb_open(): Can't map message passing buffer\n");
+    sccmb_unmap(dev);
+    return status;
+  }
+
+  pid = sccsys_get_pid();
+  /* Add 1 to the processor ID to avoid *.*.*.0 IP addresses */
+  priv->localIp = pid + 1;
+
+#ifdef DBG_PACKET_TRACE
+/* Map 2*1MB DDR3 memory per core as packet trace buffer */
+priv->packetTrace = ioremap_nocache(0x80800000ul + 0x400000ul*x + 0x200000ul*z, 2*1024*1024);
+memset(priv->packetTrace, 0, 2*1024*1024);
+#endif
+  PRINTD(KERN_INFO "sccmb_open(): sccmb local IP = %d\n", priv->localIp);
+
+  /* Initialize the descriptor area located at the top of the message buffer */
+  address  = (int)(priv->mpb[priv->localIp-1]);
+  address += (SCC_CLINE_SIZE * priv->mpb_buffersize);
+  tmp = SCC_CORECOUNT;
+  if (multiPacket) tmp *= 4;
+  for (i=0; i<tmp; i++) *((u8*)(address+i)) = 0xFF;
+  sccmb_flush_wcbuffer(dev);
+    
+  /* Configure interrupt handling */
+  status = request_irq(dev->irq, &sccmb_interrupt, IRQF_SHARED, "sccmb", dev);
+  if (status) {
+    printk(KERN_WARNING "Can't get interrupt #%d\n", dev->irq);
+    sccmb_unmap(dev);
+    return status;
+  }
+
+  /* Assign the hardware address of the board (6 octets for Ethernet)
+   * Note that the first octet of ethernet multicast addresses is odd
+   */
+  memcpy(dev->dev_addr, "\0MPB_0", ETH_ALEN);
+    
+  netif_start_queue(dev);
+  napi_enable(&priv->napi);
+
+  /* If interrupts are not used we immediately add the polling function
+   * to the queue which would otherwise be done through the IRQ handler.
+   */
+  if (noIrq) napi_schedule(&priv->napi);
+
+  return 0;
+}
+
+
+int sccmb_close(struct net_device* dev)
+{
+  struct sccmb_priv*          priv = netdev_priv(dev);
+  napi_disable(&priv->napi);
+
+  /* Unmap the SCC resources */
+  free_irq(dev->irq, dev);
+  sccmb_unmap(dev);
+  
+  netif_stop_queue(dev);
+  return 0;
+}
+
+
+
+/*!
+ * Configuration changes (passed on by ifconfig)
+ */
+int sccmb_config(struct net_device* dev, struct ifmap* map)
+{	
+#ifdef DBG_INTERRUPT
+	int i;
+	struct sccmb_priv*          priv = netdev_priv(dev);
+#endif
+
+/* Debugging printout trigger */
+#ifdef DBG_INTERRUPT
+	if (map->base_addr == 0xdeb9) {
+	printk(KERN_INFO "sccmb_config(): got debug trigger! (0x%08x)\n", map->base_addr);
+
+	printk(KERN_DEBUG "APIC_ID  : %lx\n", apic_read(APIC_ID));
+	printk(KERN_DEBUG "APIC_LVT0: %lx\n", apic_read(APIC_LVT0));
+	printk(KERN_DEBUG "APIC_LVT1: %lx\n", apic_read(APIC_LVT1));
+	printk(KERN_DEBUG "APIC_ESR : %lx\n", apic_read(APIC_ESR));
+	for (i=0; i < 8; i++) {
+		printk(KERN_DEBUG "APIC_ISR%i: %lx\n", i, apic_read(APIC_ISR + i*0x10));
+	}
+	for (i=0; i < 8; i++) {
+		printk(KERN_DEBUG "APIC_TMR%i: %lx\n", i, apic_read(APIC_TMR + i*0x10));
+	}
+	for (i=0; i < 8; i++) {
+		printk(KERN_DEBUG "APIC_IRR%i: %lx\n", i, apic_read(APIC_IRR + i*0x10));
+	}
+
+	return 0;
+	}
+#endif
+
+#ifdef DBG_INTERRUPT
+	if (map->base_addr == 0xdeba) {
+	for (i=0; i < 48; i++) {
+		printk(KERN_DEBUG "TX int count to core %i: %i (ReTrig: %i)\n", i, priv->txIntCount[i], priv->txRetriggerCount[i]);
+		priv->txIntCount[i] = 0;
+		priv->txRetriggerCount[i] = 0;
+	}
+	printk(KERN_DEBUG "RX int count: %i\n", priv->rxIntCount);
+	priv->rxIntCount = 0;
+	
+	return 0;
+	}
+#endif
+
+#ifdef DBG_INTERRUPT
+	if (map->base_addr == 0xdebb) {
+	  if (pingPongTarget > 0) {
+	    printk(KERN_DEBUG "Triggering Ping Pong to core %i\n", pingPongTarget);
+	    sccmb_trigger_irq(dev, pingPongTarget);
+	  }
+
+	return 0;
+	}
+#endif
+
+
+  /* Can't act on a running interface */
+  if (dev->flags & IFF_UP) return -EBUSY;
+
+  /* Don't allow changing the I/O address */
+  if (map->base_addr != dev->base_addr) {
+    printk(KERN_WARNING "sccmb: Can't change I/O address\n");
+    return -EOPNOTSUPP;
+  }
+
+  /* Allow changing the IRQ */
+  if (map->irq != dev->irq) {
+    dev->irq = map->irq;
+    /* request_irq() is delayed to open-time */
+  }
+
+  /* ignore other fields */
+  return 0;
+}
+
+
+
+/*
+ * Receive a packet: retrieve, encapsulate and pass over to upper levels
+ */
+void sccmb_rx(struct net_device* dev, u8 sourceIp, u8 offset)
+{
+  struct sccmb_priv*          priv = netdev_priv(dev);
+  struct sk_buff*             skb;
+  int                         mpb_address;
+  int                         len;
+  int                         headroom;
+  int                         byteCount;
+  unsigned char*              local_mem;
+
+  /* Calculate the address of the packet */
+  mpb_address  = (int)(priv->mpb[sourceIp-1]);
+  mpb_address += offset*SCC_CLINE_SIZE;
+  
+  /* The packet length is stored in the first 2 bytes but does not include
+   * the header. Check for reasonable sizes before processing the data to
+   * prevent nasty memory overflow errors.
+   */
+  len = 256*( *((u8*)(mpb_address  )) )
+      +     ( *((u8*)(mpb_address+1)) );
+  mpb_address += 2;
+
+  if (len < sizeof(struct iphdr) || len > dev->mtu) {
+    /* Simply drop the packet */
+    sccmb_clear_descriptor(dev, sourceIp);
+    
+    printk(KERN_NOTICE "sccmb_rx(): illegal packet length %d => drop\n", len);
+    priv->stats.rx_dropped++;
+    return;
+  }
+        
+  /* Build a skb for the packet data so upper layers can handle it
+   * Note that IP headers should be aligned on 16B boundaries!
+   */
+  skb = dev_alloc_skb(len);
+  if (!skb) {
+    if (printk_ratelimit() )
+      printk(KERN_NOTICE "sccmb rx: low on mem - packet dropped\n");
+
+    /* Note: since we do not clear the offset descriptor we do not trigger
+     * a retransmission and the packet will eventually be processed.
+     */
+    priv->stats.rx_dropped++;
+    return;
+  }
+
+  /* IP headers should be aligned on 16B boundaries, i.e. we just reserve the
+   * payload area and do not prepend the header bytes which are not used
+   * anymore.
+   */
+  local_mem = skb_put(skb, len);
+
+  /* Copy the packet data (without the header) into the buffer. Because 
+   * of the circular buffer implementation we may have to do it in 2 steps.
+   */
+  /* First calculate how much data we can fetch before the wrap-around
+   * occurs so we can determine how much to copy in the first run. Note
+   * that the headroom includes the 2 header bytes as the entire packet
+   * is cacheline aligned!
+   */
+  headroom = (priv->mpb_buffersize - offset) * SCC_CLINE_SIZE - 2;
+  byteCount = len;
+  if (headroom < byteCount) byteCount = headroom;
+  memcpy(skb->data, (void*)mpb_address, byteCount);
+
+#ifdef DBG_PACKET_TRACE
+/* Store the entire packet (including header bytes) also in the trace buffer */
+memcpy((void*)(priv->packetTrace + 1024*1024 + priv->rxTracePtr), (void*)(mpb_address-2), byteCount+2);
+priv->rxTracePtr += byteCount+2;
+if (priv->rxTracePtr > 1024*1024-2048) priv->rxTracePtr = 0;
+#endif
+  
+  /* Finish with the rest if we could not copy the entire packet */
+  if (byteCount < len) {
+    memcpy(skb->data+byteCount, priv->mpb[sourceIp-1], len-byteCount);
+#ifdef DBG_PACKET_TRACE
+memcpy((void*)(priv->packetTrace + 1024*1024 + priv->rxTracePtr), priv->mpb[sourceIp-1], len-byteCount);
+priv->rxTracePtr += len-byteCount;
+if (priv->rxTracePtr > 1024*1024-2048) priv->rxTracePtr = 0;
+#endif
+  }
+
+#ifdef DBG_PACKET_TRACE
+/* Align the rx trace pointer to cacheline boundaries */
+while (priv->rxTracePtr % SCC_CLINE_SIZE) priv->rxTracePtr++;
+#endif
+
+  /* Clear the descriptor as we no longer need the message buffer content */
+  sccmb_clear_descriptor(dev, sourceIp);
+
+  /* Update the interface statistics (also count the header bytes) */
+  priv->stats.rx_packets++;
+  priv->stats.rx_bytes += 2+len;
+  dev->last_rx = jiffies;
+
+  /* Write metadata, and then pass to the receive level */
+  skb->dev        = dev;
+  skb_set_transport_header(skb, 0);
+  skb_set_network_header(skb, 0);
+  skb_set_mac_header(skb, 0);
+  skb->protocol   = htons(ETH_P_IP);
+  skb->pkt_type   = PACKET_HOST;
+  skb->ip_summed  = CHECKSUM_UNNECESSARY;
+
+#ifdef SCCMB_NO_NAPI
+    netif_rx(skb);
+#else
+    netif_receive_skb(skb);
+#endif
+
+}
+
+
+
+/*
+ * Check the receive descriptors and return the IP address if a valid
+ * packet offset is found. Note that a round robin scheme is used to
+ * ensure fairness.
+ */
+static int sccmb_nextRxPacket(struct net_device* dev, u8* offset)
+{
+  struct sccmb_priv*          priv = netdev_priv(dev);
+  static u8                   lastSource = 0;
+  u8                          source;
+  u8                          i;
+  u8                          nrOfDescriptors;
+  int                         desc_address;
+  
+  nrOfDescriptors = multiPacket ? 4*SCC_CORECOUNT : SCC_CORECOUNT;
+  
+  *offset = 0xFF;
+  for (i=0; i<nrOfDescriptors; i++) {
+    /* Calculate the descriptor address for the packet transfer from the
+     * given IP address to our node. Add 1 to the last found packet source
+     * so it gets processed with least priority.
+     */
+    source = (1+lastSource + i) % nrOfDescriptors;
+    desc_address = sccmb_get_desc_address(dev, 1+source, priv->localIp);
+    
+    /* Check for a valid receive descriptor */
+    *offset = *((u8*)desc_address);
+    if (*offset < 0xFF) {
+      /* Store the descriptor slot where we found the packet */
+      priv->mpb_rxDesc[source%SCC_CORECOUNT] = source/SCC_CORECOUNT;
+      
+      /* Remember the core number and return the IP address = 1+core number */
+      lastSource = source;
+      return 1 + source%SCC_CORECOUNT;
+    }
+  }
+  
+  /* Return 0 if no packet was found */
+  return 0;
+}
+
+
+/*
+ * The poll implementation processing all incoming packets
+ */
+static int sccmb_poll(struct napi_struct* napi, int to_do)
+{
+  struct sccmb_priv*          priv = container_of(napi, struct sccmb_priv, napi);
+  struct net_device*          dev = napi->dev;
+  int                         npackets = 0;
+  int                         quota = to_do;
+  u8                          sourceIp;
+  u8                          offset = 0;
+
+  /* Process up to quota packets from the receive queue */
+  while (npackets<quota) {
+    /* Flush the message buffer so we get up to date descriptor info */
+    CL1FLUSHMB;
+    sourceIp = sccmb_nextRxPacket(dev, &offset);
+    
+    /* Call the standard receive handler if there is a packet */
+    if (sourceIp) {
+      /* In case of level-triggered interrupts clear it now */
+      if (!edgeIrq) sccmb_clear_irq(dev, priv->localIp);
+      
+      sccmb_rx(dev, sourceIp, offset);
+      npackets++;
+    }
+    /* Else perform a clean exit */
+    else {
+
+      /* If interrupts are disabled we hae to remain in the polling function,
+       * i.e. we may signal that we are done with packet processing but must
+       * not remove us from the queueing list via netif_rx_complete().
+       */
+      if (noIrq) {
+        break;
+      }
+      
+      /* Tell the system we are done with polling */
+      napi_complete(&priv->napi);
+
+      /* Do not use enable_irq(dev->irq); since we might loose enables --> INT locked up */      
+      unset_lapic_mask(APIC_LVT0, dev->irq);
+
+      /* The interrupt was cleared before the Rx packet was processed,
+       * i.e. it will be re-set if another packet arrived since the last
+       * check. Thus the polling function will be scheduled again as
+       * soon as we enable interrupts again so we can safely return.
+       */
+      break;
+    }
+  }
+  
+  return npackets;
+}
+
+
+
+/*
+ * A NAPI interrupt handler since we can receive multiple packets 
+ * (from different sources) with a single interrupt.
+ */
+static irqreturn_t sccmb_interrupt(int irq, void* dev_id)
+{
+  struct net_device*          dev = (struct net_device*)dev_id;
+  struct sccmb_priv*          priv = netdev_priv(dev);
+
+#ifdef SCCMB_NO_NAPI
+  u8                          sourceIp = 0; 
+  u8                          offset = 0; 
+#endif
+
+  /* Paranoid */
+  if (!dev) {
+    printk(KERN_DEBUG "sccmb interrupt %d for unknown device\n", irq);
+    return IRQ_NONE;
+  }
+
+#ifdef DBG_INTERRUPT
+  priv->rxIntCount++;
+  
+  if (pingPongTarget > 0) {
+    sccmb_trigger_irq(dev, pingPongTarget);
+    return IRQ_HANDLED;
+  }
+#endif
+
+#ifdef SCCMB_NO_NAPI
+  CL1FLUSHMB;
+  sourceIp = sccmb_nextRxPacket(dev, &offset);
+  while (sourceIp) {
+    /* Call the standard receive handler if there is a packet */
+    sccmb_rx(dev, sourceIp, offset);
+    /* Flush the message buffer so we get up to date descriptor info */
+    CL1FLUSHMB;
+    sourceIp = sccmb_nextRxPacket(dev, &offset);
+  }
+  return IRQ_HANDLED;
+#else
+
+  /* Disable further interrupts and start the polling process. 
+   * We have to use the nosync version since we are inside the interrupt
+   * service routine!
+   * Do not use disable_irq_nosync(dev->irq); since we might loose enables --> INT locked up
+   */
+
+  set_lapic_mask(APIC_LVT0, dev->irq);
+
+  napi_schedule(&priv->napi);
+  return IRQ_HANDLED;
+#endif
+}
+
+
+
+
+/*
+ * Transmit a packet (low level interface)
+ * This function deals with HW details, i.e. it writes the packet
+ * into the message buffer and informs the destination.
+ */
+u8 sccmb_get_destination(struct net_device* dev, char* packet_buf)
+{
+  struct iphdr*               ipHeader;
+  u32*                        destAddr;
+  u8                          destCore;
+  u8                          netAddr[4];
+  int                         i;
+  
+  /* Extract the destination address from the IP header.
+   * The last (4th) octet is interpreted as core ID.
+   */
+  ipHeader  = (struct iphdr*)(packet_buf+dev->hard_header_len);
+  destAddr  = &(ipHeader->daddr);
+  destCore  = ((u8*)destAddr)[3];
+
+  /* Compare the network part to check if we have a local destination.
+   * Note that the module parameter is expected in A.B.C.D format to make
+   * it easier to modify so the values have to be reversed.
+   * In case a remote network is detected, the core number of the router
+   * is used as destination.
+   */
+  for (i=0; i<4; i++) netAddr[3-i] = (ownIpAddress >> 8*i) & 0xFF;
+  for (i=0; i<3; i++) {
+    if (((u8*)destAddr)[i] != netAddr[i]) destCore = routerIpAddress & 0xFF;
+  }
+  
+  /* Make sure that only valid local IP addresses are returned */
+  if ((destCore < 1) || (destCore > 48)) destCore = 1;
+  
+  return destCore;
+}
+
+static int sccmb_hw_tx(struct net_device* dev, struct sk_buff* skb, u8 destIp)
+{
+  struct sccmb_priv*          priv = netdev_priv(dev);
+  u8                          clen;
+  int                         i;
+  int                         ptr;
+  int                         owner;
+  int                         slot;
+
+  /* Determine how many cache lines we need to store the message buffer data */
+  clen = (skb->len / SCC_CLINE_SIZE);
+  if (skb->len % SCC_CLINE_SIZE) clen++;
+  
+  /* Check whether there is enough space in the circular buffer */
+  for (i=0; i<clen; i++) {
+    ptr = (priv->mpb_next + i) % priv->mpb_buffersize;
+    owner = priv->mpb_map[ptr] & 0x3F;
+    slot  = priv->mpb_map[ptr] >> 6;
+    if (sccmb_tx_pending(dev, owner, slot) ) return 1;
+    
+    /* This slot will be used for the new packet transfer - also mark the
+     * the descriptor slot that is being used.
+     */
+    priv->mpb_map[ptr] = (priv->mpb_nextDesc[destIp-1] << 6) + destIp;
+  }
+  
+  /* Copy the data into the circular buffer */
+  ptr  = (int)(priv->mpb[priv->localIp-1]);
+  ptr += (priv->mpb_next * SCC_CLINE_SIZE);
+  if (clen + priv->mpb_next > priv->mpb_buffersize) {
+    /* First fill up the remaining space to the top of the buffer */
+    i = (priv->mpb_buffersize - priv->mpb_next) * SCC_CLINE_SIZE;
+    memcpy((void*)ptr, skb->data, i);
+    
+    /* Copy the remaining bytes to the start */
+    ptr  = (int)(priv->mpb[priv->localIp-1]);
+    memcpy((void*)ptr, skb->data+i, skb->len-i);
+  }
+  else {
+    /* Simply copy all the data since there is enough space without
+     * pointer roll-over.
+     */
+    memcpy((void*)ptr, skb->data, skb->len);
+  }
+
+#ifdef DBG_PACKET_TRACE
+/* Store the packet in the trace buffer */
+memcpy((void*)(priv->packetTrace + priv->txTracePtr), skb->data, skb->len);
+priv->txTracePtr += clen*SCC_CLINE_SIZE;
+if (priv->txTracePtr > 1024*1024-2048) priv->txTracePtr = 0;
+#endif    
+  /* Set the descriptor on the receiver side
+   * Since different addresses are used, this automatically flushes the
+   * data inside the write-combining buffer so the receiver sees up to
+   * date message buffer content.
+   */
+  sccmb_set_descriptor(dev, destIp, priv->mpb_next);
+  
+  /* Update the pointer to the next circular buffer slot and flag the
+   * pending transmission.
+   */
+  slot = priv->mpb_nextDesc[destIp-1];
+  priv->mpb_next = (priv->mpb_next + clen) % priv->mpb_buffersize;
+  priv->mpb_busy[slot*SCC_CORECOUNT + destIp-1] = jiffies;
+
+  /* Interrupt the receiver so it starts processing the packet */
+  sccmb_trigger_irq(dev, destIp);
+  
+  /* Increment the next descriptor slot for the next transmission in case
+   * multiple packets may be in flight.
+   */
+  if (multiPacket)
+    priv->mpb_nextDesc[destIp-1] = (priv->mpb_nextDesc[destIp-1] + 1) % 4;
+      
+  /* Transmission succeeded so save the timestamp of the transmission,
+   * update the statistics and free the socket buffer 
+   */
+  dev->trans_start = jiffies;
+
+  priv->stats.tx_packets++;
+  priv->stats.tx_bytes += skb->len;
+  dev_kfree_skb_any(skb);
+  
+  return 0;
+}
+
+
+
+/*
+ * Transmit a packet (called by the kernel)
+ */
+int sccmb_tx(struct sk_buff* skb, struct net_device* dev)
+{
+  struct sccmb_priv*          priv = netdev_priv(dev);
+  u8                          destIp = sccmb_get_destination(dev, skb->data);
+  u8                          slot = priv->mpb_nextDesc[destIp-1];
+  u8                          i;
+  
+  //printk(KERN_DEBUG "sccmb_tx(): start\n");
+  
+  /* Perform a sanity check on the packet data, i.e. silently drop packets
+   * that are either too short or send to an illegal IP address by indicating
+   * success without executing any data transfer operations.
+   */
+  if ((skb->len < dev->hard_header_len+sizeof(struct iphdr) ) || 
+      (destIp > SCC_CORECOUNT) )
+  {
+    printk(KERN_NOTICE "sccmb_tx(): Illegal packet (%i octets to IP %d)\n", 
+                       skb->len, destIp);
+    goto drop_packet;
+  }
+  
+  /* Check if another transmission to the same destination is pending */
+  if (sccmb_tx_pending(dev, destIp, -1) ) {
+    /* If too much time elapsed since the previous transmission the receiver
+     * is probably dead and we drop the packet
+     */
+    if (jiffies > priv->mpb_busy[slot*SCC_CORECOUNT + destIp-1] + timeout) {
+      printk(KERN_NOTICE "sccmb_tx(): Timeout at destination %d\n", destIp);
+      
+      /* Also free all cache lines holding data for that destination */
+      for (i=0; i<priv->mpb_buffersize; i++) {
+        if ((priv->mpb_map[i]) == ((slot<<6)+destIp) )
+          priv->mpb_map[i] = 0;
+      }
+      
+      goto drop_packet;
+    }
+    /* Else signal the kernel that the transmission failed so it can 
+     * reschedule 
+     */
+    return 1;
+  }
+  
+  /* Now perform the low level data transfer and return the result. If it
+   * failed, e.g. because there is not enough space in the MPB, the kernel
+   * will retry transmission.
+   */
+  return sccmb_hw_tx(dev, skb, destIp);
+  
+drop_packet:
+  priv->stats.tx_errors++;
+  dev_kfree_skb_any(skb);
+  return 0;
+}
+
+
+
+/*
+ * Deal with a transmit timeout.
+ */
+void sccmb_tx_timeout(struct net_device* dev)
+{
+  /* A timeout occurs if the queue was stopped because too many packet
+   * transfers were pending. Let's simply resume regular packet processing
+   * in the hope that the receiver have cleared their backlog.
+   */
+  //printk(KERN_DEBUG "sccmb_tx_timeout()\n");
+  
+  netif_wake_queue(dev);
+  return;
+}
+
+
+
+/*
+ * ioctl commands
+ */
+int sccmb_ioctl(struct net_device* dev, struct ifreq* rq, int cmd)
+{
+  /*
+   * Custom commands
+   */
+  return 0;
+}
+
+
+
+/*
+ * Return statistics to the caller
+ */
+struct net_device_stats* sccmb_stats(struct net_device* dev)
+{
+  struct sccmb_priv* priv = netdev_priv(dev);
+  
+  return &(priv->stats);
+}
+
+
+
+/*
+ * The "change_mtu" method is usually not needed.
+ * If you need it, it must be like this.
+ */
+int sccmb_change_mtu(struct net_device* dev, int new_mtu)
+{
+  struct sccmb_priv*          priv = netdev_priv(dev);
+  unsigned long               flags;
+  spinlock_t*                 lock = &(priv->lock);
+
+  /* check ranges */
+  if ((new_mtu < 68) || (new_mtu > 1500)) return -EINVAL;
+
+  /*
+   * Simply accept the value
+   */
+  spin_lock_irqsave(lock, flags);
+  dev->mtu = new_mtu;
+  spin_unlock_irqrestore(lock, flags);
+  
+  return 0;
+}
+
+
+
+/*
+ * This function is called to fill up an eth header, since ARP is not
+ * available on the interface.
+ */
+int sccmb_rebuild_header(struct sk_buff* skb)
+{
+  printk(KERN_WARNING "sccmb_rebuild_header() called - ignoring\n");
+
+  return 0;
+}
+
+int sccmb_header(struct sk_buff* skb, struct net_device* dev,
+                 unsigned short type, const void* daddr, const void* saddr,
+                 unsigned int len)
+{
+  /* Prepend 2 header bytes containing the packet length */
+  u8* header = skb_push(skb, 2);
+
+  /* Store the length starting with the MSBs */
+  header[0] = len/256;
+  header[1] = len%256;
+      
+  return (dev->hard_header_len);
+}
+
+
+static const struct net_device_ops sccmb_netdev_ops = {
+	.ndo_open		= sccmb_open,
+	.ndo_stop		= sccmb_close,
+	.ndo_set_config		= sccmb_config,
+	.ndo_start_xmit		= sccmb_tx,
+	.ndo_do_ioctl		= sccmb_ioctl,
+	.ndo_get_stats		= sccmb_stats,
+	.ndo_change_mtu		= sccmb_change_mtu,
+	.ndo_tx_timeout		= sccmb_tx_timeout,
+};
+
+static const struct header_ops sccmb_header_ops = {
+	.create			= sccmb_header,
+	.rebuild		= sccmb_rebuild_header,
+};
+
+/*
+ * The init function (sometimes called probe).
+ * It is invoked by register_netdev()
+ */
+void sccmb_init(struct net_device* dev)
+{
+  struct sccmb_priv* priv;
+
+  /*
+   * Then initialize the priv field. This encloses the statistics
+   * and a few private fields.
+   */
+  priv = netdev_priv(dev);
+  memset(priv, 0, sizeof(struct sccmb_priv));
+  
+  spin_lock_init(&priv->lock);
+
+  /* Get meaningful default values */
+  ether_setup(dev);
+
+  /* Set the correct function pointers */
+  dev->netdev_ops = &sccmb_netdev_ops;
+  dev->header_ops = &sccmb_header_ops;
+  
+  dev->watchdog_timeo   = timeout;
+
+  /* Configure NAPI interrupt handling because we may receive multiple
+   * packets per interrupt. Note that Lehnix/MCEMU set LVT0's vector to 4.
+   */
+  netif_napi_add(dev, &priv->napi, sccmb_poll, 16);
+  dev->irq              = 4;
+
+  /* Keep the default flags; just add NOARP */
+  dev->flags           |= IFF_NOARP;
+  /* Checksum checks are not required */
+  // dev->features        |= NETIF_F_NO_CSUM;
+  /* Disable caching of (nonexistent) ARP replies */
+  //dev->hard_header_cache = NULL;
+  /* Change the hardware header as there is no need for an Ethernet format */
+  dev->hard_header_len    = 2;
+  dev->addr_len           = 0;
+}
+
+
+
+/*
+ * Finally, the module stuff
+ */
+void sccmb_cleanup(void)
+{
+  if (sccmb_dev) {
+    unregister_netdev(sccmb_dev);
+    free_netdev(sccmb_dev);
+  }
+}
+
+
+
+int sccmb_init_module(void)
+{
+  int result;
+
+  /* This driver does only work in a bare-metal environment */
+  if (!scc_bare_metal()) {
+    printk(KERN_INFO "sccmb: startup in non-SCC or paravirtualized environment.\n");
+    return -EINVAL;
+  }
+
+  /* Allocate the devices */
+  sccmb_dev = alloc_netdev(sizeof(struct sccmb_priv), "mb0", sccmb_init);
+  if (!sccmb_dev) return -ENOMEM;
+
+  result = register_netdev(sccmb_dev);
+  if (result) {
+    printk(KERN_WARNING "sccmb: error %i registering device \"%s\"\n", 
+                        result, sccmb_dev->name);
+    free_netdev(sccmb_dev);
+    return -ENODEV;
+  }
+
+  return 0;
+}
+
+
+
+module_init(sccmb_init_module);
+module_exit(sccmb_cleanup);
diff -urN linux-3.1.4/drivers/net/sccmbx.c linux-3.1.4-scc/drivers/net/sccmbx.c
--- linux-3.1.4/drivers/net/sccmbx.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-3.1.4-scc/drivers/net/sccmbx.c	2011-12-20 15:27:07.618380472 +0100
@@ -0,0 +1,1650 @@
+/*
+ * sccmb.c -- SCC message buffer driver
+ *
+ * Portions Copyright (C) 2009 Intel Corp.
+ *
+ * The code is based on sccmb.c from the book "Linux Device
+ * Drivers" by Alessandro Rubini and Jonathan Corbet, published
+ * by O'Reilly & Associates.
+ */
+
+/* Notes:
+ * - This driver assumes that the SCC system memory driver is also
+ *   loaded as it performs the appropriate initialisation
+ */
+ 
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/moduleparam.h>
+
+#include <linux/sched.h>
+#include <linux/kernel.h>       /* printk() */
+#include <linux/slab.h>         /* kmalloc() */
+#include <linux/errno.h>        /* error codes */
+#include <linux/types.h>        /* size_t */
+#include <linux/interrupt.h>    /* mark_bh */
+
+#include <linux/in.h>
+#include <linux/netdevice.h>    /* struct device, and other headers */
+#include <linux/etherdevice.h>  /* eth_type_trans */
+#include <linux/ip.h>           /* struct iphdr */
+#include <linux/tcp.h>          /* struct tcphdr */
+#include <linux/skbuff.h>
+
+#include <linux/in6.h>
+#include <asm/checksum.h>
+
+#include <asm/io.h>             /* ioremap() & friends */
+#include <asm/pgtable.h>        /* page protection bits */
+#include <asm/apic.h>           /* apic_read() & apic_write() */
+#include <asm/lapic.h>          /* (un)set_lapic_mask */
+#include <linux/kallsyms.h>	/* print_symbol */
+
+#include <linux/sccsys.h>
+#include "sccmbx.h"
+
+MODULE_AUTHOR("Werner Haas");
+MODULE_LICENSE("GPL");
+
+
+/* DEBUG messages */
+#define DEBUG_MSG 0
+#define PRINTD(format, args...) if (DEBUG_MSG) { printk(format, ##args); }
+
+
+/*
+ * use the below define to aid in debugging
+ */
+/* #define DBG_INTERRUPT */
+
+/*
+ * runs better without NAPI poll methodology
+ */
+/* #define SCCMB_NO_NAPI*/
+
+
+
+/*
+ * Module parameters
+ */
+static int timeout = 5;
+module_param(timeout, int, 0644);
+MODULE_PARM_DESC(timeout, "Timeout period in jiffies of the NETDEV watchdog");
+
+static int mpb_size = SCC_MPB_SIZE;
+module_param(mpb_size, int, 0644);
+MODULE_PARM_DESC(mpb_size, "Available MPB space for the network layer");
+
+static int mpb_stride = SCC_TILE_SIZE;
+module_param(mpb_stride, int, 0644);
+MODULE_PARM_DESC(mpb_stride, "Stride between adjacent MPBs");
+
+static int mpb_offset = 0xC0000000;
+module_param(mpb_offset, int, 0644);
+MODULE_PARM_DESC(mpb_offset, "Start address of the on-die SRAM memory range");
+
+/* Optionally allow up to 4 packets in flight per destination */
+static int multiPacket = 0;
+module_param(multiPacket, int, 0644);
+MODULE_PARM_DESC(multiPacket, "Enable/disable multiple packets in flight");
+
+/* When interrupts are disabled the NAPI poll function is added to the queue 
+ * at startup never returns 0.
+ */
+static int noIrq = 0;
+module_param(noIrq, int, 0644);
+MODULE_PARM_DESC(noIrq, "Do not use interrupts to trigger receiver");
+
+/* Trigger mode of the receiver interrupt:
+ * level = sender sets IRQ bit, cleared by receiver
+ * edge  = sender sets and clears IRQ bit
+ * Note that the pulse generation may not work reliably on SCC!
+ */
+static int edgeIrq = 1;
+module_param(edgeIrq, int, 0644);
+MODULE_PARM_DESC(edgeIrq, "Generate an IRQ edge, i.e. set&clear status bit");
+
+/* Default: do not use the SCC test&set register so the driver does not 
+ * interfere with other software such as RCCE
+ */
+static int disable_locking = 0;
+module_param(disable_locking, int, 0644);
+MODULE_PARM_DESC(disable_locking, "Enable/disable use of the test&set bits");
+
+/* IP address of the local network the router core to enable packet forwarding.
+ * Note that the parameter format is octet0|octet1|octet2|octet3.
+ */
+static unsigned int ownIpAddress = 0xC0A80000;       /* 192.168.0.0 */
+module_param(ownIpAddress, uint, 0644);
+MODULE_PARM_DESC(ownIpAddress, "IP address of the local network");
+
+static unsigned int routerIpAddress = 0xC0A80001;    /* 192.168.0.1 */
+module_param(routerIpAddress, uint, 0644);
+MODULE_PARM_DESC(routerIpAddress, "IP address of a router core");
+
+static int retriggerInt = 1;
+#ifdef DBG_INTERRUPT
+module_param(retriggerInt, int, 0644);
+MODULE_PARM_DESC(retriggerInt, "Control retriggering of interrupt in case of timeout");
+
+
+static int pingPongTarget = 0;
+module_param(pingPongTarget, int, 0644);
+MODULE_PARM_DESC(pingPongTarget, "Ping Pong Target Core number (1 - 48)");
+#endif
+
+
+
+
+/*
+ * The SCC message buffer device
+ */
+struct net_device* sccmb_dev;
+
+struct sccmb_priv {
+  struct net_device_stats     stats;
+  struct napi_struct          napi;
+  spinlock_t                  lock;
+    
+  /* Device-specific constants:
+   * - the local IP address
+   * - the size of the data FIFO (in cache lines)
+   */
+  u8                          localIp;
+  u8                          mpb_buffersize;
+  /* SCC specific physical memory addresses are mapped into kernel space 
+   */
+  /* mpb = Message Passing Buffer */
+  void*                       mpb[SCC_CORECOUNT];
+  /* The FIFO data structure for the data transfer algorithm 
+   * - pointer to the next cache line in the circular buffer
+   * - busy flag for all peers
+   * - map indicating which peer might be accessing each cache line
+   */
+  u8                          mpb_next;
+  int                         mpb_busy[4*SCC_CORECOUNT];
+  u8                          mpb_map[SCC_MPB_SIZE/SCC_CLINE_SIZE];
+  u8                          mpb_nextDesc[SCC_CORECOUNT];
+  u8                          mpb_rxDesc[SCC_CORECOUNT];
+#ifdef DBG_PACKET_TRACE
+void*     packetTrace;
+unsigned  txTracePtr;
+unsigned  rxTracePtr;
+#endif
+
+#ifdef DBG_INTERRUPT
+unsigned txIntCount[SCC_CORECOUNT];
+unsigned txRetriggerCount[SCC_CORECOUNT];
+unsigned rxIntCount;
+#endif
+};
+
+
+
+/*
+ * SCC specific helper functions
+ */
+
+
+/* ip2phys calculates the physical address of the MPB for the
+ * core ID
+ */
+int sccmb_id2phys(u8 id)
+{
+  int tile_number = (id/2);
+  int core_number = (id%2);
+  int result;
+  
+  /* Only accept IDs for the available cores */
+  if (id >= SCC_CORECOUNT) return 0;
+  
+    result  = mpb_offset;
+    result += mpb_stride*tile_number;
+    result += SCC_MPB_SIZE*core_number;
+
+  return PAGE_ALIGN(result);
+}
+
+
+/* Calculate the descriptor address for a packet transfer from the given
+ * sender IP address. Note that the corresponding descriptor data is stored
+ * in the receiver's MPB.
+ */
+int sccmb_get_desc_address(struct net_device* dev, u8 sender_ip, u8 receiver_ip)
+{
+  struct sccmb_priv*          priv = netdev_priv(dev);
+  int                         address;
+  
+  /* The packet descriptors are stored at the receiver side i.e. start with
+   * the message passing buffer of the destination
+   */
+  address  = (int)(priv->mpb[receiver_ip-1]);
+  /* The descriptor area is located at the top i.e. skip the data range */
+  address += (SCC_CLINE_SIZE * priv->mpb_buffersize);
+  /* Evaluate the offset corresponding to the sender IP */
+  address += sender_ip-1;
+
+  return address;
+}
+
+
+/* lock/unlock access the LOCK bit in the core register bank of the
+ * given IP address
+ */
+void sccmb_lock(struct net_device* dev, u8 ip_address)
+{
+	int pid = ip_address - 1;
+
+	if (disable_locking) return;
+
+	sccsys_acquire_pid_lock(pid);
+}
+
+void sccmb_unlock(struct net_device* dev, u8 ip_address)
+{
+	int pid = ip_address - 1;
+
+	if (disable_locking) return;
+
+	sccsys_release_pid_lock(pid);
+}
+
+
+/* trigger_irq/clear_irq access the interrupt bit in the core register bank 
+ * of the given IP address which is connected to the processor's INTR pin
+ */
+void sccmb_trigger_irq(struct net_device* dev, u8 ip_address)
+{
+	int pid = ip_address - 1;
+	sccsys_trigger_irq_direct(pid, SCC_INTR_MASK, edgeIrq);
+}
+
+
+/* reset interrupt bit for core 
+ */
+void sccmb_clear_irq(struct net_device* dev, u8 ip_address)
+{
+	int pid = ip_address - 1;
+	sccsys_clear_irq_direct(pid, SCC_INTR_MASK);
+}
+
+
+/* Flush the write-combining buffer
+ */
+void sccmb_flush_wcbuffer(struct net_device* dev)
+{
+  struct sccmb_priv*          priv = netdev_priv(dev);
+  int                         address;
+  int                         value;
+  int                         i;
+  
+  /* Read & write back our own message buffer to flush the write-combining
+   * data. The area at the beginning of the MPB is read-only for other cores,
+   * i.e. we can safely access these cache lines. By reading/writing data
+   * from the first 2 cache lines we have at least 1 address change and thus
+   * the data is forwarded to its destination.
+   */
+  address = (int)priv->mpb[priv->localIp-1];
+  for (i=0; i<2; i++) {
+    value = *((volatile int*)address);
+    CL1FLUSHMB;
+    *((volatile int*)address) = value;
+    
+    address += SCC_CLINE_SIZE;
+  }
+}
+
+
+
+/* tx_pending returns the current status of send operations to the specified IP
+ * In case the local busy flag is set, the remote message passing buffer is
+ * queried to check whether the operation is still pending
+ */
+int sccmb_tx_pending(struct net_device* dev, u8 ip_address, int slot)
+{
+  struct sccmb_priv*          priv = netdev_priv(dev);
+  int                         desc_address;
+  int                         i;
+  int 			      timesent;
+  static int 		      timeout_count = 0;
+
+  /* 0 is not a valid IP address and marks unused circular buffer slots */
+  if (ip_address == 0) return 0;
+  
+  /* Select the next Tx descriptor slot if none is specified */
+  if (slot < 0) slot = priv->mpb_nextDesc[ip_address-1];
+  
+  /* Check whether there is an undelivered packet */
+  if (priv->mpb_busy[slot*SCC_CORECOUNT + ip_address-1]) {
+    /* Read the descriptor from the destination to see whether it is still
+     * valid. First we have to flush potentially stale data, though.
+     */
+    CL1FLUSHMB;
+    
+    /* Calculate the descriptor address for the packet transfer from this
+     * node to the given IP address.
+     */
+    desc_address = sccmb_get_desc_address(dev, priv->localIp, ip_address)
+                 + SCC_CORECOUNT * slot;
+    
+    /* Read the packet offset
+     * 0xFF is illegal because the descriptors are located at the top i.e.
+     * this value signals transfer completed.
+     */
+    if (*((u8*)desc_address) != 0xFF) {
+      /* Check the age of the transmission and delete the packet if it is
+       * too old.
+       */
+      timesent =  priv->mpb_busy[slot*SCC_CORECOUNT+ip_address-1];
+      if (jiffies > timesent + timeout)
+      {
+        timeout_count++;
+	if (timeout_count == 1000) {
+	  printk(KERN_DEBUG "sccmb_tx_pending(): Timeout at destination %d\n", 
+		 ip_address);
+	  timeout_count = 0;
+	}
+      
+        /* Free all cache lines holding data for that destination */
+        for (i=0; i<priv->mpb_buffersize; i++) {
+          if ((priv->mpb_map[i]) == ((slot<<6)+ip_address) ) 
+            priv->mpb_map[i] = 0;
+        }
+      }
+      /* Else signal that the transmission is still pending and retrigger the target with another interrupt */
+      else {
+        if  (retriggerInt && (jiffies > timesent)) {
+          sccmb_trigger_irq(dev, ip_address);
+#ifdef DBG_INTERRUPT
+	  priv->txRetriggerCount[ip_address - 1]++;
+#endif
+	}
+	return 1;
+      }
+    }
+    
+    /* Clear the busy flag */
+    priv->mpb_busy[slot*SCC_CORECOUNT + ip_address-1] = 0;
+  }
+  
+  return 0;
+}
+
+
+/* set_descriptor updates the start of packet pointer corresponding to our
+ * own ID at the given destination
+ */
+void sccmb_set_descriptor(struct net_device* dev, u8 ip_address, u8 offset)
+{
+  struct sccmb_priv*          priv = netdev_priv(dev);
+  int                         desc_address;
+  
+  /* Make sure that the write misses in the L1 */
+  CL1FLUSHMB;
+  
+  /* Calculate the descriptor address for the packet transfer from this
+   * node to the given IP address. In case multiple packets may be in
+   * flight, the address has to be adjusted according to the current
+   * descriptor slot.
+   */
+  desc_address = sccmb_get_desc_address(dev, priv->localIp, ip_address)
+               + SCC_CORECOUNT * priv->mpb_nextDesc[ip_address-1];
+    
+  /* Write the packet offset for the new transfer */
+  sccmb_lock(dev, ip_address);
+  (*((u8*)desc_address) ) = offset;
+  sccmb_flush_wcbuffer(dev);
+  sccmb_unlock(dev, ip_address);
+}
+
+
+/* clear_descriptor invalidates the packet offset in our own descriptor area,
+ * i.e. marks the data transfer from the specified IP address as completed.
+ */
+void sccmb_clear_descriptor(struct net_device* dev, u8 ip_address)
+{
+  struct sccmb_priv*          priv = netdev_priv(dev);
+  int                         desc_address;
+    
+  /* Make sure that the write misses in the L1 */
+  CL1FLUSHMB;
+  
+  /* Calculate the descriptor address for the packet transfer from the
+   * given IP address to our node and adjust it according to the descriptor
+   * slot where we found the packet.
+   */
+  desc_address = sccmb_get_desc_address(dev, ip_address, priv->localIp)
+               + SCC_CORECOUNT * priv->mpb_rxDesc[ip_address-1];
+
+  /* Write 0xFF to enable message transfers again */
+  sccmb_lock(dev, priv->localIp);
+  (*((u8*)desc_address) ) = 0xFF;
+  sccmb_flush_wcbuffer(dev);
+  sccmb_unlock(dev, priv->localIp);
+}
+
+
+
+/*
+ * Open and close
+ * These functions are called when an interface is activated/stopped. Thus,
+ * any system resources should be registered and the device itself should
+ * be initialized.
+ */
+static irqreturn_t sccmb_interrupt(int, void*);
+
+void sccmb_unmap(struct net_device* dev)
+{
+  struct sccmb_priv*          priv = netdev_priv(dev);
+  int                         i;
+
+  for (i=0; i<SCC_CORECOUNT; i++) {
+    iounmap(priv->mpb[i]);
+  }
+}
+
+int sccmb_open(struct net_device* dev)
+{
+  struct sccmb_priv*          priv = netdev_priv(dev);
+  int                         status = 0;
+  int                         i;
+  int                         address;
+  int                         tmp;
+  int                         pid;
+
+  /* MPB initialisation */
+  
+  /* Calculate how many cache lines are available for the circular buffer
+   * We need 1 status byte per core for message descriptors; in case
+   * support for multiple packets in-flight to the same destination is
+   * enabled, the descriptor range is expanded accordingly.
+   */
+  i = SCC_CORECOUNT/SCC_CLINE_SIZE;
+  if (multiPacket) i = 4*SCC_CORECOUNT/SCC_CLINE_SIZE;
+  if (i%SCC_CLINE_SIZE) i++;
+  priv->mpb_buffersize = (mpb_size/SCC_CLINE_SIZE) - i;
+      
+  /* Map all message buffers */
+  for (i=0; i<SCC_CORECOUNT; i++) {
+    /* Map the address directly, setting the PMB bit so the memory is indeed
+     * treated as message buffer by the core. Also mark it as Write-Through
+     * in order to avoid flushing the L1 cache to get the data into the
+     * message buffer.
+     */
+    address = sccmb_id2phys(i);
+    priv->mpb[i] = ioremap_mpbt(address, mpb_size);
+    if (!priv->mpb[i]) status = -EIO;
+  }
+
+  if (status) {
+    printk(KERN_WARNING "sccmb_open(): Can't map message passing buffer\n");
+    sccmb_unmap(dev);
+    return status;
+  }
+
+  pid = sccsys_get_pid();
+  /* Add 1 to the processor ID to avoid *.*.*.0 IP addresses */
+  priv->localIp = pid + 1;
+
+#ifdef DBG_PACKET_TRACE
+/* Map 2*1MB DDR3 memory per core as packet trace buffer */
+priv->packetTrace = ioremap_nocache(0x80800000ul + 0x400000ul*x + 0x200000ul*z, 2*1024*1024);
+memset(priv->packetTrace, 0, 2*1024*1024);
+#endif
+  PRINTD(KERN_INFO "sccmb_open(): sccmb local IP = %d\n", priv->localIp);
+
+  /* Initialize the descriptor area located at the top of the message buffer */
+  address  = (int)(priv->mpb[priv->localIp-1]);
+  address += (SCC_CLINE_SIZE * priv->mpb_buffersize);
+  tmp = SCC_CORECOUNT;
+  if (multiPacket) tmp *= 4;
+  for (i=0; i<tmp; i++) *((u8*)(address+i)) = 0xFF;
+  sccmb_flush_wcbuffer(dev);
+    
+  /* Configure interrupt handling */
+  status = request_irq(dev->irq, &sccmb_interrupt, IRQF_SHARED, "sccmb", dev);
+  if (status) {
+    printk(KERN_WARNING "Can't get interrupt #%d\n", dev->irq);
+    sccmb_unmap(dev);
+    return status;
+  }
+
+  /* Assign the hardware address of the board (6 octets for Ethernet)
+   * Note that the first octet of ethernet multicast addresses is odd
+   */
+  memcpy(dev->dev_addr, "\0MPB_0", ETH_ALEN);
+    
+  netif_start_queue(dev);
+  napi_enable(&priv->napi);
+
+  /* If interrupts are not used we immediately add the polling function
+   * to the queue which would otherwise be done through the IRQ handler.
+   */
+  if (noIrq) napi_schedule(&priv->napi);
+
+  return 0;
+}
+
+
+int sccmb_close(struct net_device* dev)
+{
+  struct sccmb_priv*          priv = netdev_priv(dev);
+  napi_disable(&priv->napi);
+
+  /* Unmap the SCC resources */
+  free_irq(dev->irq, dev);
+  sccmb_unmap(dev);
+  
+  netif_stop_queue(dev);
+  return 0;
+}
+
+
+
+/*!
+ * Configuration changes (passed on by ifconfig)
+ */
+int sccmb_config(struct net_device* dev, struct ifmap* map)
+{	
+#ifdef DBG_INTERRUPT
+	int i;
+	struct sccmb_priv*          priv = netdev_priv(dev);
+#endif
+
+/* Debugging printout trigger */
+#ifdef DBG_INTERRUPT
+	if (map->base_addr == 0xdeb9) {
+	printk(KERN_INFO "sccmb_config(): got debug trigger! (0x%08x)\n", map->base_addr);
+
+	printk(KERN_DEBUG "APIC_ID  : %lx\n", apic_read(APIC_ID));
+	printk(KERN_DEBUG "APIC_LVT0: %lx\n", apic_read(APIC_LVT0));
+	printk(KERN_DEBUG "APIC_LVT1: %lx\n", apic_read(APIC_LVT1));
+	printk(KERN_DEBUG "APIC_ESR : %lx\n", apic_read(APIC_ESR));
+	for (i=0; i < 8; i++) {
+		printk(KERN_DEBUG "APIC_ISR%i: %lx\n", i, apic_read(APIC_ISR + i*0x10));
+	}
+	for (i=0; i < 8; i++) {
+		printk(KERN_DEBUG "APIC_TMR%i: %lx\n", i, apic_read(APIC_TMR + i*0x10));
+	}
+	for (i=0; i < 8; i++) {
+		printk(KERN_DEBUG "APIC_IRR%i: %lx\n", i, apic_read(APIC_IRR + i*0x10));
+	}
+
+	return 0;
+	}
+#endif
+
+#ifdef DBG_INTERRUPT
+	if (map->base_addr == 0xdeba) {
+	for (i=0; i < 48; i++) {
+		printk(KERN_DEBUG "TX int count to core %i: %i (ReTrig: %i)\n", i, priv->txIntCount[i], priv->txRetriggerCount[i]);
+		priv->txIntCount[i] = 0;
+		priv->txRetriggerCount[i] = 0;
+	}
+	printk(KERN_DEBUG "RX int count: %i\n", priv->rxIntCount);
+	priv->rxIntCount = 0;
+	
+	return 0;
+	}
+#endif
+
+#ifdef DBG_INTERRUPT
+	if (map->base_addr == 0xdebb) {
+	  if (pingPongTarget > 0) {
+	    printk(KERN_DEBUG "Triggering Ping Pong to core %i\n", pingPongTarget);
+	    sccmb_trigger_irq(dev, pingPongTarget);
+	  }
+
+	return 0;
+	}
+#endif
+
+
+  /* Can't act on a running interface */
+  if (dev->flags & IFF_UP) return -EBUSY;
+
+  /* Don't allow changing the I/O address */
+  if (map->base_addr != dev->base_addr) {
+    printk(KERN_WARNING "sccmb: Can't change I/O address\n");
+    return -EOPNOTSUPP;
+  }
+
+  /* Allow changing the IRQ */
+  if (map->irq != dev->irq) {
+    dev->irq = map->irq;
+    /* request_irq() is delayed to open-time */
+  }
+
+  /* ignore other fields */
+  return 0;
+}
+
+
+struct sccmbx_rx {
+	struct net_device* dev;
+	u8 sourceIp;
+	u8 offset;
+	int len;
+	int cleared;
+	void* mpb_address;
+	void* mpb_start;
+};
+
+/* Clear receive descriptor */
+void sccmbx_clear_rx(struct sccmbx_rx* rx)
+{
+  if (!rx->cleared) {
+    sccmb_clear_descriptor(rx->dev, rx->sourceIp);
+    rx->cleared = 1;
+  }
+}
+EXPORT_SYMBOL(sccmbx_clear_rx)
+
+/* Get pid of source node of message */
+int sccmbx_get_rx_source(struct sccmbx_rx* rx)
+{
+  return rx->sourceIp - 1;
+}
+EXPORT_SYMBOL(sccmbx_get_rx_source)
+
+/* Get pointer(s) to body of received packet. This routine can optionally
+ * allocate an sk_buff if the caller needs to have the packet data in virtually-
+ * continuous storage.
+ *
+ * If need_skb is set to 0, the pointers to the packet's contents are simply
+ * stored in the sccmbx_bufx; the packet is not acknowledged, and no additional
+ * memory allocation is needed. The routine is guaranteed to return 1 (SUCCESS)
+ * in this case.
+ * This mode is useful when receiving packets must not fail due to low-memory
+ * situations, but the caller needs to be able to parse packets that may be
+ * broken into two fragments (in case they overlap the end of our ring buffer).
+ *
+ * If need_skb is set to anything else, an sk_buff is allocated, the packet's
+ * contents are copied, and the packet is acknowledged. If memory allocation
+ * fails, the routine returns 0, and the packet is not acknowledged.
+ *
+ * Irrespective of the value of need_skb, caller should always invoke
+ * sccmbx_retrieve_packet_body_cleanup when they are done with the packet.
+ *
+ * Please note that sccmbx_retrieve_packet_body[_cleanup] do not mark the packet
+ * as received at the sending node if allocating an sk_buff fails. If the caller
+ * wants to drop the packet completely without having the kernel try to re-
+ * deliver it, the caller needs to call sccmbx_clear_rx explicitly if the call
+ * to sccmbx_retrieve_packet_body fails.
+ */
+int sccmbx_retrieve_packet_body(struct sccmbx_rx* rx, struct sccmbx_bufx* bufx, int need_skb)
+{
+  int                         len = rx->len;
+  void*                       mpb_address = rx->mpb_address;
+  void*                       mpb_start = rx->mpb_start;
+  struct sccmb_priv*          priv = netdev_priv(rx->dev);
+  struct sk_buff*             skb = NULL;
+  int                         headroom;
+  int                         byteCount;
+  unsigned char*              local_mem;
+
+  /* Clear the returned sccmbx_bufx structure */
+  memset(bufx, 0, sizeof(struct sccmbx_bufx));
+
+  /* First calculate how much data we can fetch before the wrap-around
+   * occurs so we can determine how much to copy in the first run. Note
+   * that the headroom includes the header bytes as the entire packet
+   * is cacheline aligned!
+   */
+  headroom = (priv->mpb_buffersize - rx->offset) * SCC_CLINE_SIZE - SCCMBX_HLEN;
+  byteCount = len;
+  if (headroom < byteCount) {
+    byteCount = headroom;
+  }
+
+  /* Save pointers into the message buffer in our sccmbx_bufx structure */
+  bufx->len = len;
+  bufx->f[0].p = mpb_address;
+  bufx->f[0].len = byteCount;
+  if (byteCount < len) {
+    bufx->f[1].p = mpb_start;
+    bufx->f[1].len = len - byteCount;
+  }
+
+  /* If the caller does not need an skb, we are done now. */
+  if (!need_skb) {
+    return 1;
+  }
+
+  /* Build a skb for the packet data so upper layers can handle it
+   * Note that IP headers should be aligned on 16B boundaries!
+   */
+  skb = dev_alloc_skb(len);
+  if (!skb) {
+    return 0;
+  }
+
+  /* IP headers should be aligned on 16B boundaries, i.e. we just reserve the
+   * payload area and do not prepend the header bytes which are not used
+   * anymore.
+   */
+  local_mem = skb_put(skb, len);
+
+  /* Copy the packet data (without the header) into the buffer. Because 
+   * of the circular buffer implementation we may have to do it in 2 steps.
+   */
+  memcpy(skb->data, (void*)mpb_address, byteCount);
+
+#ifdef DBG_PACKET_TRACE
+/* Store the entire packet (including header bytes) also in the trace buffer */
+memcpy((void*)(priv->packetTrace + 1024*1024 + priv->rxTracePtr), (void*)(mpb_address-SCCMBX_HLEN), byteCount+SCCMBX_HLEN);
+priv->rxTracePtr += byteCount+SCCMBX_HLEN;
+if (priv->rxTracePtr > 1024*1024-2048) priv->rxTracePtr = 0;
+#endif
+  
+  /* Finish with the rest if we could not copy the entire packet */
+  if (byteCount < len) {
+    memcpy(skb->data+byteCount, mpb_start, len-byteCount);
+#ifdef DBG_PACKET_TRACE
+memcpy((void*)(priv->packetTrace + 1024*1024 + priv->rxTracePtr), mpb_start, len-byteCount);
+priv->rxTracePtr += len-byteCount;
+if (priv->rxTracePtr > 1024*1024-2048) priv->rxTracePtr = 0;
+#endif
+  }
+
+#ifdef DBG_PACKET_TRACE
+/* Align the rx trace pointer to cacheline boundaries */
+while (priv->rxTracePtr % SCC_CLINE_SIZE) priv->rxTracePtr++;
+#endif
+
+  /* Clear the descriptor as we no longer need the message buffer content */
+  sccmbx_clear_rx(rx);
+
+  /* Write metadata, and then pass to the receive level */
+  skb->dev        = rx->dev;
+  skb_set_transport_header(skb, 0);
+  skb_set_network_header(skb, 0);
+  skb_set_mac_header(skb, 0);
+  skb->protocol   = htons(ETH_P_IP);
+  skb->pkt_type   = PACKET_HOST;
+  skb->ip_summed  = CHECKSUM_UNNECESSARY;
+
+  bufx->skb = skb;
+
+  return 1;
+}
+EXPORT_SYMBOL(sccmbx_retrieve_packet_body)
+
+/* Mark the skb referred to by the sccmbx_bufx structure as consumed. After this
+ * call, sccmbx_retrieve_packet_body_cleanup will not free the skb.
+ */
+void sccmbx_retrieve_skb_consumed(struct sccmbx_bufx* bufx)
+{
+  bufx->skb = NULL;
+}
+EXPORT_SYMBOL(sccmbx_retrieve_skb_consumed)
+
+/* Cleanup packet state after sccmbx_retrieve_packet_body has successfully read
+ * a packet. This routine should be invoked with the same values for rx and bufx
+ * than sccmbx_retrieve_packet_body.
+ *
+ * Please note that sccmbx_retrieve_packet_body[_cleanup] do not mark the packet
+ * as received at the sending node if allocating an sk_buff fails. If the caller
+ * wants to drop the packet completely without having the kernel try to re-
+ * deliver it, the caller needs to call sccmbx_clear_rx explicitly if the call
+ * to sccmbx_retrieve_packet_body fails.
+ */
+void sccmbx_retrieve_packet_body_cleanup(struct sccmbx_rx* rx, struct sccmbx_bufx* bufx)
+{
+  /* If we did not use an sk_buff for this packet, the send descriptor has not
+   * been cleared yet because the message buffer content was still needed. In
+   * this case, clear the descriptor now. sccmbx_clear_rx makes sure not to
+   * clear more than once, so this call is safe even if an sk_buff was used. */
+  sccmbx_clear_rx(rx);
+
+  /* If there was an sk_buff allocated, free it now */
+  if (bufx->skb != NULL) {
+    dev_kfree_skb_any(bufx->skb);
+    bufx->skb = NULL;
+  }
+}
+EXPORT_SYMBOL(sccmbx_retrieve_packet_body_cleanup)
+
+/*
+ * Receive a packet for the NET subsystem
+ */
+static void sccmbx_rx_net(struct sccmbx_rx* rx, struct sccmbx_hdr* header, int len)
+{
+  struct sccmb_priv*          priv = netdev_priv(rx->dev);
+  struct sccmbx_bufx          bufx;
+
+  /* Check for reasonable sizes before processing the data to
+   * prevent nasty memory overflow errors.
+   */
+  if (len < sizeof(struct iphdr)) {
+    /* Simply drop the packet */
+    sccmbx_clear_rx(rx);
+    
+    printk(KERN_NOTICE "sccmb_rx(): illegal packet length %d => no IP hdr\n", len);
+    priv->stats.rx_dropped++;
+    return;
+  }
+
+  /* Get packet */
+  if (!sccmbx_retrieve_packet_body(rx, &bufx, 1)) {
+    if (printk_ratelimit() )
+      printk(KERN_NOTICE "sccmb rx: low on mem - packet dropped\n");
+
+    /* Note: since we do not clear the offset descriptor we do not trigger
+     * a retransmission and the packet will eventually be processed.
+     */
+    priv->stats.rx_dropped++;
+    return;
+  }
+
+  /* Update the interface statistics (also count the header bytes) */
+  priv->stats.rx_packets++;
+  priv->stats.rx_bytes += SCCMBX_HLEN+len;
+  rx->dev->last_rx = jiffies;
+
+  /* Deliver packet to upper layer */
+#ifdef SCCMB_NO_NAPI
+    netif_rx(bufx.skb);
+#else
+    netif_receive_skb(bufx.skb);
+#endif
+  sccmbx_retrieve_skb_consumed(&bufx);
+
+  /* Cleanup. This really is a NOP, as the data will always be copied into an
+   * sk_buff for the NET subsystem. */
+  sccmbx_retrieve_packet_body_cleanup(rx, &bufx);
+}
+
+static sccmbx_rx_proc sccmbx_rx_procs[256] = { 0 };
+
+static void __sccmbx_set_rx_proc(u8 subsys, sccmbx_rx_proc rx_proc)
+{
+	char subsys_name[16];
+	switch (subsys) {
+	case SCCMBX_SUBSYS_NET:
+		strcpy(subsys_name, "NET");
+		break;
+	case SCCMBX_SUBSYS_SHM:
+		strcpy(subsys_name, "SHM");
+		break;
+	default:
+		sprintf(subsys_name, "%d", subsys);
+		break;
+	}
+	printk(KERN_INFO "sccmbx: setting rx_proc for subsystem %s", subsys_name);
+	print_symbol(" to %s\n", (unsigned long)rx_proc);
+
+	sccmbx_rx_procs[subsys] = rx_proc;
+}
+
+/* Set receive callback for messages to a specific subsystem ID */
+int sccmbx_set_rx_proc(u8 subsys, sccmbx_rx_proc rx_proc)
+{
+	if (unlikely(subsys == SCCMBX_SUBSYS_NET)) {
+		return -EINVAL;
+	}
+
+	__sccmbx_set_rx_proc(subsys, rx_proc);
+	return 0;
+}
+EXPORT_SYMBOL(sccmbx_set_rx_proc)
+
+/*
+ * Receive a packet: retrieve, encapsulate and pass over to upper levels
+ */
+void sccmb_rx(struct net_device* dev, u8 sourceIp, u8 offset)
+{
+  struct sccmb_priv*          priv = netdev_priv(dev);
+  void*                       mpb_start;
+  void*                       mpb_address;
+  int                         len, subsys;
+
+  struct sccmbx_hdr*          header;
+  struct sccmbx_rx            rx;
+  sccmbx_rx_proc              rx_proc;
+
+  /* Get address of message buffer of sending node */
+  mpb_start = priv->mpb[sourceIp-1];
+
+  /* Calculate the address of the packet */
+  mpb_address  = mpb_start + offset*SCC_CLINE_SIZE;
+  
+  /* The packet length is stored in the first 2 bytes but does not include
+   * the header.
+   */
+  header = (struct sccmbx_hdr*)mpb_address;
+  len = 256*( header->len[0] )
+      +     ( header->len[1] );
+  subsys = header->subsys;
+  mpb_address += SCCMBX_HLEN;
+
+  /* Check for reasonable sizes before processing the data to
+   * prevent nasty memory overflow errors.
+   */
+  if (len > dev->mtu) {
+    /* Simply drop the packet */
+    sccmb_clear_descriptor(dev, sourceIp);
+    
+    printk(KERN_NOTICE "sccmb_rx(): illegal packet length %d => too long\n", len);
+    priv->stats.rx_dropped++;
+    return;
+  }
+  
+  /* We need to hand the packet over to the responsible subsystem now. Check
+   * that the subsystem ID is in the allowed range and that the receive function
+   * has been registered. */
+  if (unlikely((subsys < 0) || (subsys >= sizeof(sccmbx_rx_procs) / sizeof(sccmbx_rx_procs[0])))) {
+    sccmb_clear_descriptor(dev, sourceIp);
+
+    printk(KERN_NOTICE "sccmbx_rx: invalid target subsystem id %d.\n", subsys);
+    priv->stats.rx_dropped++;
+    return;
+  }
+
+  rx_proc = sccmbx_rx_procs[subsys];
+  if (unlikely(!rx_proc)) {
+    sccmb_clear_descriptor(dev, sourceIp);
+
+    printk(KERN_NOTICE "sccmbx_rx: subsystem id %d not registered.\n", subsys);
+    priv->stats.rx_dropped++;
+    return;
+  }
+
+  /* Build external receive descriptor and invoke the callback */
+  memset(&rx, 0, sizeof(rx));
+  rx.dev = dev;
+  rx.sourceIp = sourceIp;
+  rx.offset = offset;
+  rx.len = len;
+  rx.mpb_address = mpb_address;
+  rx.mpb_start = mpb_start;
+
+#ifdef CONFIG_SCCMBX_DUMP_NONNET_PACKETS
+  if (subsys != SCCMBX_SUBSYS_NET) {
+    printk(KERN_INFO "sccmbx_rx: received packet for subsystem %d, len=%x\n", subsys, len);
+  }
+#endif
+  rx_proc(&rx, header, len);
+}
+
+
+
+/*
+ * Check the receive descriptors and return the IP address if a valid
+ * packet offset is found. Note that a round robin scheme is used to
+ * ensure fairness.
+ */
+static int sccmb_nextRxPacket(struct net_device* dev, u8* offset)
+{
+  struct sccmb_priv*          priv = netdev_priv(dev);
+  static u8                   lastSource = 0;
+  u8                          source;
+  u8                          i;
+  u8                          nrOfDescriptors;
+  int                         desc_address;
+  
+  nrOfDescriptors = multiPacket ? 4*SCC_CORECOUNT : SCC_CORECOUNT;
+  
+  *offset = 0xFF;
+  for (i=0; i<nrOfDescriptors; i++) {
+    /* Calculate the descriptor address for the packet transfer from the
+     * given IP address to our node. Add 1 to the last found packet source
+     * so it gets processed with least priority.
+     */
+    source = (1+lastSource + i) % nrOfDescriptors;
+    desc_address = sccmb_get_desc_address(dev, 1+source, priv->localIp);
+    
+    /* Check for a valid receive descriptor */
+    *offset = *((u8*)desc_address);
+    if (*offset < 0xFF) {
+      /* Store the descriptor slot where we found the packet */
+      priv->mpb_rxDesc[source%SCC_CORECOUNT] = source/SCC_CORECOUNT;
+      
+      /* Remember the core number and return the IP address = 1+core number */
+      lastSource = source;
+      return 1 + source%SCC_CORECOUNT;
+    }
+  }
+  
+  /* Return 0 if no packet was found */
+  return 0;
+}
+
+
+/*
+ * The poll implementation processing all incoming packets
+ */
+static int sccmb_poll(struct napi_struct* napi, int to_do)
+{
+  struct sccmb_priv*          priv = container_of(napi, struct sccmb_priv, napi);
+  struct net_device*          dev = napi->dev;
+  int                         npackets = 0;
+  int                         quota = to_do;
+  u8                          sourceIp;
+  u8                          offset = 0;
+
+  /* Process up to quota packets from the receive queue */
+  while (npackets<quota) {
+    /* Flush the message buffer so we get up to date descriptor info */
+    CL1FLUSHMB;
+    sourceIp = sccmb_nextRxPacket(dev, &offset);
+    
+    /* Call the standard receive handler if there is a packet */
+    if (sourceIp) {
+      /* In case of level-triggered interrupts clear it now */
+      if (!edgeIrq) sccmb_clear_irq(dev, priv->localIp);
+      
+      sccmb_rx(dev, sourceIp, offset);
+      npackets++;
+    }
+    /* Else perform a clean exit */
+    else {
+
+      /* If interrupts are disabled we hae to remain in the polling function,
+       * i.e. we may signal that we are done with packet processing but must
+       * not remove us from the queueing list via netif_rx_complete().
+       */
+      if (noIrq) {
+        break;
+      }
+      
+      /* Tell the system we are done with polling */
+      napi_complete(&priv->napi);
+
+      /* Do not use enable_irq(dev->irq); since we might loose enables --> INT locked up */      
+      unset_lapic_mask(APIC_LVT0, dev->irq);
+
+      /* The interrupt was cleared before the Rx packet was processed,
+       * i.e. it will be re-set if another packet arrived since the last
+       * check. Thus the polling function will be scheduled again as
+       * soon as we enable interrupts again so we can safely return.
+       */
+      break;
+    }
+  }
+  
+  return npackets;
+}
+
+
+
+/*
+ * A NAPI interrupt handler since we can receive multiple packets 
+ * (from different sources) with a single interrupt.
+ */
+static irqreturn_t sccmb_interrupt(int irq, void* dev_id)
+{
+  struct net_device*          dev = (struct net_device*)dev_id;
+  struct sccmb_priv*          priv = netdev_priv(dev);
+
+#ifdef SCCMB_NO_NAPI
+  u8                          sourceIp = 0; 
+  u8                          offset = 0; 
+#endif
+
+  /* Paranoid */
+  if (!dev) {
+    printk(KERN_DEBUG "sccmb interrupt %d for unknown device\n", irq);
+    return IRQ_NONE;
+  }
+
+#ifdef DBG_INTERRUPT
+  priv->rxIntCount++;
+  
+  if (pingPongTarget > 0) {
+    sccmb_trigger_irq(dev, pingPongTarget);
+    return IRQ_HANDLED;
+  }
+#endif
+
+#ifdef SCCMB_NO_NAPI
+  CL1FLUSHMB;
+  sourceIp = sccmb_nextRxPacket(dev, &offset);
+  while (sourceIp) {
+    /* Call the standard receive handler if there is a packet */
+    sccmb_rx(dev, sourceIp, offset);
+    /* Flush the message buffer so we get up to date descriptor info */
+    CL1FLUSHMB;
+    sourceIp = sccmb_nextRxPacket(dev, &offset);
+  }
+  return IRQ_HANDLED;
+#else
+
+  /* Disable further interrupts and start the polling process. 
+   * We have to use the nosync version since we are inside the interrupt
+   * service routine!
+   * Do not use disable_irq_nosync(dev->irq); since we might loose enables --> INT locked up
+   */
+
+  set_lapic_mask(APIC_LVT0, dev->irq);
+
+  napi_schedule(&priv->napi);
+  return IRQ_HANDLED;
+#endif
+}
+
+
+
+
+/*
+ * Transmit a packet (low level interface)
+ * This function deals with HW details, i.e. it writes the packet
+ * into the message buffer and informs the destination.
+ */
+int sccmb_get_destination(struct net_device* dev, struct sccmbx_bufx* bufx, u8* destIp)
+{
+  struct iphdr*               ipHeader;
+  char                        packet_buf[SCCMBX_HLEN > sizeof(struct iphdr) ? SCCMBX_HLEN : sizeof(struct iphdr)];
+  u32*                        destAddr;
+  u8                          destCore;
+  u8                          netAddr[4];
+  int                         i;
+  struct sccmbx_hdr*          header;
+
+  *destIp = -1;
+
+  /* Verify that the packet contains at least our hardware header */
+  if (bufx->len < dev->hard_header_len) {
+    return 0;
+  }
+
+  /* Check whether we have an explicit destination */
+  header = (struct sccmbx_hdr*)packet_buf;
+  sccmbx_copy_from_bufx(header, bufx, 0, sizeof(struct sccmbx_hdr));
+  if (header->subsys != SCCMBX_SUBSYS_NET) {
+#ifdef CONFIG_SCCMBX_DUMP_NONNET_PACKETS
+    printk(KERN_INFO "sccmbx: transferring packet for subsystem %d, len = %x...\n", header->subsys, bufx->len);
+    sccutil_dump_buffer(bufx->f[0].p, bufx->f[0].len);
+    if (bufx->f[1].len > 0) {
+      sccutil_dump_buffer(bufx->f[1].p, bufx->f[1].len);
+    }
+#endif
+    destCore = header->dest;
+    if ((destCore < 1) || (destCore > 48)) {
+      printk(KERN_ERR "sccmbx: packet for subsystem %d specifies invalid destination %d\n",
+		header->subsys, destCore);
+      destCore = 1;
+    }
+    *destIp = destCore;
+    return 1;
+  }
+
+  /* This is a network packet, so we assume it contains an IP header */
+  if (bufx->len < dev->hard_header_len+sizeof(struct iphdr)) {
+    return 0;
+  }
+
+  /* Extract the destination address from the IP header.
+   * The last (4th) octet is interpreted as core ID.
+   */
+  ipHeader  = (struct iphdr*)packet_buf;
+  sccmbx_copy_from_bufx(ipHeader, bufx, dev->hard_header_len, sizeof(struct iphdr));
+  destAddr  = &(ipHeader->daddr);
+  destCore  = ((u8*)destAddr)[3];
+
+  /* Compare the network part to check if we have a local destination.
+   * Note that the module parameter is expected in A.B.C.D format to make
+   * it easier to modify so the values have to be reversed.
+   * In case a remote network is detected, the core number of the router
+   * is used as destination.
+   */
+  for (i=0; i<4; i++) netAddr[3-i] = (ownIpAddress >> 8*i) & 0xFF;
+  for (i=0; i<3; i++) {
+    if (((u8*)destAddr)[i] != netAddr[i]) destCore = routerIpAddress & 0xFF;
+  }
+  
+  /* Make sure that only valid local IP addresses are returned */
+  if ((destCore < 1) || (destCore > 48)) destCore = 1;
+  *destIp = destCore;
+
+  return 1;
+}
+
+static int sccmb_hw_tx(struct net_device* dev, struct sccmbx_bufx* bufx, u8 destIp)
+{
+  struct sccmb_priv*          priv = netdev_priv(dev);
+  u8                          clen;
+  int                         i;
+  int                         ptr;
+  int                         owner;
+  int                         slot;
+
+  /* Determine how many cache lines we need to store the message buffer data */
+  clen = (bufx->len / SCC_CLINE_SIZE);
+  if (bufx->len % SCC_CLINE_SIZE) clen++;
+  
+  /* Check whether there is enough space in the circular buffer */
+  for (i=0; i<clen; i++) {
+    ptr = (priv->mpb_next + i) % priv->mpb_buffersize;
+    owner = priv->mpb_map[ptr] & 0x3F;
+    slot  = priv->mpb_map[ptr] >> 6;
+    if (sccmb_tx_pending(dev, owner, slot) ) return 1;
+    
+    /* This slot will be used for the new packet transfer - also mark the
+     * the descriptor slot that is being used.
+     */
+    priv->mpb_map[ptr] = (priv->mpb_nextDesc[destIp-1] << 6) + destIp;
+  }
+  
+  /* Copy the data into the circular buffer */
+  ptr  = (int)(priv->mpb[priv->localIp-1]);
+  ptr += (priv->mpb_next * SCC_CLINE_SIZE);
+  if (clen + priv->mpb_next > priv->mpb_buffersize) {
+    /* First fill up the remaining space to the top of the buffer */
+    i = (priv->mpb_buffersize - priv->mpb_next) * SCC_CLINE_SIZE;
+    sccmbx_copy_from_bufx((void*)ptr, bufx, 0, i);
+    
+    /* Copy the remaining bytes to the start */
+    ptr  = (int)(priv->mpb[priv->localIp-1]);
+    sccmbx_copy_from_bufx((void*)ptr, bufx, i, bufx->len-i);
+  }
+  else {
+    /* Simply copy all the data since there is enough space without
+     * pointer roll-over.
+     */
+    sccmbx_copy_from_bufx((void*)ptr, bufx, 0, bufx->len);
+  }
+
+#ifdef DBG_PACKET_TRACE
+/* Store the packet in the trace buffer */
+sccmbx_copy_from_bufx((void*)(priv->packetTrace + priv->txTracePtr), bufx, 0, bufx->len);
+priv->txTracePtr += clen*SCC_CLINE_SIZE;
+if (priv->txTracePtr > 1024*1024-2048) priv->txTracePtr = 0;
+#endif    
+  /* Set the descriptor on the receiver side
+   * Since different addresses are used, this automatically flushes the
+   * data inside the write-combining buffer so the receiver sees up to
+   * date message buffer content.
+   */
+  sccmb_set_descriptor(dev, destIp, priv->mpb_next);
+  
+  /* Update the pointer to the next circular buffer slot and flag the
+   * pending transmission.
+   */
+  slot = priv->mpb_nextDesc[destIp-1];
+  priv->mpb_next = (priv->mpb_next + clen) % priv->mpb_buffersize;
+  priv->mpb_busy[slot*SCC_CORECOUNT + destIp-1] = jiffies;
+
+  /* Interrupt the receiver so it starts processing the packet */
+  sccmb_trigger_irq(dev, destIp);
+  
+  /* Increment the next descriptor slot for the next transmission in case
+   * multiple packets may be in flight.
+   */
+  if (multiPacket)
+    priv->mpb_nextDesc[destIp-1] = (priv->mpb_nextDesc[destIp-1] + 1) % 4;
+      
+  /* Transmission succeeded so save the timestamp of the transmission and
+   * update the statistics
+   */
+  dev->trans_start = jiffies;
+
+  priv->stats.tx_packets++;
+  priv->stats.tx_bytes += bufx->len;
+  
+  return 0;
+}
+
+
+
+static void sccmbx_init_header(struct sccmbx_hdrbuf* buf, u8 subsys, u8 dest,
+			       unsigned int len)
+{
+  struct sccmbx_hdr* header = (struct sccmbx_hdr*)buf;
+
+  memset(header, 0, SCCMBX_HLEN);
+
+  /* Store the length starting with the MSBs */
+  header->len[0] = len/256;
+  header->len[1] = len%256;
+
+  /* Store other information */
+  header->subsys = subsys;
+  header->dest = dest;
+  header->src = sccsys_get_pid();
+}
+
+/* Initialize struct bufx with data from struct sk_buff */
+static void sccmbx_init_transmit_bufx_from_skb(struct sccmbx_bufx* bufx, struct sk_buff* skb)
+{
+	memset(bufx, 0, sizeof(struct sccmbx_bufx));
+	bufx->len = skb->len;
+	bufx->f[0].p = skb->data;
+	bufx->f[0].len = skb->len;
+}
+
+/* Initialize struct bufx for transmitting a raw packet */
+void sccmbx_init_transmit_bufx(struct sccmbx_bufx* bufx, struct sccmbx_hdrbuf* hdr, u8 subsys, u8 dest, void* data, int len)
+{
+	memset(bufx, 0, sizeof(struct sccmbx_bufx));
+	bufx->len = SCCMBX_HLEN + len;
+	bufx->f[0].p = hdr;
+	bufx->f[0].len = SCCMBX_HLEN;
+	bufx->f[1].p = data;
+	bufx->f[1].len = len;
+	sccmbx_init_header(hdr, subsys, dest + 1, len);
+}
+EXPORT_SYMBOL(sccmbx_init_transmit_bufx);
+
+
+/* Transmit a packet described by a struct sccmbx_bufx */
+static int sccmb_tx_bufx(struct net_device* dev, struct sccmbx_bufx* bufx)
+{
+  struct sccmb_priv*          priv = netdev_priv(dev);
+  u8                          destIp;
+  u8                          slot = priv->mpb_nextDesc[destIp-1];
+  u8                          i;
+  
+  //printk(KERN_DEBUG "sccmb_tx_bufx(): start\n");
+
+  /* Perform a sanity check on the packet data, i.e. silently drop packets
+   * that are either too short or send to an illegal IP address by indicating
+   * success without executing any data transfer operations.
+   */
+  if (!sccmb_get_destination(dev, bufx, &destIp)) {
+    printk(KERN_NOTICE "sccmb_tx(): Illegal packet (%i octets to IP %d)\n", 
+                       bufx->len, destIp);
+    priv->stats.tx_errors++;
+    return 0;
+  }
+
+  /* Check if another transmission to the same destination is pending */
+  if (sccmb_tx_pending(dev, destIp, -1) ) {
+    /* If too much time elapsed since the previous transmission the receiver
+     * is probably dead and we drop the packet
+     */
+    if (jiffies > priv->mpb_busy[slot*SCC_CORECOUNT + destIp-1] + timeout) {
+      printk(KERN_NOTICE "sccmb_tx(): Timeout at destination %d\n", destIp);
+      
+      /* Also free all cache lines holding data for that destination */
+      for (i=0; i<priv->mpb_buffersize; i++) {
+        if ((priv->mpb_map[i]) == ((slot<<6)+destIp) )
+          priv->mpb_map[i] = 0;
+      }
+      
+      priv->stats.tx_errors++;
+      return 0;
+    }
+    /* Else signal the kernel that the transmission failed so it can 
+     * reschedule 
+     */
+    return 1;
+  }
+  
+  /* Now perform the low level data transfer and return the result. If it
+   * failed, e.g. because there is not enough space in the MPB, the kernel
+   * will retry transmission.
+   */
+  return sccmb_hw_tx(dev, bufx, destIp);
+}
+
+/*
+ * Transmit a packet (called by the kernel)
+ */
+int sccmb_tx(struct sk_buff* skb, struct net_device* dev)
+{
+  int                         res;
+  struct sccmbx_bufx          bufx;
+  
+  //printk(KERN_DEBUG "sccmb_tx(): start\n");
+  
+  /* Convert the socket buffer to a struct sccmbx_bufx */
+  sccmbx_init_transmit_bufx_from_skb(&bufx, skb);
+
+  /* Try to transfer packet */
+  res = sccmb_tx_bufx(dev, &bufx);
+  
+  /* Free socket buffer only if transmission was successful */
+  if (res == 0) {
+    dev_kfree_skb_any(skb);
+  }
+  return res;
+}
+
+/* Transmit a raw packet described by a struct bufx */
+int sccmbx_transmit_bufx(struct sccmbx_bufx* bufx)
+{
+  return -EINVAL;
+}
+EXPORT_SYMBOL(sccmbx_transmit_bufx);
+
+
+/*
+ * Deal with a transmit timeout.
+ */
+void sccmb_tx_timeout(struct net_device* dev)
+{
+  /* A timeout occurs if the queue was stopped because too many packet
+   * transfers were pending. Let's simply resume regular packet processing
+   * in the hope that the receiver have cleared their backlog.
+   */
+  //printk(KERN_DEBUG "sccmb_tx_timeout()\n");
+  
+  netif_wake_queue(dev);
+  return;
+}
+
+
+
+/*
+ * ioctl commands
+ */
+int sccmb_ioctl(struct net_device* dev, struct ifreq* rq, int cmd)
+{
+  /*
+   * Custom commands
+   */
+  return 0;
+}
+
+
+
+/*
+ * Return statistics to the caller
+ */
+struct net_device_stats* sccmb_stats(struct net_device* dev)
+{
+  struct sccmb_priv* priv = netdev_priv(dev);
+  
+  return &(priv->stats);
+}
+
+
+
+/*
+ * The "change_mtu" method is usually not needed.
+ * If you need it, it must be like this.
+ */
+int sccmb_change_mtu(struct net_device* dev, int new_mtu)
+{
+  struct sccmb_priv*          priv = netdev_priv(dev);
+  unsigned long               flags;
+  spinlock_t*                 lock = &(priv->lock);
+
+  /* check ranges */
+  if ((new_mtu < 68) || (new_mtu > 1500)) return -EINVAL;
+
+  /*
+   * Simply accept the value
+   */
+  spin_lock_irqsave(lock, flags);
+  dev->mtu = new_mtu;
+  spin_unlock_irqrestore(lock, flags);
+  
+  return 0;
+}
+
+
+
+/*
+ * This function is called to fill up an eth header, since ARP is not
+ * available on the interface.
+ */
+int sccmb_rebuild_header(struct sk_buff* skb)
+{
+  printk(KERN_WARNING "sccmb_rebuild_header() called - ignoring\n");
+
+  return 0;
+}
+
+static void sccmbx_create_header(struct sk_buff* skb, struct net_device* dev,
+				 u8 subsys, u8 dest, unsigned int len)
+{
+  struct sccmbx_hdr* header;
+
+  /* Prepend the message header */
+  header = (struct sccmbx_hdr*)skb_push(skb, SCCMBX_HLEN);
+  sccmbx_init_header((struct sccmbx_hdrbuf*)header, subsys, dest, len);
+}
+
+int sccmb_header(struct sk_buff* skb, struct net_device* dev,
+                 unsigned short type, const void* daddr, const void* saddr,
+                 unsigned int len)
+{
+  /* Currently, we do not support anything but IP */
+  if (type != ETH_P_IP) {
+    return -dev->hard_header_len;
+  }
+
+  /* Create header. We always use SCCMBX_SUBSYS_NET when going through the
+   * header_ops callback. */
+  sccmbx_create_header(skb, dev, SCCMBX_SUBSYS_NET, 0, len);
+      
+  return (dev->hard_header_len);
+}
+
+/* Allocate a packet */
+struct sk_buff* sccmbx_allocate_packet(u8 subsys, u8 dest, int len, void* *body)
+{
+	struct sk_buff* skb;
+	struct net_device* dev = sccmb_dev;
+
+	/* Verify parameters */
+	if (!dev || (subsys == SCCMBX_SUBSYS_NET) || (len < 0) || (len>dev->mtu)) {
+		return NULL;
+	}
+
+	/* Allocate packet */
+	skb = alloc_skb(len + LL_ALLOCATED_SPACE(dev) + 15, GFP_KERNEL);
+	if (!skb) {
+		return NULL;
+	}
+	skb_reserve(skb, LL_RESERVED_SPACE(dev));
+	*body = skb_put(skb, len);
+
+	/* Prepare header */
+	sccmbx_create_header(skb, dev, subsys, dest + 1, len);
+
+	skb->dev = dev;
+	skb->protocol = 0;
+
+	return skb;
+}
+EXPORT_SYMBOL(sccmbx_allocate_packet)
+
+static const struct net_device_ops sccmb_netdev_ops = {
+	.ndo_open		= sccmb_open,
+	.ndo_stop		= sccmb_close,
+	.ndo_set_config		= sccmb_config,
+	.ndo_start_xmit		= sccmb_tx,
+	.ndo_do_ioctl	        = sccmb_ioctl,
+	.ndo_get_stats		= sccmb_stats,
+	.ndo_change_mtu		= sccmb_change_mtu,
+	.ndo_tx_timeout		= sccmb_tx_timeout,
+};
+
+static const struct header_ops sccmb_header_ops = {
+	.create			= sccmb_header,
+	.rebuild		= sccmb_rebuild_header,
+};
+
+/*
+ * The init function (sometimes called probe).
+ * It is invoked by register_netdev()
+ */
+void sccmb_init(struct net_device* dev)
+{
+  struct sccmb_priv* priv;
+
+  /*
+   * Then initialize the priv field. This encloses the statistics
+   * and a few private fields.
+   */
+  priv = netdev_priv(dev);
+  memset(priv, 0, sizeof(struct sccmb_priv));
+  
+  spin_lock_init(&priv->lock);
+
+  /* Get meaningful default values */
+  ether_setup(dev);
+
+  /* Set the correct function pointers */
+  dev->netdev_ops = &sccmb_netdev_ops;
+  dev->header_ops = &sccmb_header_ops;
+  
+  dev->watchdog_timeo   = timeout;
+
+  /* Configure NAPI interrupt handling because we may receive multiple
+   * packets per interrupt. Note that Lehnix/MCEMU set LVT0's vector to 4.
+   */
+  netif_napi_add(dev, &priv->napi, sccmb_poll, 16);
+  dev->irq              = 4;
+
+  /* Keep the default flags; just add NOARP */
+  dev->flags           |= IFF_NOARP;
+  /* Checksum checks are not required */
+  // dev->features        |= NETIF_F_NO_CSUM;
+  /* Disable caching of (nonexistent) ARP replies */
+  //dev->hard_header_cache = NULL;
+  /* Change the hardware header as there is no need for an Ethernet format */
+  dev->hard_header_len    = SCCMBX_HLEN;
+  dev->addr_len           = 0;
+}
+
+
+
+/*
+ * Finally, the module stuff
+ */
+void sccmb_cleanup(void)
+{
+  if (sccmb_dev) {
+    unregister_netdev(sccmb_dev);
+    free_netdev(sccmb_dev);
+    sccmb_dev = NULL;
+  }
+}
+
+
+
+int sccmb_init_module(void)
+{
+  int result;
+
+  /* This driver does only work in a bare-metal environment */
+  if (!scc_bare_metal()) {
+    printk(KERN_INFO "sccmbx: startup in non-SCC or paravirtualized environment.\n");
+    return -EINVAL;
+  }
+
+  __sccmbx_set_rx_proc(SCCMBX_SUBSYS_NET, sccmbx_rx_net);
+
+  /* Allocate the devices */
+  sccmb_dev = alloc_netdev(sizeof(struct sccmb_priv), "mb0", sccmb_init);
+  if (!sccmb_dev) return -ENOMEM;
+
+  result = register_netdev(sccmb_dev);
+  if (result) {
+    printk(KERN_WARNING "sccmb: error %i registering device \"%s\"\n", 
+                        result, sccmb_dev->name);
+    free_netdev(sccmb_dev);
+    return -ENODEV;
+  }
+
+  return 0;
+}
+
+
+
+module_init(sccmb_init_module);
+module_exit(sccmb_cleanup);
diff -urN linux-3.1.4/drivers/net/sccmbx.h linux-3.1.4-scc/drivers/net/sccmbx.h
--- linux-3.1.4/drivers/net/sccmbx.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-3.1.4-scc/drivers/net/sccmbx.h	2011-12-20 15:27:07.618380472 +0100
@@ -0,0 +1,137 @@
+
+struct sccmbx_hdr {
+	unsigned char len[2];
+	unsigned char dest;
+	unsigned char src;
+	unsigned char subsys;
+} __attribute__((packed));
+
+struct sccmbx_rx;
+
+#define SCCMBX_SUBSYS_NET	0
+#define SCCMBX_SUBSYS_SHM	1
+
+#define SCCMBX_HLEN		16
+
+struct sccmbx_hdrbuf {
+	unsigned char __data[SCCMBX_HLEN];
+};
+
+struct sccmbx_fragment {
+	void* p;
+	int len;
+};
+
+#define SCCMBX_BUFX_FRAGMENTS	2
+struct sccmbx_bufx {
+	int len;
+	struct sk_buff* skb;
+	struct sccmbx_fragment f[SCCMBX_BUFX_FRAGMENTS];
+};
+
+/* Type of receive callback routines */
+typedef void (*sccmbx_rx_proc)(struct sccmbx_rx* rx, struct sccmbx_hdr* header, int len);
+
+
+/* Set receive callback for messages to a specific subsystem ID */
+extern int sccmbx_set_rx_proc(u8 subsys, sccmbx_rx_proc rx_proc);
+
+/* Clear receive descriptor */
+extern void sccmbx_clear_rx(struct sccmbx_rx* rx);
+
+/* Get pid of source node of message */
+extern int sccmbx_get_rx_source(struct sccmbx_rx* rx);
+
+/* Get pointer(s) to body of received packet. This routine can optionally
+ * allocate an sk_buff if the caller needs to have the packet data in virtually-
+ * continuous storage.
+ *
+ * If need_skb is set to 0, the pointers to the packet's contents are simply
+ * stored in the sccmbx_bufx; the packet is not acknowledged, and no additional
+ * memory allocation is needed. The routine is guaranteed to return 1 (SUCCESS)
+ * in this case.
+ * This mode is useful when receiving packets must not fail due to low-memory
+ * situations, but the caller needs to be able to parse packets that may be
+ * broken into two fragments (in case they overlap the end of our ring buffer).
+ *
+ * If need_skb is set to anything else, an sk_buff is allocated, the packet's
+ * contents are copied, and the packet is acknowledged. If memory allocation
+ * fails, the routine returns 0, and the packet is not acknowledged.
+ *
+ * Irrespective of the value of need_skb, caller should always invoke
+ * sccmbx_retrieve_packet_body_cleanup when they are done with the packet.
+ *
+ * Please note that sccmbx_retrieve_packet_body[_cleanup] do not mark the packet
+ * as received at the sending node if allocating an sk_buff fails. If the caller
+ * wants to drop the packet completely without having the kernel try to re-
+ * deliver it, the caller needs to call sccmbx_clear_rx explicitly if the call
+ * to sccmbx_retrieve_packet_body fails.
+ */
+extern int sccmbx_retrieve_packet_body(struct sccmbx_rx* rx, struct sccmbx_bufx* bufx, int need_skb);
+
+/* Mark the skb referred to by the sccmbx_bufx structure as consumed. After this
+ * call, sccmbx_retrieve_packet_body_cleanup will not free the skb.
+ */
+extern void sccmbx_retrieve_skb_consumed(struct sccmbx_bufx* bufx);
+
+/* Cleanup packet state after sccmbx_retrieve_packet_body has successfully read
+ * a packet. This routine should be invoked with the same values for rx and bufx
+ * than sccmbx_retrieve_packet_body.
+ *
+ * Please note that sccmbx_retrieve_packet_body[_cleanup] do not mark the packet
+ * as received at the sending node if allocating an sk_buff fails. If the caller
+ * wants to drop the packet completely without having the kernel try to re-
+ * deliver it, the caller needs to call sccmbx_clear_rx explicitly if the call
+ * to sccmbx_retrieve_packet_body fails.
+ */
+extern void sccmbx_retrieve_packet_body_cleanup(struct sccmbx_rx* rx, struct sccmbx_bufx* bufx);
+
+/* Copy data from struct sccmbx_bufx to regular buffer */
+static inline int sccmbx_copy_from_bufx(void* target, struct sccmbx_bufx* bufx, int offset, int len)
+{
+  int clen = 0, i;
+
+  if (offset < 0 || len < 0 || offset > bufx->len) {
+    VM_BUG_ON(1);
+    return -EINVAL;
+  }
+
+  for (i = 0; (i < SCCMBX_BUFX_FRAGMENTS) && (len > 0); i++) {
+    /* Length of current fragment. If the requested subrange is not contained
+     * in it, simply adjust the offset value for the next fragment and continue.
+     */
+    int flen = bufx->f[i].len;
+    if (offset >= flen) {
+      offset -= flen;
+      continue;
+    }
+
+    /* Adjust flen to reflect the maximum number of bytes that can be copied
+     * from the current fragment. */
+    flen -= offset;
+
+    if (flen > len) {
+      flen = len;
+    }
+
+    /* Copy data, then adjust variables for the next fragment */
+    memcpy(target, bufx->f[i].p + offset, flen);
+    offset = 0;
+    target += flen;
+    clen += flen;
+    len -= flen;
+  }
+
+  VM_BUG_ON(clen < len);
+  return clen;
+}
+
+/* Allocate a packet */
+extern struct sk_buff* sccmbx_allocate_packet(u8 subsys, u8 dest, int len, void* *body);
+
+/* Initialize struct bufx for transmitting a packet */
+extern void sccmbx_init_transmit_bufx(struct sccmbx_bufx* bufx, struct sccmbx_hdrbuf* hdr, u8 subsys, u8 dest, void* data, int len);
+
+/* Transmit a raw packet described by a struct bufx */
+extern int sccmbx_transmit_bufx(struct sccmbx_bufx* bufx);
+
diff -urN linux-3.1.4/drivers/net/sccpc.c linux-3.1.4-scc/drivers/net/sccpc.c
--- linux-3.1.4/drivers/net/sccpc.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-3.1.4-scc/drivers/net/sccpc.c	2011-12-20 15:27:07.618380472 +0100
@@ -0,0 +1,792 @@
+/*
+ * sccpc.c -- SCC PC network driver
+ *
+ * Portions Copyright (C) 2009 Intel Corp.
+ *
+ * The code is based on snull.c from the book "Linux Device
+ * Drivers" by Alessandro Rubini and Jonathan Corbet, published
+ * by O'Reilly & Associates.
+ */
+
+/* Notes:
+ * - This driver assumes that the SCC system memory driver is also
+ *   loaded as it performs the appropriate initialisation
+ */
+
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/moduleparam.h>
+
+#include <linux/sched.h>
+#include <linux/kernel.h>       /* printk() */
+#include <linux/slab.h>         /* kmalloc() */
+#include <linux/errno.h>        /* error codes */
+#include <linux/types.h>        /* size_t */
+#include <linux/interrupt.h>    /* mark_bh */
+
+#include <linux/in.h>
+#include <linux/netdevice.h>    /* struct device, and other headers */
+#include <linux/etherdevice.h>  /* eth_type_trans */
+#include <linux/ip.h>           /* struct iphdr */
+#include <linux/tcp.h>          /* struct tcphdr */
+#include <linux/skbuff.h>
+
+#include <linux/sccsys.h>
+
+#include <linux/in6.h>
+#include <asm/checksum.h>
+
+#include <asm/io.h>             /* ioremap() & friends */
+#include <asm/pgtable.h>        /* page protection bits */
+#include <asm/apic.h>           /* apic_read() & apic_write() */
+#include <asm/lapic.h>          /* (un)set_lapic_mask */
+
+MODULE_AUTHOR("Werner Haas");
+MODULE_LICENSE("GPL");
+
+/* DEBUG messages */
+#define DEBUG_MSG 0
+#define PRINTD(format, args...) if (DEBUG_MSG) { printk(format, ##args); }
+
+
+/* SCC specific constants
+ * Note that values that can be changed through LUT configuration are made
+ * available as module parameters so it is not necessary to recompile the
+ * driver if a non-default setup is used.
+ */
+/* Maximum size of data transfers */
+#define MAX_PACKET_SIZE     4096
+/* Number of bytes in a cache line */
+#define SCC_CLINE_SIZE      32
+/* Size of a SCC memory tile (16MB) */
+#define SCC_TILE_SIZE       0x01000000
+
+/* Interrupt configuration (register+mask bit) */
+#define SCCPC_LVT           APIC_LVT1
+#define SCCPC_IRQ_MASK      0x00000001
+
+/* The new instruction to flush message buffer content from L1 */
+#define CL1FLUSHMB __asm__ volatile ( ".byte 0x0f; .byte 0x0a;\n" )
+
+/* Register offsets of the host mailbox registers
+ * Note that we hae to use different cachelines to use the write
+ * combining buffer (WCB) efficiently.
+ */
+#define MBX_NULL            0x00 /* Dummy address to flush WCB */
+#define MBX_CONFIG          0x20
+#define MBX_PACKETSTART     0x40
+#define MBX_PACKETDATA      0x60
+#define MBX_RXDONE          0x80
+
+
+
+/*
+ * Module parameters
+ */
+/* Make the number of packets in flight configurable. Note that an unsigend
+ * variable is used to store the status at the host, i.e. the value should
+ * be <= 32!
+ */
+static int rxPacketSlots = 16;
+module_param(rxPacketSlots, int, 0644);
+MODULE_PARM_DESC(rxPacketSlots, "Number of packet slots in DDR3 memory");
+
+/* Start address of the network buffer in the shared memory space. It is
+ * assumed that 4 consecutive memory tiles are resered for the 4 quadrants
+ * and that each core uses txPacketSlots*MAX_PACKET_SIZE bytes.
+ * In the default memory layout the shared region starts at 0x8000.0000 but
+ * we hae to leave space for the shared memory TTY.
+ */
+static int rxBaseAddress = 0x80200000;
+module_param(rxBaseAddress, int, 0644);
+MODULE_PARM_DESC(rxBaseAddress, "Start address of the packet space at the MC");
+
+/* Address of the Tx mailbox. This address is mapped to the host instead
+ * of DDR3 in the LUTs.
+ */
+static int txMailbox = 0xFA000000;
+module_param(txMailbox, int, 0644);
+MODULE_PARM_DESC(txMailbox, "Start address of the Tx mailbox at the host");
+
+/* We need the local CRB registers so we can determine our core ID and clear
+ * the interrupt.
+ */
+static int local_crb_offset = 0xF8000000;
+module_param(local_crb_offset, int, 0644);
+MODULE_PARM_DESC(local_crb_offset, "Start address of the local register bank");
+
+/* When interrupts are disabled the NAPI poll function is added to the queue
+ * at startup never returns 0.
+ */
+static int noIrq = 0;
+module_param(noIrq, int, 0644);
+MODULE_PARM_DESC(noIrq, "Do not use interrupts to trigger receiver");
+
+
+
+/*
+ * The SCC message buffer device
+ */
+struct net_device* sccpc_dev;
+
+struct sccpc_priv {
+  struct net_device_stats     stats;
+  struct napi_struct          napi;
+  spinlock_t                  lock;
+  u8                          shutdown;
+
+  /* Device-specific constants: */
+  u8                          currentSlot;
+  void*                       irqAddress;
+  /* SCC specific physical memory addresses are mapped into
+   * kernel space:  Core Register Bank
+   *                Reiceive Buffer
+   *                Host mailbox
+   */
+  void*                       crb;
+  void*                       rxb;
+  void*                       mailbox;
+};
+
+
+
+/*
+ * Open and close
+ * These functions are called when an interface is activated/stopped. Thus,
+ * any system resources should be registered and the device itself should
+ * be initialized.
+ */
+static irqreturn_t sccpc_interrupt(int, void*);
+
+void sccpc_unmap(struct net_device* dev)
+{
+  struct sccpc_priv*          priv = netdev_priv(dev);
+
+
+  iounmap(priv->crb);
+  iounmap(priv->rxb);
+  iounmap(priv->mailbox);
+}
+
+int sccpc_open(struct net_device* dev)
+{
+  struct sccpc_priv*          priv = netdev_priv(dev);
+  unsigned                    address;
+  int                         tmp;
+  int                         status = 0;
+  int                         x, y, z;
+  int                         quadrant = 0;
+  int                         position = 0;
+
+  /* First map the local CRB register space so we can determine our ID */
+  priv->crb = ioremap_nocache(local_crb_offset, PAGE_SIZE);
+  if (!priv->crb) {
+    printk(KERN_WARNING "sccpc_open: Failed to map register bank\n");
+    return -EIO;
+  }
+
+  /* Determine our location from the tile ID register */
+  tmp = readl((void*)(priv->crb + SCC_TILEID));
+  x = (tmp>>3) & 0x0f; /* bits 06:03 */
+  y = (tmp>>7) & 0x0f; /* bits 10:07 */
+  z = (tmp   ) & 0x07; /* bits 02:00 */
+
+  /* Set the interrupt register address accordingly */
+  if (z == 0) priv->irqAddress = priv->crb + SCC_GLCFG0;
+  else        priv->irqAddress = priv->crb + SCC_GLCFG1;
+
+
+  /* Calculate the quadrant and the location within. Note that rows 1/3
+   * correspond to positions 6..12!
+   */
+  position = 6*y + 2*x + z;
+  if (x > 2) {
+    quadrant += 1;
+    position -= 6;
+  }
+  if (y > 1) {
+    quadrant += 2;
+    position -= 12;
+  }
+  /* Map space for rxPacketSlots and mark it as message passing buffer so it
+   * can be easily invalidated. The write-through flag is also set so every
+   * write is immediately propagated to main memory - note that the Rx buffer
+   * is mostly read-only.
+   */
+  address = PAGE_ALIGN(rxBaseAddress
+                       + quadrant*SCC_TILE_SIZE
+                       + position*rxPacketSlots*MAX_PACKET_SIZE);
+  priv->rxb = ioremap_mpbt(address, rxPacketSlots*MAX_PACKET_SIZE);
+  if (!priv->rxb) {
+    printk(KERN_WARNING "sccpc_open: Failed to map receive buffer\n");
+    sccpc_unmap(dev);
+    return -EIO;
+  }
+  PRINTD(KERN_INFO "sccpc: core %i in quadrant %i (0x%08X)\n",
+                   position, quadrant, address);
+
+  /* Finally map the host mailbox */
+  priv->mailbox = ioremap_mpbt(txMailbox, PAGE_SIZE);
+  if (!priv->mailbox) {
+    printk(KERN_WARNING "sccpc_open: Failed to map host mailbox\n");
+    sccpc_unmap(dev);
+    return -EIO;
+  }
+  PRINTD(KERN_INFO "sccpc: rxb = %p, mailbox = %p\n", priv->rxb, priv->mailbox);
+
+  /* Configure interrupt handling */
+  status = request_irq(dev->irq, &sccpc_interrupt, IRQF_SHARED, "sccpc", dev);
+  if (status) {
+    printk(KERN_WARNING "Can't get interrupt #%d\n", dev->irq);
+    sccpc_unmap(dev);
+    return status;
+  }
+
+  /* Initialize the receive slots */
+  for (position=0; position<rxPacketSlots; position++) {
+    *((unsigned*)(priv->rxb + position*MAX_PACKET_SIZE)) = 0;
+  }
+  /* Tell the peer side we will start receiving at slot 0 and flush the
+   * write-combining buffer so all data gets out to its destination
+   */
+  priv->currentSlot = 0;
+  *((unsigned*)(priv->mailbox+MBX_CONFIG))  = 0;
+  *((unsigned*)(priv->mailbox+MBX_NULL))    = 0;
+
+
+  /* Assign the hardware address of the board (6 octets for Ethernet)
+   * Note that the first octet of ethernet multicast addresses is odd
+   */
+  memcpy(dev->dev_addr, "\0HOST0", ETH_ALEN);
+
+  netif_start_queue(dev);
+  napi_enable(&priv->napi);
+
+  PRINTD(KERN_NOTICE "sccpc_init: napi = %p, priv = %p, dev = %p\n", &priv->napi, priv, dev);
+
+  /* If interrupts are not used we immediately add the polling function
+   * to the queue which would otherwise be done through the IRQ handler.
+   */
+  if (noIrq) napi_schedule(&priv->napi);
+
+  return 0;
+}
+
+
+int sccpc_close(struct net_device* dev)
+{
+  struct sccpc_priv*          priv = netdev_priv(dev);
+  napi_disable(&priv->napi);
+
+  /* Unmap/release the SCC resources */
+  free_irq(dev->irq, dev);
+  sccpc_unmap(dev);
+
+  netif_stop_queue(dev);
+  return 0;
+}
+
+
+
+/*
+ * Configuration changes (passed on by ifconfig)
+ */
+int sccpc_config(struct net_device* dev, struct ifmap* map)
+{
+  /* Can't act on a running interface */
+  if (dev->flags & IFF_UP) return -EBUSY;
+
+  /* Don't allow changing the I/O address */
+  if (map->base_addr != dev->base_addr) {
+    printk(KERN_WARNING "sccpc: Can't change I/O address\n");
+    return -EOPNOTSUPP;
+  }
+
+  /* Allow changing the IRQ */
+  if (map->irq != dev->irq) {
+    dev->irq = map->irq;
+    /* request_irq() is delayed to open-time */
+  }
+
+  /* ignore other fields */
+  return 0;
+}
+
+
+
+/*
+ * Receive a packet: retrieve, encapsulate and pass over to upper levels
+ */
+void sccpc_rx(struct net_device* dev, u8 slot, unsigned len)
+{
+  struct sccpc_priv*          priv = netdev_priv(dev);
+  void*                       address = priv->rxb + slot*MAX_PACKET_SIZE;
+  struct sk_buff*             skb;
+
+#ifdef CONFIG_SCCPC_DUMP_PACKETS
+  printk(KERN_NOTICE "sccpc_rx(): slot = %d, len = %u\n", slot, len);
+#endif
+
+  if (len < sizeof(struct iphdr) || len > dev->mtu) {
+    /* Simply drop the packet */
+    printk(KERN_NOTICE "sccpc_rx(): illegal packet length %d => drop\n", len);
+    priv->stats.rx_errors++;
+    priv->stats.rx_dropped++;
+
+    goto rxDone;
+  }
+
+  /* Build a skb for the packet data so upper layers can handle it
+   * Note that IP headers should be aligned on 16B boundaries, i.e. skip
+   * the first 2 header bytes.
+   */
+  skb = dev_alloc_skb(len);
+  if (!skb) {
+    if (printk_ratelimit() )
+      printk(KERN_NOTICE "sccpc rx: low on mem - packet dropped\n");
+
+    /* Note: since we do not clear the offset descriptor we do not trigger
+     * a retransmission and the packet will eventually be processed.
+     */
+    priv->stats.rx_dropped++;
+    return;
+  }
+
+  /* IP headers should be aligned on 16B boundaries, i.e. we just reserve the
+   * payload area and do not prepend the header bytes which are not used
+   * anymore.
+   */
+  skb_put(skb, len);
+  memcpy(skb->data, address+2, len);
+
+#ifdef CONFIG_SCCPC_DUMP_PACKETS
+  printk(KERN_NOTICE "sccpc: received packet, len = %u\n", len);
+  sccutil_dump_buffer(skb->data, len);
+#endif
+
+  /* Update the interface statistics (also count the header bytes) */
+  priv->stats.rx_packets++;
+  priv->stats.rx_bytes += 2+len;
+  dev->last_rx = jiffies;
+
+  /* Write metadata, and then pass to the receive level */
+  skb->dev        = dev;
+  skb_set_transport_header(skb, 0);
+  skb_set_network_header(skb, 0);
+  skb_set_mac_header(skb, 0);
+  skb->protocol   = htons(ETH_P_IP);
+  skb->pkt_type   = PACKET_HOST;
+  skb->ip_summed  = CHECKSUM_UNNECESSARY;
+
+  netif_rx(skb);
+
+rxDone:
+  /* Invalidate the packet length so we do not process the data again */
+  *((unsigned*)(priv->rxb + slot*MAX_PACKET_SIZE)) = 0;
+
+  /* Tell the host we are done with the slot and move on to the next */
+  *((unsigned*)(priv->mailbox+MBX_RXDONE)) = slot;
+  *((unsigned*)(priv->mailbox+MBX_NULL)) = 0;
+  priv->currentSlot = (priv->currentSlot + 1) % rxPacketSlots;
+
+  return;
+}
+
+
+
+/*
+ * The poll function looks for valid packets in the receive buffer
+ */
+static int sccpc_poll(struct napi_struct* napi, int to_do)
+{
+  struct sccpc_priv*          priv = container_of(napi, struct sccpc_priv, napi);
+  struct net_device*          dev = napi->dev;
+  int                         npackets = 0;
+  int                         quota = to_do;
+  void*                       address;
+  unsigned                    len;
+  unsigned                    tmp;
+
+#ifdef CONFIG_SCCPC_DUMP_PACKETS
+  printk(KERN_NOTICE "sccpc_poll: entry. napi = %p, priv = %p, dev = %p, quota = %d\n", napi, priv, dev, quota);
+#endif
+
+  /* Process up to quota packets from the receive queue */
+  while (npackets<quota) {
+    /* Quit polling when told to do so */
+    if (priv->shutdown) {
+      napi_complete(napi);
+      return 0;
+    }
+    
+    
+    /* Flush the message buffer so we get up to date message buffer data */
+    CL1FLUSHMB;
+
+    /* Check for valid packet lengths */
+    address = priv->rxb + priv->currentSlot*MAX_PACKET_SIZE;
+    len = 256*( *((u8*)(address  )) )
+        +     ( *((u8*)(address+1)) );
+
+#ifdef CONFIG_SCCPC_DUMP_PACKETS
+    printk(KERN_NOTICE "sccpc_poll: currentSlot = %d, len = %u\n", priv->currentSlot, len);
+#endif
+
+    /* Call the standard receive handler if there is a packet */
+    if (len) {
+      /* Clear the interrupt bit */
+      tmp  = readl((void*)priv->irqAddress);
+      tmp &= (~SCCPC_IRQ_MASK);
+      writel(tmp, (void*)priv->irqAddress);
+
+      /* Fetch the packet data */
+      sccpc_rx(dev, priv->currentSlot, len);
+      npackets++;
+    }
+    /* Else perform a clean exit */
+    else {
+      unsigned long v;
+
+      /* If interrupts are disabled we have to remain in the polling function,
+       * i.e. we may signal that we are done with packet processing but must
+       * not remove us from the queueing list via netif_rx_complete().
+       */
+      if (noIrq) {
+        break;
+      }
+
+      /* Tell the system we are done with polling */
+      napi_complete(napi);
+
+      unset_lapic_mask(SCCPC_LVT, dev->irq);
+
+      /* The interrupt was cleared before the Rx packet was processed,
+       * i.e. it will be re-set if another packet arrived since the last
+       * check. Thus the polling function will be scheduled again as
+       * soon as we enable interrupts again so we can safely return.
+       */
+      break;
+    }
+  }
+
+  return npackets;
+}
+
+
+
+/*
+ * A NAPI interrupt handler since we can receive multiple packets
+ * with a single interrupt.
+ */
+static irqreturn_t sccpc_interrupt(int irq, void* dev_id)
+{
+  struct net_device*          dev = (struct net_device*)dev_id;
+  struct sccpc_priv*          priv = netdev_priv(dev);
+
+  unsigned long               v;
+  /* Paranoid */
+  if (!priv) {
+    printk(KERN_DEBUG "sccpc interrupt %d for unknown device\n", irq);
+    return IRQ_NONE;
+  }
+
+#ifdef CONFIG_SCCPC_DUMP_PACKETS
+  printk("sccpc_interrupt: irq = %d, napi = %p, priv = %p, dev = %p\n", irq, &priv->napi, priv, dev);
+#endif
+
+  /* Mask further interrupts and start the polling process */
+  set_lapic_mask(SCCPC_LVT, dev->irq);
+
+  napi_schedule(&priv->napi);
+  
+  return IRQ_HANDLED;
+}
+
+
+
+/*
+ * Transmit a packet (called by the kernel)
+ */
+int sccpc_tx(struct sk_buff* skb, struct net_device* dev)
+{
+  struct sccpc_priv*          priv = netdev_priv(dev);
+  unsigned                    bytesToCopy;
+  unsigned                    bytesTransferred  = 0;
+  unsigned                    bytesRemaining    = 0;
+  unsigned long               flags;
+  spinlock_t*                 lock = &(priv->lock);
+
+  spin_lock_irqsave(lock, flags);
+
+#ifdef CONFIG_SCCPC_DUMP_PACKETS
+  printk(KERN_NOTICE "sccpc_tx: sending packet. len = %u\n", skb->len);
+  sccpc_dump_buffer(skb->data, skb->len);
+#endif
+
+  /* Send the packet data to the host. Note that the first cacheline
+   * has its own mailbox address.
+   */
+  bytesToCopy = (skb->len > SCC_CLINE_SIZE) ? SCC_CLINE_SIZE : skb->len;
+  memcpy(priv->mailbox+MBX_PACKETSTART, skb->data, bytesToCopy);
+  bytesTransferred += bytesToCopy;
+  
+  /* Copy all remaining data to the PACKETDATA mailbox address */
+  while (bytesTransferred < skb->len) {
+    bytesRemaining = skb->len - bytesTransferred;
+    bytesToCopy = (bytesRemaining > SCC_CLINE_SIZE) ? SCC_CLINE_SIZE 
+                                                    : bytesRemaining;
+    
+    memcpy(priv->mailbox+MBX_PACKETDATA, 
+           skb->data+bytesTransferred, bytesToCopy);
+    bytesTransferred += bytesToCopy;
+  }
+  /* If the last memcpy did not fill the entire cache line we have to flush
+   * the write combining buffer.
+   */
+  if (bytesToCopy < SCC_CLINE_SIZE) {
+    /* Write to a different address if the partial data fits into a single
+     * packet.
+     */
+    if (bytesToCopy <= 8) *((unsigned*)(priv->mailbox+MBX_NULL)) = 0;
+    /* Else fill up the cacheline with dummy data */
+    else memcpy(priv->mailbox+MBX_PACKETDATA+bytesToCopy, skb->data, 
+                SCC_CLINE_SIZE-bytesToCopy);
+  }
+  spin_unlock_irqrestore(lock, flags);
+
+
+  /* Transmission always succeeds so save the timestamp of the transmission,
+   * update the statistics and free the socket buffer
+   */
+  dev->trans_start = jiffies;
+
+  priv->stats.tx_packets++;
+  priv->stats.tx_bytes += skb->len;
+  dev_kfree_skb_any(skb);
+
+  return NETDEV_TX_OK;
+}
+
+
+
+/*
+ * Deal with a transmit timeout.
+ */
+void sccpc_tx_timeout(struct net_device* dev)
+{
+  /* A timeout should never occur as we send all data to the host. Wake the
+   * queue in case it was stopped by someone.
+   */
+  //printk(KERN_NOTICE "sccpc_tx_timeout()\n");
+
+  netif_wake_queue(dev);
+  return;
+}
+
+
+
+/*
+ * ioctl commands
+ */
+int sccpc_ioctl(struct net_device* dev, struct ifreq* rq, int cmd)
+{
+  /*
+   * Custom commands
+   */
+  return 0;
+}
+
+
+
+/*
+ * Return statistics to the caller
+ */
+struct net_device_stats* sccpc_stats(struct net_device* dev)
+{
+  struct sccpc_priv* priv = netdev_priv(dev);
+
+  return &(priv->stats);
+}
+
+
+
+/*
+ * The "change_mtu" method is usually not needed.
+ */
+int sccpc_change_mtu(struct net_device* dev, int new_mtu)
+{
+  struct sccpc_priv*          priv = netdev_priv(dev);
+  unsigned long               flags;
+  spinlock_t*                 lock = &(priv->lock);
+
+  /* Check ranges, i.e. make sure that we send at least the IP header and
+   * do not cross the maximum packet size (note the 2 header bytes).
+   */
+  if ((new_mtu < sizeof(struct iphdr)) || (new_mtu > MAX_PACKET_SIZE-2))
+    return -EINVAL;
+
+  /*
+   * Simply accept the value
+   */
+  spin_lock_irqsave(lock, flags);
+  dev->mtu = new_mtu;
+  spin_unlock_irqrestore(lock, flags);
+
+  return 0;
+}
+
+
+
+/*
+ * This function is called to fill up an eth header, since ARP is not
+ * available on the interface.
+ */
+int sccpc_rebuild_header(struct sk_buff* skb)
+{
+  printk(KERN_WARNING "sccpc_rebuild_header() called - ignoring\n");
+
+  return 0;
+}
+
+int sccpc_header(struct sk_buff* skb, struct net_device* dev,
+                 unsigned short type, const void* daddr, const void* saddr,
+                 unsigned len)
+{
+  /* Prepend 2 header bytes containing the packet length */
+  u8* header = skb_push(skb, 2);
+
+  /* Store the length, starting with the MSBs */
+  header[0] = len/256;
+  header[1] = len%256;
+
+  return (dev->hard_header_len);
+}
+
+
+static const struct net_device_ops sccpc_netdev_ops = {
+	.ndo_open		= sccpc_open,
+	.ndo_stop		= sccpc_close,
+	.ndo_set_config		= sccpc_config,
+	.ndo_start_xmit		= sccpc_tx,
+	.ndo_do_ioctl		= sccpc_ioctl,
+	.ndo_get_stats		= sccpc_stats,
+	.ndo_change_mtu		= sccpc_change_mtu,
+	.ndo_tx_timeout		= sccpc_tx_timeout,
+};
+
+static const struct header_ops sccpc_header_ops = {
+	.create			= sccpc_header,
+	.rebuild		= sccpc_rebuild_header,
+};
+
+/*
+ * The init function (sometimes called probe).
+ * It is invoked by register_netdev()
+ */
+void sccpc_init(struct net_device* dev)
+{
+  struct sccpc_priv* priv;
+
+  /*
+   * Then initialize the priv field. This encloses the statistics
+   * and a few private fields.
+   */
+  priv = netdev_priv(dev);
+  memset(priv, 0, sizeof(struct sccpc_priv));
+
+  spin_lock_init(&priv->lock);
+
+  /* Get meaningful default values */
+  ether_setup(dev);
+
+  /* Set the correct function pointers */
+  dev->netdev_ops = &sccpc_netdev_ops;
+  dev->header_ops = &sccpc_header_ops;
+
+  dev->watchdog_timeo     = 10;
+
+  /* Configure NAPI interrupt handling because we may receive multiple
+   * packets per interrupt. Note that Lehnix/MCEMU set LVT1's vector to 3.
+   */
+  netif_napi_add(dev, &priv->napi, sccpc_poll, 16);
+  dev->irq                = 3;
+
+  /* Only set the NOARP flag as there is no broad-/multicast support */
+  dev->flags              = IFF_NOARP;
+  /* Disable caching of (nonexistent) ARP replies */
+  //dev->hard_header_cache  = NULL;
+  /* Change the hardware header as there is no need for an Ethernet format */
+  dev->hard_header_len    = 2;
+  dev->addr_len           = 0;
+}
+
+
+
+/*
+ * Finally, the module stuff
+ */
+int sccpc_dev_event_handler(struct notifier_block* self, 
+                            unsigned long event, void* data)
+{
+  struct net_device* dev  = (struct net_device*)data;
+  struct sccpc_priv* priv = netdev_priv(dev);
+
+    
+  /* Watch out for the interface going down so we can stop the polling
+   * function.
+   */
+  if ((strcmp(dev->name, "pc0") == 0) && (event == NETDEV_GOING_DOWN)) {
+    priv->shutdown = 1;
+  }
+  
+  return NOTIFY_DONE;
+}
+
+static struct notifier_block sccpc_dev_notifier = {
+  .notifier_call = sccpc_dev_event_handler
+};
+
+
+
+void sccpc_cleanup(void)
+{
+  if (sccpc_dev) {
+    unregister_netdev(sccpc_dev);
+    free_netdev(sccpc_dev);
+  }
+}
+
+
+
+int sccpc_init_module(void)
+{
+  int result;
+
+  /* This driver does only work in a bare-metal environment */
+  if (!scc_bare_metal()) {
+    printk(KERN_INFO "sccpc: startup in non-SCC or paravirtualized environment.\n");
+    return -EINVAL;
+  }
+
+  /* Allocate the devices */
+  sccpc_dev = alloc_netdev(sizeof(struct sccpc_priv), "pc0", sccpc_init);
+  if (!sccpc_dev) return -ENOMEM;
+
+  result = register_netdev(sccpc_dev);
+  if (result) {
+    printk(KERN_WARNING "sccpc: error %i registering device \"%s\"\n",
+                        result, sccpc_dev->name);
+    free_netdev(sccpc_dev);
+    return -ENODEV;
+  }
+
+  /* Make sure we are informed about events affecting this device */
+  register_netdevice_notifier(&sccpc_dev_notifier);
+
+  return 0;
+}
+
+
+
+module_init(sccpc_init_module);
+module_exit(sccpc_cleanup);
diff -urN linux-3.1.4/drivers/tty/serial/8250.c linux-3.1.4-scc/drivers/tty/serial/8250.c
--- linux-3.1.4/drivers/tty/serial/8250.c	2011-11-28 23:48:14.000000000 +0100
+++ linux-3.1.4-scc/drivers/tty/serial/8250.c	2011-12-20 15:24:05.000000000 +0100
@@ -1173,7 +1173,7 @@
 		 */
 		scratch = serial_inp(up, UART_IER);
 		serial_outp(up, UART_IER, 0);
-#ifdef __i386__
+#if defined(__i386__) && !defined(CONFIG_X86_SCC)
 		outb(0xff, 0x080);
 #endif
 		/*
@@ -1182,7 +1182,7 @@
 		 */
 		scratch2 = serial_inp(up, UART_IER) & 0x0f;
 		serial_outp(up, UART_IER, 0x0F);
-#ifdef __i386__
+#if defined(__i386__) && !defined(CONFIG_X86_SCC)
 		outb(0, 0x080);
 #endif
 		scratch3 = serial_inp(up, UART_IER) & 0x0f;
diff -urN linux-3.1.4/drivers/video/Kconfig linux-3.1.4-scc/drivers/video/Kconfig
--- linux-3.1.4/drivers/video/Kconfig	2011-11-28 23:48:14.000000000 +0100
+++ linux-3.1.4-scc/drivers/video/Kconfig	2011-12-20 15:27:07.618380472 +0100
@@ -1836,6 +1836,15 @@
 	  DECstation series (Personal DECstation 5000/20, /25, /33, /50,
 	  Codename "Maxine").
 
+config FB_SCCGFX
+	tristate "SCC GFX framebuffer support"
+	depends on FB && X86
+	select FB_CFB_FILLRECT
+	select FB_CFB_COPYAREA
+	select FB_CFB_IMAGEBLIT
+	help
+	  Support for the SCC GFX framebuffer.
+
 config FB_G364
 	bool "G364 frame buffer support"
 	depends on (FB = y) && (MIPS_MAGNUM_4000 || OLIVETTI_M700)
diff -urN linux-3.1.4/drivers/video/Makefile linux-3.1.4-scc/drivers/video/Makefile
--- linux-3.1.4/drivers/video/Makefile	2011-11-28 23:48:14.000000000 +0100
+++ linux-3.1.4-scc/drivers/video/Makefile	2011-12-20 15:27:07.618380472 +0100
@@ -112,6 +112,7 @@
 obj-$(CONFIG_FB_PMAG_BA)	  += pmag-ba-fb.o
 obj-$(CONFIG_FB_PMAGB_B)	  += pmagb-b-fb.o
 obj-$(CONFIG_FB_MAXINE)		  += maxinefb.o
+obj-$(CONFIG_FB_SCCGFX)		  += sccgfx.o
 obj-$(CONFIG_FB_METRONOME)        += metronomefb.o
 obj-$(CONFIG_FB_BROADSHEET)       += broadsheetfb.o
 obj-$(CONFIG_FB_S1D13XXX)	  += s1d13xxxfb.o
diff -urN linux-3.1.4/drivers/video/sccgfx.c linux-3.1.4-scc/drivers/video/sccgfx.c
--- linux-3.1.4/drivers/video/sccgfx.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-3.1.4-scc/drivers/video/sccgfx.c	2011-12-20 15:27:07.618380472 +0100
@@ -0,0 +1,332 @@
+/*******************************************************************************
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms of the GNU General Public License as published by the Free
+  Software Foundation; either version 2 of the License, or (at your option)
+  any later version.
+
+  This program is distributed in the hope that it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+  more details.
+
+  You should have received a copy of the GNU General Public License along with
+  this program; if not, write to the Free Software Foundation, Inc., 59
+  Temple Place - Suite 330, Boston, MA  02111-1307, USA.
+
+  The full GNU General Public License is included in this distribution in the
+  file called LICENSE.
+
+  Contact Information:
+  Jan-Michael Brummer <jan-michael.brummer@intel.com>
+  Intel Braunschweig
+
+*******************************************************************************/
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/errno.h>
+#include <linux/string.h>
+#include <linux/mm.h>
+#include <linux/tty.h>
+#include <linux/slab.h>
+#include <linux/delay.h>
+#include <linux/init.h>
+#include <linux/fb.h>
+
+/* Change Log
+ * 0.2.0	04/19/2011
+ *   o add the possibility to set the resolution via cmdline
+ * 0.1.0	08/19/2010
+ *   o first release
+ */
+
+#define MODVERSTRING	"0.1.0"
+
+/* Default screen resolution */
+#define WIDTH	800
+#define HEIGHT	600
+#define BPP		16
+
+/* Base address (310MB) */
+#define MEGABYTE			0x100000
+#define GFX_BASE_ADDRESS	(630 * MEGABYTE)
+/* Length (10MB) */
+#define GFX_LENGTH			(10 * MEGABYTE)
+#define GFX_HEADER			4096
+
+/** framebuffer var screen information */
+static struct fb_var_screeninfo sccgfxfb_var = {
+	.xres			= WIDTH,
+	.yres			= HEIGHT,
+	.xres_virtual	= WIDTH,
+	.yres_virtual	= HEIGHT,
+	.bits_per_pixel	= BPP,
+	.activate		= FB_ACTIVATE_NOW,
+	.height			= HEIGHT,
+	.width			= WIDTH,
+	.sync			= FB_SYNC_HOR_HIGH_ACT | FB_SYNC_VERT_HIGH_ACT,
+	.vmode			= FB_VMODE_NONINTERLACED,
+};
+
+/** framebuffer fix screen information */
+static struct fb_fix_screeninfo sccgfxfb_fix = {
+	.id				= "SCC GFX",
+	.smem_len		= GFX_LENGTH,
+	.type			= FB_TYPE_PACKED_PIXELS,
+	.visual			= FB_VISUAL_PSEUDOCOLOR,
+	.line_length	= WIDTH*BPP/8,
+	.accel			= FB_ACCEL_NONE,
+};
+
+/* Our own framebuffer information structure */
+static struct fb_info fb_info;
+
+static long grb_offset = 0xF9000000;
+static unsigned long base_address = GFX_BASE_ADDRESS;
+static unsigned long memory_size = GFX_LENGTH;
+static unsigned long res_x = 0;
+static unsigned long res_y = 0;
+static unsigned long res_bpp = 0;
+
+/**
+ * \brief Set <pseudo> color register
+ * \param regno register number
+ * \param red red value
+ * \param green green value
+ * \param blue blue value
+ * \param transp alpha value
+ * \param fbinfo framebuffer information pointer
+ * \return 0 = handled, 1 = not handled
+ */
+static int sccgfx_setcolreg(unsigned regno, unsigned red, unsigned green,
+                            unsigned blue, unsigned transp,
+                            struct fb_info *fbinfo) {
+	/* if the number is above our color map length, drop it */
+	if (regno >= fbinfo->cmap.len) {
+		return 1;
+	}
+
+	if (fbinfo->var.bits_per_pixel != 8 && regno < 16) {
+		switch (fbinfo->var.bits_per_pixel) {
+			case 16:
+				if (fbinfo->var.red.offset != 10) {
+					/* 0:5:6:5 */
+					((u32*)(fbinfo->pseudo_palette))[regno] = ((red & 0xf800)) |
+                           ((green & 0xfc00) >> 5) | ((blue & 0xf800) >> 11);
+				}
+				break;
+			case 24:
+			case 32:
+				red >>= 8;
+				green >>= 8;
+				blue >>= 8;
+				((u32*)(fbinfo->pseudo_palette))[regno] = 
+                        (red << fbinfo->var.red.offset) |
+                        (green << fbinfo->var.green.offset) |
+                        (blue << fbinfo->var.blue.offset);
+				break;
+			default:
+				break;
+		}
+	}
+
+	return 0;
+}
+
+/** framebuffer operations */
+static struct fb_ops sccgfxfb_ops = {
+	.owner			= THIS_MODULE,
+	.fb_setcolreg	= sccgfx_setcolreg,
+	.fb_fillrect	= cfb_fillrect,
+	.fb_copyarea	= cfb_copyarea,
+	.fb_imageblit	= cfb_imageblit,
+};
+
+static int __init sccgfx_setup(char *options) {
+	char *this_opt;
+
+	if (!options || !*options) {
+		return 0;
+	}
+
+	while ((this_opt = strsep(&options, ",")) != NULL) {
+		if (!*this_opt) {
+			continue;
+		}
+
+		if (!strncmp(this_opt, "base:", 5)) {
+			base_address = simple_strtoul(this_opt + 5, NULL, 0) * MEGABYTE;
+		} else if (!strncmp(this_opt, "size:", 5)) {
+			memory_size = simple_strtoul(this_opt + 5, NULL, 0) * MEGABYTE;
+		} else if (!strncmp(this_opt, "x:", 2)) {
+			res_x = simple_strtoul(this_opt + 2, NULL, 0);
+		} else if (!strncmp(this_opt, "y:", 2)) {
+			res_y = simple_strtoul(this_opt + 2, NULL, 0);
+		} else if (!strncmp(this_opt, "bpp:", 4)) {
+			res_bpp = simple_strtoul(this_opt + 4, NULL, 0);
+		} else {
+			printk(KERN_INFO "Unknown value!\n");
+		}
+	}
+
+	return 0;
+}
+
+/**
+ * \brief Initialize scc graphic framebuffer
+ * \return error code, 0 = success, otherwise error
+ */
+int __init sccgfxfb_init(void) {
+	unsigned char *shm;
+	unsigned char *fb_off;
+	int x = WIDTH;
+	int y = HEIGHT;
+	int bpp = BPP;
+	char *option = NULL;
+
+	printk(KERN_INFO "sccgfxfb: Initializing framebuffer\n");
+
+	fb_get_options("rckgfx", &option);
+	sccgfx_setup(option);
+
+	if (res_x != 0 && res_y != 0 && res_bpp != 0) {
+		printk(KERN_INFO "sccgfxfb: Valid information from cmdline\n");
+		x = res_x;
+		y = res_y;
+		bpp = res_bpp;
+	} else {
+		printk(KERN_INFO "sccgfxfb: Trying FPGA values\n");
+		/* Read requested display values */
+		shm = ioremap_nocache(grb_offset, 0x10000);
+		if (shm != NULL) {
+			res_x = readl(shm + 0x8238) & 0xFFFFFFFF;
+			res_y = readl(shm + 0x823C) & 0xFFFFFFFF;
+			res_bpp = readl(shm + 0x8240) & 0xFFFFFFFF;
+			if (res_x != 0 && res_y != 0 && res_bpp != 0) {
+				printk(KERN_INFO "sccgfxfb: Valid information from FPGA\n");
+				x = res_x;
+				y = res_y;
+				bpp = res_bpp;
+			}
+			iounmap(shm);
+		} else {
+			printk(KERN_DEBUG "sccgfxfb: could not map grb and therefore not read display "
+                   "values, setting default values (%dx%dx%d)!\n", x, y, bpp);
+		}
+	}
+
+	/* Update var information */
+	sccgfxfb_var.xres = x;
+	sccgfxfb_var.yres = y;
+	sccgfxfb_var.xres_virtual = x;
+	sccgfxfb_var.yres_virtual = y;
+	sccgfxfb_var.bits_per_pixel = bpp;
+	sccgfxfb_var.width = x;
+	sccgfxfb_var.height = y;
+
+	/* Update fix information */
+	sccgfxfb_fix.line_length = x * bpp / 8;
+
+	printk(KERN_INFO "sccgfxfb: Requested display values (%dx%dx%d)\n",
+	       x, y, bpp);
+
+	/* Framebuffer display memory base address */
+	sccgfxfb_fix.smem_start = base_address;
+	/* Length of buffer */
+	sccgfxfb_fix.smem_len = memory_size;
+	/* Map in private video memory */
+	fb_off = ioremap_nocache(sccgfxfb_fix.smem_start, sccgfxfb_fix.smem_len);
+	if (fb_off == NULL) {
+		printk(KERN_INFO "sccgfxfb: cannot remap!!\n");
+		return -1;
+	}
+
+	/* Clear memory */
+	memset(fb_off, 0x00, sccgfxfb_fix.smem_len);
+
+	/* Write display informations */
+	writel(sccgfxfb_var.xres, (u8*)fb_off + 0x00);
+	writel(sccgfxfb_var.yres, (u8*)fb_off + 0x04);
+	writel(sccgfxfb_var.bits_per_pixel, (u8*)fb_off + 0x08);
+
+	/* Set start address into shared memory (sccDisplay will use this one as
+     * base address)
+     */
+	shm = ioremap_nocache(0x80000940, 4);
+	if (shm != NULL) {
+		writel(sccgfxfb_fix.smem_start, shm);
+		iounmap(shm);
+	} else {
+		printk(KERN_DEBUG "sccgfxfb: could not map crb and therefore not write base "
+               "address. sccDisplay will not work!\n");
+	}
+
+	/* Skip framebuffer header */
+	fb_off += GFX_HEADER;
+	sccgfxfb_fix.smem_start += GFX_HEADER;
+	sccgfxfb_fix.smem_len -= GFX_HEADER;
+
+	/* set color offsets/lengths */
+	if (sccgfxfb_var.bits_per_pixel != 8) {
+		sccgfxfb_fix.visual = FB_VISUAL_TRUECOLOR;
+		switch (sccgfxfb_var.bits_per_pixel) {
+			case 16:
+				sccgfxfb_var.red.offset = 11;
+				sccgfxfb_var.red.length = 5;
+				sccgfxfb_var.green.offset = 5;
+				sccgfxfb_var.green.length = 6;
+				sccgfxfb_var.blue.offset = 0;
+				sccgfxfb_var.blue.length = 5;
+				break;
+			case 32:
+				sccgfxfb_var.red.offset = 24;
+				sccgfxfb_var.red.length = 8;
+				sccgfxfb_var.green.offset = 16;
+				sccgfxfb_var.green.length = 8;
+				sccgfxfb_var.blue.offset = 8;
+				sccgfxfb_var.blue.length = 8;
+				sccgfxfb_var.transp.offset = 0;
+				sccgfxfb_var.transp.length = 8;
+				break;
+		}
+	}
+	sccgfxfb_var.yres_virtual = y;
+
+	fb_info.fbops = &sccgfxfb_ops;
+	fb_info.screen_base = fb_off;
+	fb_info.var = sccgfxfb_var;
+	fb_info.fix = sccgfxfb_fix;
+	fb_info.flags = FBINFO_FLAG_DEFAULT;
+	fb_info.pseudo_palette = kzalloc(sizeof(u32) * 256, GFP_KERNEL);
+
+	/* Allocate color map */
+	fb_alloc_cmap(&fb_info.cmap, 256, 0);
+
+	if (register_framebuffer(&fb_info) < 0) {
+		printk(KERN_WARNING "sccgfxfb: failed while register_framebuffer\n");
+		return 1;
+	}
+
+	printk(KERN_INFO "sccgfxfb: register framebuffer (%dx%dx%d)\n",
+           fb_info.var.xres, fb_info.var.yres, fb_info.var.bits_per_pixel);
+
+	return 0;
+}
+
+/**
+ * \brief Remove framebuffer
+ */
+static void __exit sccgfxfb_exit(void) {
+	unregister_framebuffer(&fb_info);
+}
+
+#ifdef MODULE
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Jan-Michael Brummer");
+MODULE_VERSION(MODVERSTRING);
+MODULE_DESCRIPTION("Intel(R) SCC graphic driver");
+#endif
+module_init(sccgfxfb_init);
+module_exit(sccgfxfb_exit);
diff -urN linux-3.1.4/fs/Kconfig linux-3.1.4-scc/fs/Kconfig
--- linux-3.1.4/fs/Kconfig	2011-11-28 23:48:14.000000000 +0100
+++ linux-3.1.4-scc/fs/Kconfig	2011-12-20 15:27:07.618380472 +0100
@@ -169,6 +169,12 @@
 
 	  If unsure, say N.
 
+config POPSHM
+	bool "POPSHM support"
+	depends on HUGETLBFS
+	help
+		POPSHM feature for Intel's Single Chip Cloud Computer.
+
 config HUGETLB_PAGE
 	def_bool HUGETLBFS
 
diff -urN linux-3.1.4/include/linux/clocksource.h linux-3.1.4-scc/include/linux/clocksource.h
--- linux-3.1.4/include/linux/clocksource.h	2011-11-28 23:48:14.000000000 +0100
+++ linux-3.1.4-scc/include/linux/clocksource.h	2011-12-20 15:27:07.618380472 +0100
@@ -278,7 +278,7 @@
 extern void clocksource_change_rating(struct clocksource *cs, int rating);
 extern void clocksource_suspend(void);
 extern void clocksource_resume(void);
-extern struct clocksource * __init __weak clocksource_default_clock(void);
+extern struct clocksource * __weak clocksource_default_clock(void);
 extern void clocksource_mark_unstable(struct clocksource *cs);
 
 extern void
@@ -338,6 +338,7 @@
 #endif
 
 extern void timekeeping_notify(struct clocksource *clock);
+extern struct clocksource* timekeeping_get_clock(void);
 
 extern cycle_t clocksource_mmio_readl_up(struct clocksource *);
 extern cycle_t clocksource_mmio_readl_down(struct clocksource *);
diff -urN linux-3.1.4/include/linux/sccsys.h linux-3.1.4-scc/include/linux/sccsys.h
--- linux-3.1.4/include/linux/sccsys.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-3.1.4-scc/include/linux/sccsys.h	2011-12-20 15:27:07.618380472 +0100
@@ -0,0 +1,385 @@
+/*
+ *  Copyright 2011 Jan-Arne Sobania <jan-arne.sobania@hpi.uni-potsdam.de>, HPI
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2, or (at your option)
+ *  any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; see the file COPYING.  If not, write to
+ *  the Free Software Foundation, 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ */
+
+#ifndef __LINUX_SCCSYS_H__
+#define __LINUX_SCCSYS_H__
+
+/* Query bus frequency at boot time. */
+extern unsigned long scc_get_boot_busclock(void);
+
+/* Check whether we are running on the lowest layer on SCC hardware */
+extern int scc_bare_metal(void);
+
+/* SCC-specific constants
+ */
+
+/* Number of cores on a SCC chip */
+#define SCC_CORECOUNT		48
+/* Number of tiles on a SCC chip */
+#define SCC_TILECOUNT		24
+/* Number of bytes in a cache line */
+#define SCC_CLINE_SIZE		32
+/* Raw size of the message passing buffer (bytes per core) */
+#define SCC_MPB_SIZE		8192
+/* Raw size of the configuration register bank (bytes per tile) */
+#define SCC_CRB_SIZE		0x2000
+/* Raw size of the global register bank */
+#define SCC_GRB_SIZE		0x10000
+/* Size of a SCC memory tile (16MB) */
+#define SCC_TILE_SIZE		0x01000000
+
+/* Stride between adjacent LUT entries */
+#define SCC_LUT_STRIDE		8
+
+/* Register offsets */
+#define SCC_GLCFG0		0x0010
+#define SCC_GLCFG1		0x0018
+#define SCC_L2CFG0		0x0020
+#define SCC_L2CFG1		0x0028
+#define SCC_SENSOR		0x0040
+#define SCC_GCBCFG		0x0080
+#define SCC_TILEID		0x0100
+#define SCC_LOCK0		0x0200
+#define SCC_LOCK1		0x0400
+#define SCC_LUT0		0x0800
+#define SCC_LUT1		0x1000
+
+
+/* Mask of the interrupt bits */
+#define SCC_INTR_MASK		0x00000002
+#define SCC_NMI_MASK		0x00000001
+
+
+/* Global register offsets */
+/* Please note the GRB space is particularly nasty, as accessing an undefined
+ * register result in undefined behaviour, up to and including the whole SCC
+ * locking up. */
+#define SCCGRB_EMAC_MACBASE_HI	0x7E00
+#define SCCGRB_EMAC_MACBASE_LO	0x7E04
+#define SCCGRB_EMAC_IP_START	0x7E08
+#define SCCGRB_EMAC_HOST_IP	0x7E0C
+#define SCCGRB_EMAC_GW_IP	0x7E10
+
+#define SCCGRB_FPGA_CONFIG	0x822C
+#define SCCGRB_CLKFREQ		0x8230
+
+#define SCCGRB_PRIVATE_SLOTS	0x8244
+
+
+/* Valid sub destination IDs */
+#define SCC_SUBDEST_CORE0	0
+#define SCC_SUBDEST_CORE1	1
+#define SCC_SUBDEST_CRB		2
+#define SCC_SUBDEST_MPB		3
+#define SCC_SUBDEST_PERIE	4
+#define SCC_SUBDEST_PERIS	5
+#define SCC_SUBDEST_PERIW	6
+#define SCC_SUBDEST_PERIN	7
+
+static const char* sccsys_subdest_names[] __attribute__((unused)) = {
+	"CORE0", "CORE1", "-CRB-", "-MPB-", "PERIE", "PERIS", "PERIW", "PERIN"
+};
+
+/* The new instruction to flush message buffer content from L1 */
+#define CL1FLUSHMB __asm__ volatile ( ".byte 0x0f; .byte 0x0a;\n" )
+
+typedef unsigned char scc_pid_t;
+#define SCC_NO_PID ((scc_pid_t)(-1))
+
+/* Perform hex dump */
+static void __attribute__((unused)) sccutil_dump_buffer(unsigned char* buffer, unsigned len)
+{
+	char linebuf[16*3 + 10 + 64], *wptr;
+	int li, ci;
+	int lc = (len + 15) / 16;
+
+	printk(KERN_INFO "<-- buffer @ %p, len = %x\n", buffer, len);
+	for (li = 0; li < lc; li++) {
+		int cc;
+
+		wptr = linebuf;
+		wptr += sprintf(wptr, "%04x: ", li * 16);
+
+		cc = ((li + 1 == lc) && (len % 16)) ? (len % 16) : 16;
+
+		for (ci = 0; ci < cc; ci++) {
+			wptr += sprintf(wptr, " %02x", buffer[li*16+ci]);
+		}
+		for (; ci < 16; ci++) {
+			wptr += sprintf(wptr, "   ");
+		}
+		wptr += sprintf(wptr, "  ");
+
+		for (ci = 0; ci < cc; ci++) {
+			char c = buffer[li*16+ci];
+			if (c < 0x20 || c >= 0x7F) c = '.';
+			wptr += sprintf(wptr, "%c", c);
+		}
+		for (; ci < 16; ci++) {
+			wptr += sprintf(wptr, " ");
+		}
+
+		printk(KERN_CONT "%s\n", linebuf);
+	}
+	printk(KERN_CONT "-- end -->\n");
+}
+
+
+/* LookUp Table Entry */
+typedef union scc_lut {
+	struct {
+		unsigned int address : 10;
+		unsigned int subdest :  3;
+		unsigned int x       :  4;
+		unsigned int y       :  4;
+		unsigned int bypass  :  1;
+	};
+	unsigned int raw;
+} scc_lut_t;
+
+#define sccsys_get_route(t)		(((t).y << 4) | (t).x)
+
+/* Type for PFNs */
+typedef unsigned long pfn_t;
+
+#if PAGE_SHIFT != 12
+#error SCCSYS expects a PAGE_SHIFT of 12.
+#endif
+
+/******************************************************************************/
+/* Explicit cache control routines
+ *
+ * These routines are meant to allow direct control of the processor's caches
+ * in regards to a specific page. */
+
+/* Perform write-back/invalidate for all cachelines holding the specified pfn */
+extern void scc_cop_wbinv_pfn(pfn_t pfn);
+
+/* Perform write-back for all cachelines holding the specified pfn */
+/* NOTE: This call must only be used if no invalidation is needed. However, it
+ *       may be possible that invalidation is still performed, depending on
+ *       whether the caches support issuing write-back only. */
+#define scc_cop_wb_pfn(pfn)	scc_cop_wbinv_pfn(pfn)
+
+/* Perform invalidate for all cachelines holding the specified pfn */
+/* NOTE: This call may perform a writeback if required by the implementation.
+ *       The caller merely provides a hint that a writeback is not needed. */
+#define scc_cop_inv_pfn(pfn)	scc_cop_wbinv_pfn(pfn)
+
+/* System address. */
+typedef union scc_addr {
+	struct {
+		unsigned long long offset  : 24;
+		unsigned long long address : 10;
+		unsigned long long subdest :  3;
+		unsigned long long x       :  4;
+		unsigned long long y       :  4;
+		unsigned long long bypass  :  1;
+	};
+	unsigned long long raw;
+} scc_addr_t;
+
+/* Convert a node-local physical address into a system address */
+extern scc_addr_t sccsys_physical_to_system(scc_pid_t pid, unsigned long pa);
+
+#define sccsys_local_physical_to_system(pa) \
+	sccsys_physical_to_system(sccsys_get_pid(), pa);
+
+/* System page frame number. */
+typedef union scc_syspfn {
+	struct {
+		unsigned long long offset  : 12;
+		unsigned long long address : 10;
+		unsigned long long subdest :  3;
+		unsigned long long x       :  4;
+		unsigned long long y       :  4;
+		unsigned long long bypass  :  1;
+	};
+	unsigned long long raw;
+} scc_syspfn_t;
+
+/* Get system pfn from system address */
+static inline scc_syspfn_t sccsys_get_syspfn(scc_addr_t addr)
+{
+	scc_syspfn_t pfn;
+	pfn.offset = addr.offset >> PAGE_SHIFT;
+	pfn.address = addr.address;
+	pfn.subdest = addr.subdest;
+	pfn.x = addr.x;
+	pfn.y = addr.y;
+	pfn.bypass = addr.bypass;
+	return pfn;
+}
+
+/*
+ * Coordinates of a single component on the SCC die
+ * This structure has specially been crafted s.t. its raw fields has the same
+ * format as the TILEID configuration register.
+ */
+typedef union scc_coord {
+	struct {
+		int z : 3;
+		int x : 4;
+		int y : 4;
+	};
+	int raw;
+} scc_coord_t;
+
+/* Convert tileid into decoded coordinate structure. */
+static inline scc_coord_t scc_tileid_to_coord(int tileid)
+{
+	scc_coord_t coord;
+	coord.raw = tileid;
+	return coord;
+}
+
+/* Convert decoded coordinate structure into tileid. */
+static inline int scc_coord_to_tileid(scc_coord_t coord)
+{
+	return coord.raw;
+}
+
+/* Convert decoded coordinate structure into processor id. */
+static inline scc_pid_t scc_coord_to_pid(scc_coord_t coord)
+{
+	scc_pid_t pid;
+	pid=( ( coord.x + ( 6 * coord.y ) ) * 2 ) + coord.z;
+	return pid;
+}
+
+/* Convert processor id to decoded coordinate structure. */
+static inline scc_coord_t scc_pid_to_coord(scc_pid_t pid)
+{
+	scc_coord_t coord = { .raw = 0 };
+	coord.z = (pid % 2);
+	coord.y = (pid / 2) / 6;
+	coord.x = (pid / 2) % 6;
+	return coord;
+}
+
+/* Convert tileid to processor id. */
+static inline scc_pid_t scc_tileid_to_pid(int tileid)
+{
+	return scc_coord_to_pid(scc_tileid_to_coord(tileid));
+}
+
+/* Convert processor id to tileid. */
+static inline int scc_pid_to_tileid(scc_pid_t pid)
+{
+	return scc_coord_to_tileid(scc_pid_to_coord(pid));
+}
+
+/*
+ * Global Clock Unit (GCU) Configuration Register
+ */
+typedef union scc_gckcfg {
+	struct {
+		unsigned long RESC0 : 1;
+		unsigned long RESC1 : 1;
+		unsigned long RESL20 : 1;
+		unsigned long RESL21 : 1;
+		unsigned long SREC0 : 1;
+		unsigned long SREC1 : 1;
+		unsigned long SREL21 : 1;
+		unsigned long SREL20 : 1;
+		unsigned long divider : 4;
+		unsigned long ratio : 7;
+		unsigned long router : 7;
+		unsigned long reserved : 6;
+	};
+	unsigned long raw;
+} scc_gckcfg_t;
+
+/* Get value of own tileid. The format is 0...0_00000yyy_yxxxxzzz (in bits).
+ * Tile IDs are not consecutive; if a consecutive number is needed, consider
+ * using the PID instead.
+ */
+extern int sccsys_get_tileid(void);
+
+/* Get own processor id. This is a consecutive number from 0 to SCC_CORECOUNT-1.
+ */
+extern scc_pid_t sccsys_get_pid(void);
+
+/* Get own coordinates. This is the decoded version of the TILEID returned by
+ * sccsys_get_tileid. */
+extern scc_coord_t sccsys_get_coord(void);
+
+/* Get logically next processor id. This is a consecutive number from 0 to
+ * SCC_CORECOUNT-1, but is not neccessarily ((pid+1) % SCC_CORECOUNT). The
+ * number is usually chosen to reflect the adjacent core having the shortest
+ * distance, although it guarantees that the cycle closes after exactly
+ * SCC_CORECOUNT invocations.
+ */
+extern scc_pid_t sccsys_get_next_pid(scc_pid_t pid);
+
+/* Acquire the test&set register of the specified PID. This call returns 1 if
+ * the lock has successfully been acquired, or 0 otherwise.
+ *
+ * In the current implementation, the call does only fail if the PID is invalid
+ * (i.e., outside of the range from 0 to SCC_CORECOUNT-1) or mapping of the
+ * configuration registers has failed.
+ */
+extern int sccsys_acquire_pid_lock(scc_pid_t pid);
+
+/* Release the test&set register of the specified PID. This call returns 1 if
+ * the lock has successfully been released, or 0 otherwise.
+ *
+ * In the current implementation, the call does only fail if the PID is invalid
+ * (i.e., outside of the range from 0 to SCC_CORECOUNT-1) or mapping of the
+ * configuration registers has failed.
+ */
+extern int sccsys_release_pid_lock(scc_pid_t pid);
+
+/* Set a bit in a core's configuration register */
+extern int sccsys_set_config_bits(scc_pid_t pid, unsigned int bits);
+
+/* Clear a bit for a core's configuration register */
+extern int sccsys_clear_config_bits(scc_pid_t pid, unsigned int bits);
+
+/* Trigger an interrupt to a core (directly via the configuration register) */
+extern int sccsys_trigger_irq_direct(scc_pid_t pid, unsigned int bits, int edgeIrq);
+
+/* Clear an interrupt request bit for a processor identified by pid. */
+extern int sccsys_clear_irq_direct(scc_pid_t pid, unsigned int bits);
+
+/* Read LUT entry */
+extern scc_lut_t sccsys_read_lut_entry(scc_pid_t pid, unsigned int index);
+
+/* Write LUT entry */
+extern int sccsys_write_lut_entry(scc_pid_t pid, unsigned int index, scc_lut_t lut);
+
+/* Read Global Clock Unit (GCU) configuration register */
+extern scc_gckcfg_t sccsys_read_gcbcfg(scc_pid_t pid);
+
+/* Write Global Clock Unit (GCU) configuration register */
+extern int sccsys_write_gcbcfg(scc_pid_t pid, scc_gckcfg_t cfg);
+
+/* Get address of mapped global configuration register bank */
+extern void* sccsys_get_grb(void);
+
+/* Read global configuration register */
+extern unsigned sccsys_read_grb_entry(unsigned int offset);
+
+/* Write global configuration register */
+extern void sccsys_write_grb_entry(unsigned int offset, unsigned value);
+
+/* Read frequency of the fast clock from the global configuration register bank */
+extern unsigned short sccsys_read_grb_fastclock(void);
+
+#endif /* __LINUX_SCCSYS_H__ */
diff -urN linux-3.1.4/init/Kconfig linux-3.1.4-scc/init/Kconfig
--- linux-3.1.4/init/Kconfig	2011-11-28 23:48:14.000000000 +0100
+++ linux-3.1.4-scc/init/Kconfig	2011-12-20 15:27:07.618380472 +0100
@@ -130,10 +130,13 @@
 config HAVE_KERNEL_LZO
 	bool
 
+config HAVE_KERNEL_NONE
+	bool
+
 choice
 	prompt "Kernel compression mode"
 	default KERNEL_GZIP
-	depends on HAVE_KERNEL_GZIP || HAVE_KERNEL_BZIP2 || HAVE_KERNEL_LZMA || HAVE_KERNEL_XZ || HAVE_KERNEL_LZO
+	depends on HAVE_KERNEL_GZIP || HAVE_KERNEL_BZIP2 || HAVE_KERNEL_LZMA || HAVE_KERNEL_XZ || HAVE_KERNEL_LZO || HAVE_KERNEL_NONE
 	help
 	  The linux kernel is a kind of self-extracting executable.
 	  Several compression algorithms are available, which differ
@@ -201,6 +204,12 @@
 	  size is about 10% bigger than gzip; however its speed
 	  (both compression and decompression) is the fastest.
 
+config KERNEL_NONE
+	bool "None"
+	depends on HAVE_KERNEL_NONE
+	help
+	  No compression.
+
 endchoice
 
 config DEFAULT_HOSTNAME
diff -urN linux-3.1.4/kernel/time/jiffies.c linux-3.1.4-scc/kernel/time/jiffies.c
--- linux-3.1.4/kernel/time/jiffies.c	2011-11-28 23:48:14.000000000 +0100
+++ linux-3.1.4-scc/kernel/time/jiffies.c	2011-12-20 15:27:07.618380472 +0100
@@ -91,7 +91,7 @@
 
 core_initcall(init_jiffies_clocksource);
 
-struct clocksource * __init __weak clocksource_default_clock(void)
+struct clocksource * __weak clocksource_default_clock(void)
 {
 	return &clocksource_jiffies;
 }
diff -urN linux-3.1.4/kernel/time/timekeeping.c linux-3.1.4-scc/kernel/time/timekeeping.c
--- linux-3.1.4/kernel/time/timekeeping.c	2011-11-28 23:48:14.000000000 +0100
+++ linux-3.1.4-scc/kernel/time/timekeeping.c	2011-12-20 15:27:07.618380472 +0100
@@ -460,6 +460,16 @@
 }
 
 /**
+ * timekeeping_get_clock - Get clock source currently used by the timekeeper
+ *
+ * returns the pointer to the clock source
+ */
+struct clocksource* timekeeping_get_clock(void)
+{
+	return timekeeper.clock;
+}
+
+/**
  * ktime_get_real - get the real (wall-) time in ktime_t format
  *
  * returns the time in ktime_t format
diff -urN linux-3.1.4/mm/hugetlb.c linux-3.1.4-scc/mm/hugetlb.c
--- linux-3.1.4/mm/hugetlb.c	2011-11-28 23:48:14.000000000 +0100
+++ linux-3.1.4-scc/mm/hugetlb.c	2011-12-20 15:27:07.628922062 +0100
@@ -1,6 +1,14 @@
 /*
  * Generic hugetlb support.
  * (C) William Irwin, April 2004
+ *
+ * April 2011:
+ * Added POPSHM feature for Intel's Single Chip Cloud Computer.
+ *
+ * Isaias Alberto Compres Urenai(isaias.a.compres.urena@intel.com)
+ *
+ * Portions copyright 2011 Intel Corporation.
+ *
  */
 #include <linux/list.h>
 #include <linux/init.h>
@@ -30,6 +38,46 @@
 #include <linux/node.h>
 #include "internal.h"
 
+#ifdef CONFIG_POPSHM
+// legacy shared memory
+#define SHM_X0_Y0 0x80000000
+#define SHM_X5_Y0 0x81000000
+#define SHM_X0_Y2 0x82000000
+#define SHM_X5_Y2 0x83000000
+#define SHM_ADDR  SHM_X0_Y0
+// CRB of this core
+#define CRB_OWN   0xf8000000
+// LUTs
+#define LUT0     0x00800
+#define LUT1     0x01000
+// Tile ID
+#define MYTILEID 0x100
+
+#define POPSHM_PAGEDATA_BASE (SHM_X5_Y0 + 0x0800)
+#define POPSHM_PAGEDATA_SLOTSIZE 8
+#define POPSHM_PAGEDATA_LOCALSLOT (POPSHM_PAGEDATA_BASE + \
+		    (local_core_id * POPSHM_PAGEDATA_SLOTSIZE))
+#define POPSHM_LUT_BASE (CRB_OWN + (local_core_z?LUT1:LUT0))
+#define POPSHM_LUT_SLOTSIZE 8
+#define POPSHM_LUT_OFFSET ((popshm_base_address >> POPSHMPAGE_SHIFT)\
+		    * POPSHM_LUT_SLOTSIZE)
+
+#define POPSHMPAGE_MAXCOUNT  5
+#define POPSHMPAGE_SHIFT  24
+#define POPSHMPAGE_SIZE ((1UL) << POPSHMPAGE_SHIFT)
+#define POPSHMPAGE_MASK (~(POPSHMPAGE_SIZE - 1))
+
+static void free_all_popshm(struct hstate *h, nodemask_t *nodes_allowed);
+static void publish_popshm_data(struct hstate *h);
+static void try_to_free_low(struct hstate *h, unsigned long count,
+						nodemask_t *nodes_allowed);
+
+static unsigned int popshm_base_address;
+static int local_core_z, local_core_id;
+#endif
+
+#define persistent_huge_pages(h) (h->nr_huge_pages - h->surplus_huge_pages)
+
 const unsigned long hugetlb_zero = 0, hugetlb_infinity = ~0UL;
 static gfp_t htlb_alloc_mask = GFP_HIGHUSER;
 unsigned long hugepages_treat_as_movable;
@@ -659,6 +707,38 @@
 	return nid;
 }
 
+#ifdef CONFIG_POPSHM
+static int alloc_fresh_huge_page(struct hstate *h, nodemask_t *nodes_allowed,
+		unsigned int *page_address)
+{
+	struct page *page;
+	int start_nid;
+	int next_nid;
+	int ret = 0;
+
+	start_nid = hstate_next_node_to_alloc(h, nodes_allowed);
+	next_nid = start_nid;
+
+	do {
+		page = alloc_fresh_huge_page_node(h, next_nid);
+		if (page) {
+			// privide physical address of the page
+			*page_address = page_to_phys(page);
+			printk(KERN_WARNING "fresh page address: %x\n", *page_address);
+			ret = 1;
+			break;
+		}
+		next_nid = hstate_next_node_to_alloc(h, nodes_allowed);
+	} while (next_nid != start_nid);
+
+	if (ret)
+		count_vm_event(HTLB_BUDDY_PGALLOC);
+	else
+		count_vm_event(HTLB_BUDDY_PGALLOC_FAIL);
+
+	return ret;
+}
+#else
 static int alloc_fresh_huge_page(struct hstate *h, nodemask_t *nodes_allowed)
 {
 	struct page *page;
@@ -685,6 +765,7 @@
 
 	return ret;
 }
+#endif
 
 /*
  * helper for free_pool_huge_page() - return the previously saved
@@ -1130,6 +1211,129 @@
 	}
 }
 
+
+#ifdef CONFIG_POPSHM
+static void free_all_popshm(struct hstate *h, nodemask_t *nodes_allowed){
+	int min_count;
+
+	min_count = 0;
+
+	spin_lock(&hugetlb_lock);
+	try_to_free_low(h, min_count, nodes_allowed);
+	while (min_count < persistent_huge_pages(h)) {
+		if (!free_pool_huge_page(h, nodes_allowed, 0))
+			break;
+	}
+	spin_unlock(&hugetlb_lock);
+}
+
+static void publish_popshm_data(struct hstate *h) {
+	void *lut_address, *popshm_table_address;
+	unsigned int data;
+
+	lut_address = ioremap_nocache((POPSHM_LUT_BASE +
+				POPSHM_LUT_OFFSET), sizeof(int));
+
+	popshm_table_address = ioremap_nocache(POPSHM_PAGEDATA_LOCALSLOT,
+			POPSHM_PAGEDATA_SLOTSIZE);
+
+	if (lut_address && popshm_table_address) {
+		data = readl(lut_address);
+		writel(data, popshm_table_address);
+		data = h->nr_huge_pages/4;
+		data += popshm_base_address;
+		writel(data, popshm_table_address + sizeof(int));
+
+		iounmap(lut_address);
+		iounmap(popshm_table_address);
+
+		printk(KERN_WARNING "local popshm page data published\n");
+	}
+}
+
+static void __init hugetlb_hstate_alloc_pages(struct hstate *h)
+{
+	unsigned page_address, previous_page_address;
+	int contig_pages, count, current_page_count, i;
+	void *popshm_table_address;
+
+	NODEMASK_ALLOC(nodemask_t, nodes_allowed,
+			GFP_KERNEL | __GFP_NORETRY);
+
+	printk(KERN_WARNING "hugetlb_hstate_alloc_pages with %ld\n", h->max_huge_pages);
+
+	if (h->max_huge_pages <= 0) { // no pages
+		popshm_table_address = ioremap_nocache(POPSHM_PAGEDATA_LOCALSLOT,
+				POPSHM_PAGEDATA_SLOTSIZE);
+
+		if (popshm_table_address) {
+			writel(0x00000000, popshm_table_address);
+			writel(0x00000000, popshm_table_address + sizeof(int));
+			iounmap(popshm_table_address);
+		}
+	} else { // popshm pages requested
+		if(h->max_huge_pages <= POPSHMPAGE_MAXCOUNT){
+			count = h->max_huge_pages * 4;
+		} else {
+			count = POPSHMPAGE_MAXCOUNT * 4;
+		}
+
+		contig_pages = 1;
+		previous_page_address = 0;
+		popshm_base_address = 0;
+
+		while (!popshm_base_address) {
+			if (!alloc_fresh_huge_page(h, &node_states[N_HIGH_MEMORY], &page_address)) {
+				free_all_popshm(h, nodes_allowed);
+				publish_popshm_data(h);
+				h->max_huge_pages = 0;
+				printk(KERN_ERR "Failed to allocater POPSHM pages\n");
+			}
+			if (previous_page_address - page_address == HPAGE_SIZE) {
+				contig_pages++;
+				if ((contig_pages >= count) && !(page_address & (~POPSHMPAGE_MASK))) {
+					popshm_base_address = page_address;
+					printk(KERN_WARNING "popshm base page address: %x\n", popshm_base_address);
+				}
+			} else {
+				contig_pages = 1;
+			}
+
+			previous_page_address = page_address;
+		}
+
+		// free excess pages
+		if (!( init_nodemask_of_mempolicy(nodes_allowed))) {
+			NODEMASK_FREE(nodes_allowed);
+			nodes_allowed = &node_states[N_HIGH_MEMORY];
+		}
+
+		current_page_count = 0;
+		for_each_node_mask(i, *nodes_allowed) {
+			struct page *page, *next;
+			struct list_head *freel = &h->hugepage_freelists[i];
+			list_for_each_entry_safe(page, next, freel, lru) {
+				current_page_count++;
+				if (current_page_count > count){
+					printk(KERN_WARNING "freeing page with address: %x\n", page_to_phys(page));
+					list_del(&page->lru);
+					update_and_free_page(h, page);
+					h->free_huge_pages--;
+					h->free_huge_pages_node[page_to_nid(page)]--;
+				}
+			}
+		}
+
+		if (nodes_allowed != &node_states[N_HIGH_MEMORY])
+			NODEMASK_FREE(nodes_allowed);
+
+		publish_popshm_data(h);
+	}
+
+	printk(KERN_WARNING "hugetlb_hstate_alloc_pages exit %ld\n", h->max_huge_pages);
+
+}
+#else
 static void __init hugetlb_hstate_alloc_pages(struct hstate *h)
 {
 	unsigned long i;
@@ -1144,6 +1348,7 @@
 	}
 	h->max_huge_pages = i;
 }
+#endif
 
 static void __init hugetlb_init_hstates(void)
 {
@@ -1185,6 +1390,7 @@
 						nodemask_t *nodes_allowed)
 {
 	int i;
+	printk(KERN_WARNING "try_to_free_low\n");
 
 	if (h->order >= MAX_ORDER)
 		return;
@@ -1263,7 +1469,54 @@
 	return ret;
 }
 
-#define persistent_huge_pages(h) (h->nr_huge_pages - h->surplus_huge_pages)
+#ifdef CONFIG_POPSHM
+static unsigned long set_max_huge_pages(struct hstate *h, unsigned long count,
+						nodemask_t *nodes_allowed)
+{
+	unsigned long min_count, ret;
+	unsigned page_address;
+	printk(KERN_WARNING "set_max_huge_pages nr max: %ld; count: %ld;\n", h->max_huge_pages, count);
+
+	if (h->order >= MAX_ORDER)
+		return h->max_huge_pages;
+
+	spin_lock(&hugetlb_lock);
+	while (h->surplus_huge_pages && count > persistent_huge_pages(h)) {
+		if (!adjust_pool_surplus(h, nodes_allowed, -1))
+			break;
+	}
+
+	while (count > persistent_huge_pages(h)) {
+		spin_unlock(&hugetlb_lock);
+
+		ret = alloc_fresh_huge_page(h, nodes_allowed, &page_address);
+		printk(KERN_WARNING "fresh page address: %x\n", page_address);
+		spin_lock(&hugetlb_lock);
+		if (!ret)
+			goto out;
+
+		/* Bail for signals. Probably ctrl-c from user */
+		if (signal_pending(current))
+			goto out;
+	}
+
+	min_count = h->resv_huge_pages + h->nr_huge_pages - h->free_huge_pages;
+	min_count = max(count, min_count);
+	try_to_free_low(h, min_count, nodes_allowed);
+	while (min_count < persistent_huge_pages(h)) {
+		if (!free_pool_huge_page(h, nodes_allowed, 0))
+			break;
+	}
+	while (count < persistent_huge_pages(h)) {
+		if (!adjust_pool_surplus(h, nodes_allowed, 1))
+			break;
+	}
+out:
+	ret = persistent_huge_pages(h);
+	spin_unlock(&hugetlb_lock);
+	return ret;
+}
+#else
 static unsigned long set_max_huge_pages(struct hstate *h, unsigned long count,
 						nodemask_t *nodes_allowed)
 {
@@ -1337,6 +1590,7 @@
 	spin_unlock(&hugetlb_lock);
 	return ret;
 }
+#endif // !POPSHM
 
 #define HSTATE_ATTR_RO(_name) \
 	static struct kobj_attribute _name##_attr = __ATTR_RO(_name)
@@ -1767,6 +2021,22 @@
 
 static int __init hugetlb_init(void)
 {
+#ifdef CONFIG_POPSHM
+	void* coreinfo_address;
+	int config_register_data, local_core_x, local_core_y;
+
+	coreinfo_address = ioremap_nocache(CRB_OWN + MYTILEID, sizeof(int));
+	if (coreinfo_address) {
+		config_register_data = readl(coreinfo_address);
+		local_core_x = (config_register_data >> 3) & 0x0f;
+		local_core_y = (config_register_data >> 7) & 0x0f;
+		local_core_z = (config_register_data) & 0x07;
+		local_core_id = ((local_core_x + (6 * local_core_y)) * 2) + local_core_z;
+
+		iounmap(coreinfo_address);
+	}
+#endif
+
 	/* Some platform decide whether they support huge pages at boot
 	 * time. On these, such as powerpc, HPAGE_SHIFT is set to 0 when
 	 * there is no such support
@@ -1776,6 +2046,7 @@
 
 	if (!size_to_hstate(default_hstate_size)) {
 		default_hstate_size = HPAGE_SIZE;
+		printk(KERN_WARNING "hugepages size = %lu\n", default_hstate_size );
 		if (!size_to_hstate(default_hstate_size))
 			hugetlb_add_hstate(HUGETLB_PAGE_ORDER);
 	}
@@ -1824,6 +2095,37 @@
 	parsed_hstate = h;
 }
 
+#ifdef CONFIG_POPSHM
+static int __init hugetlb_nrpages_setup(char *s)
+{
+	unsigned long *mhp;
+	static unsigned long *last_mhp;
+
+	printk(KERN_WARNING "hugetlb_nrpages_setup\n");
+
+	if (!max_hstate)
+		mhp = &default_hstate_max_huge_pages;
+	else
+		mhp = &parsed_hstate->max_huge_pages;
+
+	if (mhp == last_mhp) {
+		printk(KERN_WARNING "popshmpages= specified twice without "
+			"interleaving hugepagesz=, ignoring\n");
+		return 1;
+	}
+
+	if (sscanf(s, "%lu", mhp) <= 0)
+		*mhp = 0;
+
+	if (max_hstate && parsed_hstate->order >= MAX_ORDER)
+		hugetlb_hstate_alloc_pages(parsed_hstate);
+
+	last_mhp = mhp;
+
+	return 1;
+}
+__setup("popshmpages=", hugetlb_nrpages_setup);
+#else
 static int __init hugetlb_nrpages_setup(char *s)
 {
 	unsigned long *mhp;
@@ -1860,6 +2162,7 @@
 	return 1;
 }
 __setup("hugepages=", hugetlb_nrpages_setup);
+#endif
 
 static int __init hugetlb_default_setup(char *s)
 {
@@ -1978,6 +2281,17 @@
 void hugetlb_report_meminfo(struct seq_file *m)
 {
 	struct hstate *h = &default_hstate;
+#ifdef CONFIG_POPSHM
+	seq_printf(m,
+			"POPSHM pages:    %5lu\n"
+			"POPSHM page size:    %5lu kB\n"
+			"POPSHM buffer size:  %5lu kB\n"
+			"POPSHM base address: 0x%08x\n",
+			h->nr_huge_pages/4,
+			POPSHMPAGE_SIZE/1024,
+			(HPAGE_SIZE * h->nr_huge_pages )/1024,
+			popshm_base_address);
+#else
 	seq_printf(m,
 			"HugePages_Total:   %5lu\n"
 			"HugePages_Free:    %5lu\n"
@@ -1989,6 +2303,7 @@
 			h->resv_huge_pages,
 			h->surplus_huge_pages,
 			1UL << (huge_page_order(h) + PAGE_SHIFT - 10));
+#endif
 }
 
 int hugetlb_report_node_meminfo(int nid, char *buf)
@@ -2353,12 +2668,14 @@
 			unsigned long address, pte_t *ptep, pte_t pte,
 			struct page *pagecache_page)
 {
-	struct hstate *h = hstate_vma(vma);
+	struct hstate *h;
 	struct page *old_page, *new_page;
 	int avoidcopy;
 	int outside_reserve = 0;
 
+	printk(KERN_WARNING "hugetlb_cow\n");
 	old_page = pte_page(pte);
+	h = hstate_vma(vma);
 
 retry_avoidcopy:
 	/* If no-one else is actually using this page, avoid the copy
